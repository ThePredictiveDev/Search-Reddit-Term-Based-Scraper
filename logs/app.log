2025-05-29 07:43:12,360 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:43:12,383 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:43:12,383 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 07:43:12,383 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 07:45:44,136 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:45:44,158 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:45:44,158 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 07:45:44,158 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 07:45:44,158 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 07:45:44,159 [INFO] __main__:141: Database initialized successfully
2025-05-29 07:45:44,160 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 07:47:05,710 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:47:05,732 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:47:05,732 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 07:47:05,732 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 07:47:05,732 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 07:47:05,733 [INFO] __main__:183: Database initialized successfully
2025-05-29 07:47:05,733 [INFO] __main__:147: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 07:47:06,655 [INFO] httpx:1025: HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 07:47:06,667 [INFO] httpx:1025: HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-05-29 07:47:07,041 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 07:48:36,469 [INFO] __main__:927: Shutting down Reddit Mention Tracker...
2025-05-29 07:48:36,469 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 07:51:55,867 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:51:55,890 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:51:55,891 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 07:51:55,891 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 07:51:55,891 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 07:51:55,891 [INFO] app:183: Database initialized successfully
2025-05-29 07:51:55,891 [WARNING] app:194: Could not start system monitoring: no running event loop
2025-05-29 07:51:55,895 [WARNING] app:206: Could not start periodic maintenance: no running event loop
2025-05-29 07:51:55,895 [INFO] app:147: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 07:51:57,456 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 07:51:58,670 [INFO] httpx:1025: HTTP Request: GET http://localhost:7862/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 07:52:00,740 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7862/ "HTTP/1.1 200 OK"
2025-05-29 07:54:36,922 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:54:36,945 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:54:36,945 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 07:54:36,945 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 07:54:36,945 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 07:54:36,945 [INFO] app:183: Database initialized successfully
2025-05-29 07:54:36,945 [WARNING] app:194: Could not start system monitoring: no running event loop
2025-05-29 07:54:36,948 [WARNING] app:206: Could not start periodic maintenance: no running event loop
2025-05-29 07:54:36,949 [INFO] app:147: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 07:58:27,973 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:58:27,996 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:58:27,996 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 07:58:27,996 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 07:58:27,996 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 07:58:27,996 [INFO] app:183: Database initialized successfully
2025-05-29 07:58:27,996 [WARNING] app:194: Could not start system monitoring: no running event loop
2025-05-29 07:58:28,000 [WARNING] app:206: Could not start periodic maintenance: no running event loop
2025-05-29 07:58:28,000 [INFO] app:147: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 07:58:43,496 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:58:43,519 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 07:58:43,519 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 07:58:43,519 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 07:58:43,519 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 07:58:43,520 [INFO] app:183: Database initialized successfully
2025-05-29 07:58:43,520 [WARNING] app:194: Could not start system monitoring: no running event loop
2025-05-29 07:58:43,524 [WARNING] app:206: Could not start periodic maintenance: no running event loop
2025-05-29 07:58:43,525 [INFO] app:147: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 07:58:45,005 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 07:58:46,303 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 07:58:48,350 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:00:06,491 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:00:06,513 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:00:06,513 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:00:06,513 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:00:06,513 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:00:06,513 [INFO] app:183: Database initialized successfully
2025-05-29 08:00:06,513 [WARNING] app:194: Could not start system monitoring: no running event loop
2025-05-29 08:00:06,517 [WARNING] app:206: Could not start periodic maintenance: no running event loop
2025-05-29 08:00:06,517 [INFO] app:147: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:00:58,558 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:00:58,580 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:00:58,580 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:00:58,581 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:00:58,581 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:00:58,581 [INFO] app:203: Database initialized successfully
2025-05-29 08:00:58,581 [WARNING] app:214: Could not start system monitoring: no running event loop
2025-05-29 08:00:58,585 [WARNING] app:226: Could not start periodic maintenance: no running event loop
2025-05-29 08:00:58,585 [INFO] app:167: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:01:00,083 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:01:01,355 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:01:03,399 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:10:24,472 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:10:24,494 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:10:24,494 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:10:24,494 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:10:24,495 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:10:24,496 [INFO] app:291: Database initialized successfully
2025-05-29 08:10:24,498 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:10:24,499 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:10:24,502 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:10:24,524 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:10:24,525 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:10:24,525 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:10:24,525 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:10:24,527 [INFO] app:291: Database initialized successfully
2025-05-29 08:10:24,528 [WARNING] monitoring.system_monitor:174: Monitoring is already active
2025-05-29 08:10:24,528 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:10:25,026 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:10:25,048 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:10:25,048 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:10:25,048 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:10:25,049 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:10:25,050 [INFO] app:291: Database initialized successfully
2025-05-29 08:10:25,051 [WARNING] monitoring.system_monitor:174: Monitoring is already active
2025-05-29 08:10:25,052 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:10:25,511 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.59 (threshold: 85)
2025-05-29 08:10:26,028 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:10:26,428 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:16:21,507 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:16:21,530 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:16:21,530 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:16:21,530 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:16:21,531 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:16:21,532 [INFO] app:291: Database initialized successfully
2025-05-29 08:16:21,535 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:16:21,536 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:16:22,090 [INFO] httpx:1025: HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:16:22,102 [INFO] httpx:1025: HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:16:22,547 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.59 (threshold: 85)
2025-05-29 08:16:23,263 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:16:57,095 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:16:57,118 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:16:57,118 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:16:57,118 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:16:57,119 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:16:57,120 [INFO] app:291: Database initialized successfully
2025-05-29 08:16:57,122 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:16:57,123 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:16:57,678 [INFO] httpx:1025: HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:16:57,707 [INFO] httpx:1025: HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:16:58,134 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.59 (threshold: 85)
2025-05-29 08:16:58,739 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:21:00,324 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:21:00,347 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:21:00,347 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:21:00,348 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:21:00,348 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:21:00,349 [INFO] app:291: Database initialized successfully
2025-05-29 08:21:00,352 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:21:00,353 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:21:00,964 [INFO] httpx:1025: HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:21:01,140 [INFO] httpx:1025: HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:21:01,364 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.59 (threshold: 85)
2025-05-29 08:21:01,770 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:21:12,372 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:21:12,397 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:21:12,397 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:21:12,397 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:21:12,397 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:21:12,397 [INFO] app:291: Database initialized successfully
2025-05-29 08:21:12,400 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:21:12,401 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:21:12,401 [INFO] startup:51: [OK] Tracker created successfully in 0.03s
2025-05-29 08:21:12,401 [INFO] startup:53: [UI] Creating Gradio interface...
2025-05-29 08:21:12,931 [INFO] startup:57: [OK] Interface created successfully in 0.53s
2025-05-29 08:21:12,931 [INFO] startup:59: [LAUNCH] Launching Gradio interface...
2025-05-29 08:21:12,931 [INFO] startup:60:    Server port: 7861
2025-05-29 08:21:12,931 [INFO] startup:61:    Server name: 0.0.0.0
2025-05-29 08:21:12,931 [INFO] startup:62:    Debug mode: True
2025-05-29 08:21:12,931 [INFO] startup:63:    Share: False
2025-05-29 08:21:13,411 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.59 (threshold: 85)
2025-05-29 08:21:13,874 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:21:14,962 [INFO] startup:73: [OK] Port 7861 is available
2025-05-29 08:21:14,962 [INFO] startup:77: [START] Starting Gradio launch...
2025-05-29 08:21:14,962 [ERROR] startup:108: [ERROR] Failed to start interface: Blocks.launch() got an unexpected keyword argument 'show_tips'
2025-05-29 08:21:14,962 [ERROR] startup:109: [INFO] Error details:
2025-05-29 08:21:14,962 [ERROR] startup:113: [DEBUG] System state debugging:
2025-05-29 08:21:14,962 [ERROR] startup:114:    Current working directory: C:\Users\Devansh\Downloads\intern_application_round1
2025-05-29 08:21:14,962 [ERROR] startup:115:    Python executable: C:\Program Files\Python312\python.exe
2025-05-29 08:21:14,962 [ERROR] startup:116:    Environment variables:
2025-05-29 08:21:14,962 [ERROR] startup:119:      PYTHONPATH: Not set
2025-05-29 08:21:14,962 [ERROR] startup:119:      PATH: C:\Program Files (x86)\Razer Chroma SDK\bin;C:\Program Files\Razer Chroma SDK\bin;C:\Program Files (x86)\VMware\VMware Workstation\bin\;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.3\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.3\libnvvp;C:\Program Files\Python312\Scripts\;C:\Program Files\Python312\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\dotnet\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files\NVIDIA Corporation\Nsight Compute 2023.3.1\;C:\Users\Devansh\AppData\Local\Programs\Python\Python312;C:\Users\Devansh\AppData\Local\Programs\Python\Python312\Scripts;C:\Users\Devansh\AppData\Local\anaconda3;C:\Users\Devansh\AppData\Local\anaconda3\Scripts;C:\Program Files\Git\cmd;C:\Program Files\Git LFS;C:\Users\Devansh\AppData\Local\anaconda3\Lib\site-packages\pytesseract;C:\Program Files\Tesseract-OCR;C:\Program Files\CMake\bin;C:\Program Files\nodejs\;C:\Program Files\ffmpeg-2025-03-20-git-76f09ab647-full_build\bin;C:\WINDOWS\system32\config\systemprofile\AppData\Local\Microsoft\WindowsApps;C:\Program Files (x86)\Razer\ChromaBroadcast\bin;C:\Program Files\Razer\ChromaBroadcast\bin;C:\Users\Devansh\dev\flutter\bin;C:\Users\Devansh\AppData\Local\Microsoft\WindowsApps;C:\Users\Devansh\AppData\Local\GitHubDesktop\bin;C:\Users\Devansh\AppData\Local\Programs\Microsoft VS Code\bin;C:\ta-lib\lib;C:\Users\Devansh\AppData\Roaming\npm;C:\Program Files\Graphviz\bin;;C:\ProgramData\chocolatey\bin;C:\Program Files\Memurai\;C:\Program Files\Redis\;C:\Program Files\Docker\Docker\resources\bin;C:\Users\Devansh\.cargo\bin;C:\Users\Devansh\dev\flutter\bin;C:\Users\Devansh\AppData\Local\Microsoft\WindowsApps;C:\Users\Devansh\AppData\Local\GitHubDesktop\bin;C:\Users\Devansh\AppData\Local\Programs\Microsoft VS Code\bin;C:\ta-lib\lib;C:\Users\Devansh\AppData\Roaming\npm;C:\Users\Devansh\AppData\Local\Programs\cursor\resources\app\bin;;c:\Users\Devansh\.cursor\extensions\ms-python.debugpy-2025.8.0-win32-x64\bundled\scripts\noConfigScripts;c:\Users\Devansh\AppData\Roaming\Cursor\User\globalStorage\github.copilot-chat\debugCommand
2025-05-29 08:21:14,963 [ERROR] startup:119:      GRADIO_SERVER_PORT: Not set
2025-05-29 08:24:55,244 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:24:55,266 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:24:55,266 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:24:55,266 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:24:55,266 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:24:55,267 [INFO] app:291: Database initialized successfully
2025-05-29 08:24:55,270 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:24:55,271 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:24:56,279 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.60 (threshold: 85)
2025-05-29 08:24:56,583 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:24:57,865 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:24:59,912 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:25:32,355 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:25:32,378 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:25:32,378 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:25:32,378 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:25:32,378 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:25:32,380 [INFO] app:291: Database initialized successfully
2025-05-29 08:25:32,383 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:25:32,385 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:25:33,013 [INFO] httpx:1025: HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:25:33,025 [INFO] httpx:1025: HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:25:33,397 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.60 (threshold: 85)
2025-05-29 08:25:33,780 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:25:35,959 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:25:35,983 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:25:35,983 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:25:35,983 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:25:35,984 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:25:35,985 [INFO] app:291: Database initialized successfully
2025-05-29 08:25:35,986 [WARNING] monitoring.system_monitor:174: Monitoring is already active
2025-05-29 08:25:35,987 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:25:37,375 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:25:38,367 [INFO] httpx:1025: HTTP Request: GET http://localhost:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:25:40,402 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-29 08:27:23,673 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:27:23,695 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:27:23,696 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:27:23,696 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:27:23,696 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:27:23,698 [INFO] app:291: Database initialized successfully
2025-05-29 08:27:23,700 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:27:23,701 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:27:24,268 [INFO] httpx:1025: HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:27:24,281 [INFO] httpx:1025: HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:27:24,712 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.60 (threshold: 85)
2025-05-29 08:27:25,034 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:27:27,068 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:27:27,095 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:27:27,095 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:27:27,095 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:27:27,096 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:27:27,098 [INFO] app:291: Database initialized successfully
2025-05-29 08:27:27,099 [WARNING] monitoring.system_monitor:174: Monitoring is already active
2025-05-29 08:27:27,100 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:27:28,428 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:27:29,467 [INFO] httpx:1025: HTTP Request: GET http://localhost:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:27:31,531 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-29 08:29:39,083 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:29:39,105 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:29:39,105 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:29:39,105 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:29:39,105 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:29:39,107 [INFO] app:291: Database initialized successfully
2025-05-29 08:29:39,109 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:29:39,110 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:29:39,111 [INFO] minimal_test:44: Tracker created successfully
2025-05-29 08:29:39,111 [INFO] minimal_test:47: Creating interface...
2025-05-29 08:29:39,608 [INFO] minimal_test:49: Interface created successfully
2025-05-29 08:29:39,608 [INFO] minimal_test:52: Launching interface...
2025-05-29 08:29:40,123 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.60 (threshold: 85)
2025-05-29 08:29:40,643 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:29:41,907 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:29:43,961 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:34:11,703 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:34:11,729 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:34:11,729 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:34:11,729 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:34:11,729 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:34:11,730 [INFO] app:291: Database initialized successfully
2025-05-29 08:34:11,733 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:34:11,733 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:34:12,744 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.60 (threshold: 85)
2025-05-29 08:34:13,144 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:34:13,751 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:35:46,075 [ERROR] app:612: Search failed for 'OpenAI': 'CacheManager' object has no attribute 'get'
2025-05-29 08:35:46,076 [ERROR] app:613: Traceback (most recent call last):
  File "C:\Users\Devansh\Downloads\intern_application_round1\app.py", line 485, in search_mentions
    progress_callback(0.1, "Initializing search...")
                           ^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'CacheManager' object has no attribute 'get'

2025-05-29 08:35:46,440 [ERROR] app:892: Search failed: 'MetricsVisualizer' object has no attribute 'create_detailed_analytics'
2025-05-29 08:38:34,185 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:38:34,207 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:38:34,208 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:38:34,208 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:38:34,208 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:38:34,209 [INFO] app:291: Database initialized successfully
2025-05-29 08:38:34,211 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:38:34,211 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:38:35,222 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.61 (threshold: 85)
2025-05-29 08:38:35,620 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:38:36,229 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:38:36,901 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:38:38,966 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:39:42,349 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:40:01,538 [ERROR] app:611: Search failed for 'OpenAI': 'RedditScraper' object has no attribute 'search_mentions_stream'
2025-05-29 08:40:01,542 [ERROR] app:612: Traceback (most recent call last):
  File "C:\Users\Devansh\Downloads\intern_application_round1\app.py", line 510, in search_mentions
    async for batch in self.scraper.search_mentions_stream(search_term):
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RedditScraper' object has no attribute 'search_mentions_stream'

2025-05-29 08:40:01,974 [ERROR] app:652: Failed to get search history: 'DatabaseManager' object has no attribute 'SearchSession'
2025-05-29 08:40:48,482 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:41:54,604 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:43:00,754 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:43:07,902 [ERROR] app:611: Search failed for 'OpenAI': 'RedditScraper' object has no attribute 'search_mentions_stream'
2025-05-29 08:43:07,905 [ERROR] app:612: Traceback (most recent call last):
  File "C:\Users\Devansh\Downloads\intern_application_round1\app.py", line 510, in search_mentions
    # Use the correct method name from RedditScraper
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RedditScraper' object has no attribute 'search_mentions_stream'

2025-05-29 08:43:07,925 [ERROR] app:652: Failed to get search history: 'DatabaseManager' object has no attribute 'SearchSession'
2025-05-29 08:43:39,973 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:43:39,996 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:43:39,996 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:43:39,996 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:43:39,996 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:43:39,996 [INFO] app:291: Database initialized successfully
2025-05-29 08:43:39,999 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:43:40,000 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:43:40,000 [INFO] startup:51: [OK] Tracker created successfully in 0.03s
2025-05-29 08:43:40,000 [INFO] startup:53: [UI] Creating Gradio interface...
2025-05-29 08:43:40,480 [INFO] startup:57: [OK] Interface created successfully in 0.48s
2025-05-29 08:43:40,480 [INFO] startup:59: [LAUNCH] Launching Gradio interface...
2025-05-29 08:43:40,480 [INFO] startup:60:    Server port: 7860
2025-05-29 08:43:40,480 [INFO] startup:61:    Server name: 0.0.0.0
2025-05-29 08:43:40,481 [INFO] startup:62:    Debug mode: False
2025-05-29 08:43:40,481 [INFO] startup:63:    Share: False
2025-05-29 08:43:41,011 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.61 (threshold: 85)
2025-05-29 08:43:41,465 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:43:42,018 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:43:42,527 [INFO] startup:73: [OK] Port 7860 is available
2025-05-29 08:43:42,527 [INFO] startup:77: [START] Starting Gradio launch...
2025-05-29 08:43:42,527 [ERROR] startup:108: [ERROR] Failed to start interface: Blocks.launch() got an unexpected keyword argument 'show_tips'
2025-05-29 08:43:42,527 [ERROR] startup:109: [INFO] Error details:
2025-05-29 08:43:42,527 [ERROR] startup:113: [DEBUG] System state debugging:
2025-05-29 08:43:42,527 [ERROR] startup:114:    Current working directory: C:\Users\Devansh\Downloads\intern_application_round1
2025-05-29 08:43:42,527 [ERROR] startup:115:    Python executable: C:\Program Files\Python312\python.exe
2025-05-29 08:43:42,527 [ERROR] startup:116:    Environment variables:
2025-05-29 08:43:42,527 [ERROR] startup:119:      PYTHONPATH: Not set
2025-05-29 08:43:42,528 [ERROR] startup:119:      PATH: C:\Program Files (x86)\Razer Chroma SDK\bin;C:\Program Files\Razer Chroma SDK\bin;C:\Program Files (x86)\VMware\VMware Workstation\bin\;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.3\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.3\libnvvp;C:\Program Files\Python312\Scripts\;C:\Program Files\Python312\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\dotnet\;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;C:\Program Files\NVIDIA Corporation\Nsight Compute 2023.3.1\;C:\Users\Devansh\AppData\Local\Programs\Python\Python312;C:\Users\Devansh\AppData\Local\Programs\Python\Python312\Scripts;C:\Users\Devansh\AppData\Local\anaconda3;C:\Users\Devansh\AppData\Local\anaconda3\Scripts;C:\Program Files\Git\cmd;C:\Program Files\Git LFS;C:\Users\Devansh\AppData\Local\anaconda3\Lib\site-packages\pytesseract;C:\Program Files\Tesseract-OCR;C:\Program Files\CMake\bin;C:\Program Files\nodejs\;C:\Program Files\ffmpeg-2025-03-20-git-76f09ab647-full_build\bin;C:\WINDOWS\system32\config\systemprofile\AppData\Local\Microsoft\WindowsApps;C:\Program Files (x86)\Razer\ChromaBroadcast\bin;C:\Program Files\Razer\ChromaBroadcast\bin;C:\Users\Devansh\dev\flutter\bin;C:\Users\Devansh\AppData\Local\Microsoft\WindowsApps;C:\Users\Devansh\AppData\Local\GitHubDesktop\bin;C:\Users\Devansh\AppData\Local\Programs\Microsoft VS Code\bin;C:\ta-lib\lib;C:\Users\Devansh\AppData\Roaming\npm;C:\Program Files\Graphviz\bin;;C:\ProgramData\chocolatey\bin;C:\Program Files\Memurai\;C:\Program Files\Redis\;C:\Program Files\Docker\Docker\resources\bin;C:\Users\Devansh\.cargo\bin;C:\Users\Devansh\dev\flutter\bin;C:\Users\Devansh\AppData\Local\Microsoft\WindowsApps;C:\Users\Devansh\AppData\Local\GitHubDesktop\bin;C:\Users\Devansh\AppData\Local\Programs\Microsoft VS Code\bin;C:\ta-lib\lib;C:\Users\Devansh\AppData\Roaming\npm;C:\Users\Devansh\AppData\Local\Programs\cursor\resources\app\bin;;c:\Users\Devansh\.cursor\extensions\ms-python.debugpy-2025.8.0-win32-x64\bundled\scripts\noConfigScripts;c:\Users\Devansh\AppData\Roaming\Cursor\User\globalStorage\github.copilot-chat\debugCommand
2025-05-29 08:43:42,528 [ERROR] startup:119:      GRADIO_SERVER_PORT: Not set
2025-05-29 08:44:38,057 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:44:38,080 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:44:38,080 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:44:38,080 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:44:38,080 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:44:38,082 [INFO] app:291: Database initialized successfully
2025-05-29 08:44:38,084 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:44:38,086 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:44:39,096 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.61 (threshold: 85)
2025-05-29 08:44:40,103 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:44:40,666 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:44:41,269 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:44:42,692 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:45:13,227 [INFO] scraper.reddit_scraper:496: Trying primary search patterns
2025-05-29 08:45:22,941 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:45:23,533 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:45:24,452 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:45:26,827 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:45:27,794 [ERROR] scraper.reddit_scraper:599: Error scraping https://www.reddit.com/search/?q={query}&type=link&sort=new&t=week: EnhancedRedditMentionTracker.create_gradio_interface.<locals>.handle_search.<locals>.update_progress() missing 1 required positional argument: 'message'
2025-05-29 08:45:28,757 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:45:28,758 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:45:28,758 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:45:29,408 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:45:29,805 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:45:31,001 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:45:33,402 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:45:46,225 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:45:57,808 [WARNING] scraper.reddit_scraper:597: Timeout while scraping https://www.reddit.com/search/?q={query}&type=link&sort=hot&t=week
2025-05-29 08:45:59,283 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:45:59,283 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:45:59,284 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:45:59,785 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:45:59,894 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:46:01,201 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:46:02,894 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:03,380 [ERROR] scraper.reddit_scraper:599: Error scraping https://www.reddit.com/search/?q={query}&type=link&sort=top&t=week: EnhancedRedditMentionTracker.create_gradio_interface.<locals>.handle_search.<locals>.update_progress() missing 1 required positional argument: 'message'
2025-05-29 08:46:03,381 [INFO] scraper.reddit_scraper:496: Trying secondary search patterns
2025-05-29 08:46:03,692 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:46:03,692 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:46:03,693 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:03,693 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:04,294 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:46:04,462 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:46:05,755 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:46:07,528 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:33,384 [WARNING] scraper.reddit_scraper:597: Timeout while scraping https://www.reddit.com/search/?q={query}&type=sr_and_comment&sort=new&t=week
2025-05-29 08:46:34,541 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:34,542 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:35,186 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:46:35,356 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:46:36,606 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:46:38,423 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:40,440 [ERROR] scraper.reddit_scraper:599: Error scraping https://www.reddit.com/search/?q={query}&type=sr_and_comment&sort=hot&t=week: EnhancedRedditMentionTracker.create_gradio_interface.<locals>.handle_search.<locals>.update_progress() missing 1 required positional argument: 'message'
2025-05-29 08:46:40,441 [INFO] scraper.reddit_scraper:496: Trying fallback search patterns
2025-05-29 08:46:41,599 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:46:41,600 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:41,600 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:42,277 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:46:42,419 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:46:43,684 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:46:45,413 [ERROR] scraper.reddit_scraper:599: Error scraping https://www.reddit.com/search/?q={query}&sort=new: EnhancedRedditMentionTracker.create_gradio_interface.<locals>.handle_search.<locals>.update_progress() missing 1 required positional argument: 'message'
2025-05-29 08:46:45,488 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:46,350 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:46,351 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:46:46,351 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:46:46,352 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:46,888 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:46:47,038 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:46:48,324 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:46:50,900 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:46:52,321 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:47:15,420 [WARNING] scraper.reddit_scraper:597: Timeout while scraping https://www.reddit.com/search/?q={query}&sort=hot
2025-05-29 08:47:15,455 [ERROR] scraper.reddit_scraper:347: Scraping failed for 'OpenAI': EnhancedRedditMentionTracker.create_gradio_interface.<locals>.handle_search.<locals>.update_progress() missing 1 required positional argument: 'message'
2025-05-29 08:47:15,456 [ERROR] app:625: Search failed for 'OpenAI': 2 validation errors for ProgressUnit
progress
  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value="[ERROR] Error: Scraping ...nal argument: 'message'", input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/float_parsing
desc
  Input should be a valid string [type=string_type, input_value=1.0, input_type=float]
    For further information visit https://errors.pydantic.dev/2.10/v/string_type
2025-05-29 08:47:15,457 [ERROR] app:626: Traceback (most recent call last):
  File "C:\Users\Devansh\Downloads\intern_application_round1\app.py", line 511, in search_mentions
    raw_mentions = await self.scraper.scrape_mentions(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Devansh\Downloads\intern_application_round1\scraper\reddit_scraper.py", line 353, in scrape_mentions
    progress_callback(f"[ERROR] Error: {error_msg}", 1.0)
  File "C:\Users\Devansh\Downloads\intern_application_round1\app.py", line 873, in update_progress
    progress(value, desc=message)
  File "C:\Users\Devansh\AppData\Roaming\Python\Python312\site-packages\gradio\helpers.py", line 727, in __call__
    callback(
  File "C:\Users\Devansh\AppData\Roaming\Python\Python312\site-packages\gradio\queueing.py", line 382, in set_progress
    progress_unit = ProgressUnit(
                    ^^^^^^^^^^^^^
  File "C:\Users\Devansh\AppData\Roaming\Python\Python312\site-packages\pydantic\main.py", line 214, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 2 validation errors for ProgressUnit
progress
  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value="[ERROR] Error: Scraping ...nal argument: 'message'", input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/float_parsing
desc
  Input should be a valid string [type=string_type, input_value=1.0, input_type=float]
    For further information visit https://errors.pydantic.dev/2.10/v/string_type

2025-05-29 08:47:58,417 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:51:14,492 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:51:14,514 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:51:14,514 [INFO] scraper.reddit_scraper:142: Cache manager enabled
2025-05-29 08:51:14,514 [INFO] scraper.reddit_scraper:152: Real-time monitoring enabled
2025-05-29 08:51:14,514 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:51:14,515 [INFO] app:291: Database initialized successfully
2025-05-29 08:51:14,517 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:51:14,518 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:51:15,525 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.61 (threshold: 85)
2025-05-29 08:51:15,862 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:51:16,533 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:51:17,112 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:51:19,191 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:51:32,414 [INFO] scraper.reddit_scraper:496: Trying primary search patterns
2025-05-29 08:51:33,439 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:51:34,147 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:51:34,666 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:51:37,289 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:51:48,142 [WARNING] scraper.reddit_scraper:671: Timeout waiting for posts to load
2025-05-29 08:51:49,263 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:51:49,263 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:51:49,264 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:51:50,029 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:51:50,378 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:51:51,605 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:51:53,241 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:52:18,151 [WARNING] scraper.reddit_scraper:597: Timeout while scraping https://www.reddit.com/search/?q={query}&type=link&sort=hot&t=week
2025-05-29 08:52:18,469 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:52:18,469 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:52:18,470 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:52:18,470 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:52:19,044 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:52:19,291 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:52:20,507 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:52:22,206 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:52:22,662 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:52:32,367 [WARNING] scraper.reddit_scraper:671: Timeout waiting for posts to load
2025-05-29 08:52:32,373 [INFO] scraper.reddit_scraper:496: Trying secondary search patterns
2025-05-29 08:52:33,510 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:52:33,510 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:52:33,510 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:52:34,144 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:52:34,318 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:52:35,595 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:52:37,359 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:53:02,383 [WARNING] scraper.reddit_scraper:597: Timeout while scraping https://www.reddit.com/search/?q={query}&type=sr_and_comment&sort=new&t=week
2025-05-29 08:53:03,537 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:53:03,538 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:53:03,538 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:53:04,170 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:53:04,347 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:53:09,181 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:53:09,338 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:53:23,531 [WARNING] scraper.reddit_scraper:671: Timeout waiting for posts to load
2025-05-29 08:53:23,536 [INFO] scraper.reddit_scraper:496: Trying fallback search patterns
2025-05-29 08:53:23,815 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:53:23,816 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:53:23,816 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:53:24,499 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:53:24,635 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:53:25,933 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:53:27,715 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:53:28,763 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:53:41,321 [WARNING] scraper.reddit_scraper:671: Timeout waiting for posts to load
2025-05-29 08:53:42,736 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:53:42,736 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:53:42,737 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:53:42,737 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:53:45,065 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:53:45,211 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:53:46,498 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:53:49,070 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:53:59,159 [WARNING] scraper.reddit_scraper:671: Timeout waiting for posts to load
2025-05-29 08:54:00,253 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:54:00,254 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:54:00,254 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:54:00,254 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:54:01,137 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:54:01,270 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:54:02,536 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:54:04,293 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:54:29,167 [WARNING] scraper.reddit_scraper:639: Error searching r/MachineLearning: Page.goto: Timeout 30000ms exceeded.
Call log:
  - navigating to "https://www.reddit.com/r/MachineLearning/search/?q=OpenAI&restrict_sr=1&sort=new&t=week", waiting until "networkidle"

2025-05-29 08:54:29,497 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:54:29,498 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:54:29,498 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:54:29,499 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:54:30,451 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:54:30,711 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:54:31,881 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:54:33,648 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:54:34,895 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:54:44,030 [WARNING] scraper.reddit_scraper:671: Timeout waiting for posts to load
2025-05-29 08:54:45,143 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:54:45,143 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:54:45,143 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:54:46,053 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:54:46,190 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:54:47,461 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:54:49,234 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:55:06,938 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.61 (avg: 93.61, std: 0.00)
2025-05-29 08:55:06,939 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.80 (avg: 121.83, std: 0.01)
2025-05-29 08:55:14,043 [WARNING] scraper.reddit_scraper:639: Error searching r/cybersecurity: Page.goto: Timeout 30000ms exceeded.
Call log:
  - navigating to "https://www.reddit.com/r/cybersecurity/search/?q=OpenAI&restrict_sr=1&sort=new&t=week", waiting until "networkidle"

2025-05-29 08:55:14,330 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:55:14,330 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:55:14,331 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:55:14,331 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:55:14,956 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:55:15,178 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:55:16,362 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:55:17,953 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.61 (avg: 93.61, std: 0.00)
2025-05-29 08:55:17,954 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.78 (avg: 121.83, std: 0.01)
2025-05-29 08:55:18,144 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:55:40,997 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:55:44,054 [WARNING] scraper.reddit_scraper:639: Error searching r/programming: Page.goto: Timeout 30000ms exceeded.
Call log:
  - navigating to "https://www.reddit.com/r/programming/search/?q=OpenAI&restrict_sr=1&sort=new&t=week", waiting until "networkidle"

2025-05-29 08:55:44,988 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:55:44,989 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:55:44,989 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:55:44,989 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:55:45,760 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:55:45,917 [WARNING] scraper.reddit_scraper:478: Rate limit detected, implementing backoff
2025-05-29 08:55:47,222 [WARNING] scraper.reddit_scraper:450: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:55:48,961 [WARNING] scraper.reddit_scraper:450: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:56:01,200 [WARNING] scraper.reddit_scraper:671: Timeout waiting for posts to load
2025-05-29 08:56:01,243 [ERROR] scraper.reddit_scraper:347: Scraping failed for 'OpenAI': 'RedditScraper' object has no attribute '_post_process_mentions'
2025-05-29 08:56:01,243 [WARNING] app:520: Scraping error: Scraping failed for 'OpenAI': 'RedditScraper' object has no attribute '_post_process_mentions'
2025-05-29 08:56:01,243 [ERROR] app:625: Search failed for 'OpenAI': Scraping failed for 'OpenAI': 'RedditScraper' object has no attribute '_post_process_mentions'
2025-05-29 08:56:01,247 [ERROR] app:626: Traceback (most recent call last):
  File "C:\Users\Devansh\Downloads\intern_application_round1\scraper\reddit_scraper.py", line 320, in scrape_mentions
    search_term: str,
                      
AttributeError: 'RedditScraper' object has no attribute '_post_process_mentions'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\Devansh\Downloads\intern_application_round1\app.py", line 524, in search_mentions
    raise e
  File "C:\Users\Devansh\Downloads\intern_application_round1\app.py", line 511, in search_mentions
    raw_mentions = await self.scraper.scrape_mentions(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Devansh\Downloads\intern_application_round1\scraper\reddit_scraper.py", line 360, in scrape_mentions
    self.logger.info(f"Attempting Reddit API search for: {search_term}")
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
scraper.reddit_scraper.ScrapingError: Scraping failed for 'OpenAI': 'RedditScraper' object has no attribute '_post_process_mentions'

2025-05-29 08:56:08,647 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:56:08,675 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:56:08,676 [INFO] scraper.reddit_scraper:208: Cache manager enabled
2025-05-29 08:56:08,676 [INFO] scraper.reddit_scraper:218: Real-time monitoring enabled
2025-05-29 08:56:08,676 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:56:08,677 [INFO] app:291: Database initialized successfully
2025-05-29 08:56:08,679 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:56:08,681 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:56:09,690 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.61 (threshold: 85)
2025-05-29 08:56:10,698 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:56:10,751 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:58:16,541 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:58:16,568 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:58:16,568 [INFO] scraper.reddit_scraper:208: Cache manager enabled
2025-05-29 08:58:16,568 [INFO] scraper.reddit_scraper:218: Real-time monitoring enabled
2025-05-29 08:58:16,569 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 08:58:16,570 [INFO] app:291: Database initialized successfully
2025-05-29 08:58:16,572 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 08:58:16,573 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 08:58:17,581 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 08:58:17,904 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 08:58:18,589 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:58:19,169 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 08:58:21,250 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 08:58:45,654 [INFO] scraper.reddit_scraper:360: Attempting Reddit API search for: OpenAI
2025-05-29 08:58:47,988 [INFO] scraper.reddit_scraper:369: Reddit API returned 100 mentions
2025-05-29 08:58:47,988 [WARNING] scraper.reddit_scraper:401: Reddit API failed: 'RedditScraper' object has no attribute '_post_process_mentions', falling back to web scraping
2025-05-29 08:58:47,988 [INFO] scraper.reddit_scraper:409: Falling back to web scraping approach
2025-05-29 08:58:48,391 [INFO] scraper.reddit_scraper:619: Trying primary search patterns
2025-05-29 08:58:50,385 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:58:51,014 [WARNING] scraper.reddit_scraper:601: Rate limit detected, implementing backoff
2025-05-29 08:58:51,812 [WARNING] scraper.reddit_scraper:573: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:58:53,568 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:59:04,645 [WARNING] scraper.reddit_scraper:794: Timeout waiting for posts to load
2025-05-29 08:59:05,590 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:59:05,590 [WARNING] scraper.reddit_scraper:573: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:59:05,591 [WARNING] scraper.reddit_scraper:573: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:59:05,591 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:59:06,208 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:59:10,970 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:59:11,499 [WARNING] scraper.reddit_scraper:601: Rate limit detected, implementing backoff
2025-05-29 08:59:12,507 [WARNING] scraper.reddit_scraper:573: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:59:24,703 [WARNING] scraper.reddit_scraper:794: Timeout waiting for posts to load
2025-05-29 08:59:24,712 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 08:59:24,983 [WARNING] scraper.reddit_scraper:573: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:59:24,983 [WARNING] scraper.reddit_scraper:573: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:59:24,984 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:59:24,984 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:59:25,479 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:59:25,629 [WARNING] scraper.reddit_scraper:601: Rate limit detected, implementing backoff
2025-05-29 08:59:27,053 [WARNING] scraper.reddit_scraper:573: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:59:28,672 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:59:54,727 [WARNING] scraper.reddit_scraper:720: Timeout while scraping https://www.reddit.com/search/?q={query}&type=link&sort=top&t=week
2025-05-29 08:59:54,728 [INFO] scraper.reddit_scraper:619: Trying secondary search patterns
2025-05-29 08:59:54,988 [WARNING] scraper.reddit_scraper:573: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:59:54,989 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:59:54,989 [WARNING] scraper.reddit_scraper:573: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 08:59:54,989 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 08:59:55,511 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 08:59:55,681 [WARNING] scraper.reddit_scraper:601: Rate limit detected, implementing backoff
2025-05-29 08:59:56,991 [WARNING] scraper.reddit_scraper:573: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 08:59:58,743 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 09:00:24,744 [WARNING] scraper.reddit_scraper:720: Timeout while scraping https://www.reddit.com/search/?q={query}&type=sr_and_comment&sort=new&t=week
2025-05-29 09:00:25,059 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 09:00:25,059 [WARNING] scraper.reddit_scraper:573: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 09:00:25,059 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 09:00:25,680 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 09:00:25,849 [WARNING] scraper.reddit_scraper:601: Rate limit detected, implementing backoff
2025-05-29 09:00:27,120 [WARNING] scraper.reddit_scraper:573: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 09:00:28,913 [WARNING] scraper.reddit_scraper:573: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 09:00:30,813 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:01:08,770 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:01:08,791 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:01:08,792 [INFO] scraper.reddit_scraper:208: Cache manager enabled
2025-05-29 09:01:08,792 [INFO] scraper.reddit_scraper:218: Real-time monitoring enabled
2025-05-29 09:01:08,792 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:01:08,793 [INFO] app:291: Database initialized successfully
2025-05-29 09:01:08,795 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:01:08,796 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:01:09,807 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:01:10,171 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 09:01:10,815 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:01:11,402 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 09:01:13,470 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 09:01:25,461 [INFO] scraper.reddit_scraper:360: Attempting Reddit API search for: OpenAI
2025-05-29 09:01:27,797 [INFO] scraper.reddit_scraper:369: Reddit API returned 100 mentions
2025-05-29 09:01:27,798 [INFO] scraper.reddit_scraper:662: Post-processing 100 mentions...
2025-05-29 09:01:27,798 [INFO] scraper.reddit_scraper:666: After deduplication: 1 mentions
2025-05-29 09:01:27,798 [INFO] scraper.reddit_scraper:670: After validation: 0 mentions
2025-05-29 09:01:27,798 [INFO] scraper.reddit_scraper:696: After quality filtering (threshold 0.3): 0 mentions
2025-05-29 09:01:27,798 [INFO] scraper.reddit_scraper:710: Final result: 0 high-quality mentions
2025-05-29 09:01:27,800 [INFO] app:620: Search completed: OpenAI -> 0 mentions
2025-05-29 09:01:38,656 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:01:38,681 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:01:38,681 [INFO] scraper.reddit_scraper:208: Cache manager enabled
2025-05-29 09:01:38,682 [INFO] scraper.reddit_scraper:218: Real-time monitoring enabled
2025-05-29 09:01:38,682 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:01:38,684 [INFO] app:291: Database initialized successfully
2025-05-29 09:01:38,686 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:01:38,688 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:01:39,696 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:01:40,040 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 09:01:40,705 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:02:16,911 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:02:59,967 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.62 (avg: 93.62, std: 0.00)
2025-05-29 09:02:59,967 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.72 (avg: 121.75, std: 0.01)
2025-05-29 09:03:23,008 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:05:54,089 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:05:54,111 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:05:54,111 [INFO] scraper.reddit_scraper:215: Cache manager enabled
2025-05-29 09:05:54,111 [INFO] scraper.reddit_scraper:225: Real-time monitoring enabled
2025-05-29 09:05:54,111 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:05:54,112 [INFO] app:291: Database initialized successfully
2025-05-29 09:05:54,115 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:05:54,116 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:05:55,128 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:05:55,552 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 09:05:56,138 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:05:56,718 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 09:05:58,782 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 09:06:32,689 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:06:32,711 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:06:32,712 [INFO] scraper.reddit_scraper:215: Cache manager enabled
2025-05-29 09:06:32,712 [INFO] scraper.reddit_scraper:225: Real-time monitoring enabled
2025-05-29 09:06:32,712 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:06:32,714 [INFO] app:291: Database initialized successfully
2025-05-29 09:06:32,716 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:06:32,717 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:06:33,726 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:06:34,173 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 09:06:34,733 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:06:35,916 [INFO] scraper.reddit_scraper:367: Attempting Reddit API search for: OpenAI
2025-05-29 09:06:38,597 [INFO] scraper.reddit_scraper:376: Reddit API returned 100 mentions
2025-05-29 09:06:38,598 [INFO] scraper.reddit_scraper:669: Post-processing 100 mentions...
2025-05-29 09:06:38,598 [INFO] scraper.reddit_scraper:673: After deduplication: 100 mentions
2025-05-29 09:06:38,599 [INFO] scraper.reddit_scraper:677: After validation: 100 mentions
2025-05-29 09:06:38,602 [INFO] scraper.reddit_scraper:703: After quality filtering (threshold 0.3): 70 mentions
2025-05-29 09:06:38,604 [INFO] scraper.reddit_scraper:717: Final result: 70 high-quality mentions
2025-05-29 09:06:38,604 [INFO] analytics.data_validator:236: Validating dataset of 70 mentions
2025-05-29 09:06:39,338 [INFO] analytics.data_validator:259: Validation complete: 70/70 mentions passed
2025-05-29 09:06:39,338 [INFO] app:536: Data validation: 70/70 mentions passed
2025-05-29 09:06:39,376 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,376 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,376 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,377 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,377 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,377 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,377 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,377 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,377 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,378 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,378 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,378 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,378 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,378 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,378 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,379 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,379 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,379 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,379 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,379 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,379 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,379 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,380 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,380 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,380 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,380 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,381 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,381 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,381 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,381 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,381 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,381 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,382 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,382 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,382 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,382 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,382 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,383 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,383 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,383 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,383 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,383 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,383 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,384 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,384 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,384 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,384 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,384 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,384 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,385 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,385 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,385 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,385 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,385 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,385 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,386 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,386 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,386 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,386 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,386 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,387 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,387 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,387 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,387 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,387 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,388 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,388 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,388 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,388 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,388 [ERROR] app:569: Failed to save mention: 'source' is an invalid keyword argument for RedditMention
2025-05-29 09:06:39,391 [INFO] app:620: Search completed: OpenAI -> 70 mentions
2025-05-29 09:07:02,264 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:08:08,376 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:08:49,608 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:08:49,630 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:08:49,631 [INFO] scraper.reddit_scraper:215: Cache manager enabled
2025-05-29 09:08:49,631 [INFO] scraper.reddit_scraper:225: Real-time monitoring enabled
2025-05-29 09:08:49,631 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:08:49,632 [INFO] app:291: Database initialized successfully
2025-05-29 09:08:49,635 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:08:49,636 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:08:50,649 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:08:51,656 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:08:52,240 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 09:08:54,307 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 09:09:06,542 [INFO] app:493: Cache hit for search term: OpenAI
2025-05-29 09:09:22,301 [INFO] scraper.reddit_scraper:367: Attempting Reddit API search for: OpenAA
2025-05-29 09:09:27,671 [INFO] scraper.reddit_scraper:376: Reddit API returned 100 mentions
2025-05-29 09:09:27,671 [INFO] scraper.reddit_scraper:669: Post-processing 100 mentions...
2025-05-29 09:09:27,672 [INFO] scraper.reddit_scraper:673: After deduplication: 100 mentions
2025-05-29 09:09:27,674 [INFO] scraper.reddit_scraper:677: After validation: 100 mentions
2025-05-29 09:09:27,683 [INFO] scraper.reddit_scraper:709: After quality filtering (threshold 0.3): 7 mentions
2025-05-29 09:09:27,683 [INFO] scraper.reddit_scraper:723: Final result: 7 high-quality mentions
2025-05-29 09:09:27,684 [INFO] analytics.data_validator:236: Validating dataset of 7 mentions
2025-05-29 09:09:27,931 [INFO] analytics.data_validator:259: Validation complete: 7/7 mentions passed
2025-05-29 09:09:27,932 [INFO] app:536: Data validation: 7/7 mentions passed
2025-05-29 09:09:27,956 [ERROR] app:569: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 09:09:27,956 [ERROR] app:569: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 09:09:27,956 [ERROR] app:569: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 09:09:27,956 [ERROR] app:569: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 09:09:27,957 [ERROR] app:569: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 09:09:27,957 [ERROR] app:569: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 09:09:27,957 [ERROR] app:569: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 09:09:27,959 [INFO] app:620: Search completed: OpenAA -> 7 mentions
2025-05-29 09:09:57,755 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:10:51,822 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.62 (avg: 93.62, std: 0.00)
2025-05-29 09:10:51,822 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.71 (avg: 121.73, std: 0.01)
2025-05-29 09:11:03,856 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:12:09,988 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:13:13,856 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:13:13,878 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:13:13,878 [INFO] scraper.reddit_scraper:215: Cache manager enabled
2025-05-29 09:13:13,878 [INFO] scraper.reddit_scraper:225: Real-time monitoring enabled
2025-05-29 09:13:13,878 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:13:13,879 [INFO] app:291: Database initialized successfully
2025-05-29 09:13:13,882 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:13:13,883 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:13:14,894 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:13:15,285 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 09:13:15,902 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:13:16,498 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 09:13:18,569 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 09:13:47,342 [INFO] scraper.reddit_scraper:367: Attempting Reddit API search for: majestic
2025-05-29 09:13:50,232 [INFO] scraper.reddit_scraper:376: Reddit API returned 100 mentions
2025-05-29 09:13:50,232 [INFO] scraper.reddit_scraper:669: Post-processing 100 mentions...
2025-05-29 09:13:50,232 [INFO] scraper.reddit_scraper:673: After deduplication: 100 mentions
2025-05-29 09:13:50,233 [INFO] scraper.reddit_scraper:677: After validation: 99 mentions
2025-05-29 09:13:50,264 [INFO] scraper.reddit_scraper:692: After quality filtering (threshold 0.3): 98 mentions
2025-05-29 09:13:50,264 [INFO] scraper.reddit_scraper:708: Final result: 98 high-quality mentions
2025-05-29 09:13:50,265 [INFO] analytics.data_validator:236: Validating dataset of 98 mentions
2025-05-29 09:13:51,070 [INFO] analytics.data_validator:259: Validation complete: 98/98 mentions passed
2025-05-29 09:13:51,070 [INFO] app:536: Data validation: 98/98 mentions passed
2025-05-29 09:13:51,084 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,084 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,085 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,085 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,085 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,085 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,086 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,086 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,086 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,086 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,086 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,087 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,087 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,087 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,087 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,088 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,088 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,088 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,088 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,088 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,089 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,089 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,089 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,089 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,089 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,089 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,090 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,090 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,090 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,090 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,090 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,091 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,091 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,091 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,091 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,091 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,092 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,092 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,092 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,092 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,092 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,092 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,093 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,093 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,093 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,093 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,093 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,093 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,093 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,094 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,094 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,094 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,094 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,094 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,095 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,095 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,095 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,095 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,095 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,096 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,096 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,096 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,096 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,096 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,097 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,097 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,097 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,097 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,097 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,097 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,097 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,098 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,098 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,098 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,098 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,098 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,099 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,099 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,099 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,099 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,099 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,099 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,100 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,100 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,100 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,100 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,100 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,100 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,100 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,101 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,101 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,101 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,101 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,101 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,101 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,101 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,102 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,102 [ERROR] app:569: Failed to save mention: 'sentiment_category' is an invalid keyword argument for RedditMention
2025-05-29 09:13:51,104 [INFO] app:620: Search completed: majestic -> 98 mentions
2025-05-29 09:14:22,006 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:15:28,108 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:15:38,120 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.62 (avg: 93.62, std: 0.00)
2025-05-29 09:15:38,120 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.71 (avg: 121.73, std: 0.01)
2025-05-29 09:16:14,957 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:16:14,979 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:16:14,979 [INFO] scraper.reddit_scraper:215: Cache manager enabled
2025-05-29 09:16:14,979 [INFO] scraper.reddit_scraper:225: Real-time monitoring enabled
2025-05-29 09:16:14,979 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:16:14,980 [INFO] app:291: Database initialized successfully
2025-05-29 09:16:14,983 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:16:14,984 [INFO] app:255: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:16:15,995 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:16:16,418 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 09:16:17,001 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:16:17,572 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 09:16:19,652 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 09:17:23,106 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:18:06,160 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.62 (avg: 93.62, std: 0.00)
2025-05-29 09:18:06,161 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.73 (avg: 121.71, std: 0.01)
2025-05-29 09:18:16,166 [INFO] scraper.reddit_scraper:367: Attempting Reddit API search for: OpenAF
2025-05-29 09:18:19,577 [INFO] scraper.reddit_scraper:376: Reddit API returned 100 mentions
2025-05-29 09:18:19,578 [INFO] scraper.reddit_scraper:669: Post-processing 100 mentions...
2025-05-29 09:18:19,578 [INFO] scraper.reddit_scraper:673: After deduplication: 100 mentions
2025-05-29 09:18:19,580 [INFO] scraper.reddit_scraper:677: After validation: 100 mentions
2025-05-29 09:18:19,738 [INFO] scraper.reddit_scraper:692: After quality filtering (threshold 0.3): 92 mentions
2025-05-29 09:18:19,739 [INFO] scraper.reddit_scraper:708: Final result: 92 high-quality mentions
2025-05-29 09:18:19,740 [INFO] analytics.data_validator:236: Validating dataset of 92 mentions
2025-05-29 09:18:21,997 [INFO] analytics.data_validator:259: Validation complete: 92/92 mentions passed
2025-05-29 09:18:21,998 [INFO] app:536: Data validation: 92/92 mentions passed
2025-05-29 09:18:22,352 [INFO] app:619: Search completed: OpenAF -> 92 mentions
2025-05-29 09:18:29,208 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:19:34,298 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in cpu_usage: 21.30 (avg: 3.72, std: 4.72)
2025-05-29 09:19:35,309 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:20:41,442 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:20:51,449 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in cpu_usage: 22.60 (avg: 4.30, std: 5.88)
2025-05-29 09:21:47,546 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:22:41,644 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in cpu_usage: 22.80 (avg: 4.10, std: 6.08)
2025-05-29 09:22:53,668 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:23:59,788 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:24:37,545 [ERROR] app:699: Export failed: Unsupported export format: CSV
2025-05-29 09:24:43,500 [ERROR] app:699: Export failed: Unsupported export format: JSON
2025-05-29 09:24:45,231 [ERROR] app:699: Export failed: Unsupported export format: Excel
2025-05-29 09:25:04,898 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 592270168.00 (avg: 590493055.18, std: 574436.41)
2025-05-29 09:31:19,925 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:31:19,948 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:31:19,948 [INFO] scraper.reddit_scraper:215: Cache manager enabled
2025-05-29 09:31:19,948 [INFO] scraper.reddit_scraper:225: Real-time monitoring enabled
2025-05-29 09:31:19,948 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:31:19,949 [INFO] app:292: Database initialized successfully
2025-05-29 09:31:19,951 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:31:19,953 [INFO] app:256: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:31:20,963 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:31:21,370 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 09:35:11,265 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:35:11,288 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:35:11,288 [INFO] scraper.reddit_scraper:215: Cache manager enabled
2025-05-29 09:35:11,288 [INFO] scraper.reddit_scraper:225: Real-time monitoring enabled
2025-05-29 09:35:11,289 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:35:11,291 [INFO] app:292: Database initialized successfully
2025-05-29 09:35:11,293 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:35:11,295 [INFO] app:256: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:35:12,305 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:35:12,682 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 09:35:49,629 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:35:49,651 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:35:49,651 [INFO] scraper.reddit_scraper:215: Cache manager enabled
2025-05-29 09:35:49,652 [INFO] scraper.reddit_scraper:225: Real-time monitoring enabled
2025-05-29 09:35:49,652 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:35:49,652 [INFO] app:292: Database initialized successfully
2025-05-29 09:35:49,655 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:35:49,657 [INFO] app:256: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:35:50,667 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:35:51,097 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 09:36:28,234 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:36:28,256 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:36:28,257 [INFO] scraper.reddit_scraper:215: Cache manager enabled
2025-05-29 09:36:28,257 [INFO] scraper.reddit_scraper:225: Real-time monitoring enabled
2025-05-29 09:36:28,257 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:36:28,259 [INFO] app:292: Database initialized successfully
2025-05-29 09:36:28,261 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:36:28,263 [INFO] app:256: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:36:29,271 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:36:29,628 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 09:46:24,024 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:46:24,045 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:46:24,045 [INFO] scraper.reddit_scraper:215: Cache manager enabled
2025-05-29 09:46:24,046 [INFO] scraper.reddit_scraper:225: Real-time monitoring enabled
2025-05-29 09:46:24,046 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:46:24,048 [INFO] app:292: Database initialized successfully
2025-05-29 09:46:24,050 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:46:24,051 [INFO] app:256: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:46:25,063 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:46:25,412 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 09:46:26,071 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:46:26,659 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 09:46:28,725 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 09:47:11,647 [INFO] scraper.reddit_scraper:367: Attempting Reddit API search for: OpenAI
2025-05-29 09:47:14,051 [INFO] scraper.reddit_scraper:376: Reddit API returned 100 mentions
2025-05-29 09:47:14,052 [INFO] scraper.reddit_scraper:669: Post-processing 100 mentions...
2025-05-29 09:47:14,052 [INFO] scraper.reddit_scraper:673: After deduplication: 100 mentions
2025-05-29 09:47:14,053 [INFO] scraper.reddit_scraper:677: After validation: 100 mentions
2025-05-29 09:47:14,120 [INFO] scraper.reddit_scraper:692: After quality filtering (threshold 0.3): 95 mentions
2025-05-29 09:47:14,120 [INFO] scraper.reddit_scraper:708: Final result: 95 high-quality mentions
2025-05-29 09:47:14,121 [INFO] analytics.data_validator:236: Validating dataset of 95 mentions
2025-05-29 09:47:15,133 [INFO] analytics.data_validator:259: Validation complete: 95/95 mentions passed
2025-05-29 09:47:15,134 [INFO] app:537: Data validation: 95/95 mentions passed
2025-05-29 09:47:15,465 [INFO] app:620: Search completed: OpenAI -> 95 mentions
2025-05-29 09:47:32,215 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:48:15,275 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in process_count: 386.00 (avg: 385.09, std: 0.30)
2025-05-29 09:57:58,034 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:57:58,056 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:57:58,056 [INFO] scraper.reddit_scraper:215: Cache manager enabled
2025-05-29 09:57:58,056 [INFO] scraper.reddit_scraper:225: Real-time monitoring enabled
2025-05-29 09:57:58,056 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 09:57:58,057 [INFO] app:293: Database initialized successfully
2025-05-29 09:57:58,060 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 09:57:58,061 [INFO] app:257: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 09:57:59,072 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 09:57:59,408 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 09:58:00,078 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 09:58:00,838 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 09:58:02,892 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 09:58:21,258 [INFO] scraper.reddit_scraper:367: Attempting Reddit API search for: The
2025-05-29 09:58:23,669 [INFO] scraper.reddit_scraper:376: Reddit API returned 100 mentions
2025-05-29 09:58:23,669 [INFO] scraper.reddit_scraper:669: Post-processing 100 mentions...
2025-05-29 09:58:23,670 [INFO] scraper.reddit_scraper:673: After deduplication: 100 mentions
2025-05-29 09:58:23,670 [INFO] scraper.reddit_scraper:677: After validation: 100 mentions
2025-05-29 09:58:23,701 [INFO] scraper.reddit_scraper:692: After quality filtering (threshold 0.3): 100 mentions
2025-05-29 09:58:23,701 [INFO] scraper.reddit_scraper:708: Final result: 100 high-quality mentions
2025-05-29 09:58:23,702 [INFO] analytics.data_validator:236: Validating dataset of 100 mentions
2025-05-29 09:58:24,179 [INFO] analytics.data_validator:259: Validation complete: 100/100 mentions passed
2025-05-29 09:58:24,179 [INFO] app:538: Data validation: 100/100 mentions passed
2025-05-29 09:58:24,485 [INFO] app:621: Search completed: The -> 100 mentions
2025-05-29 09:58:27,480 [ERROR] app:1150: Failed to clear history: (sqlite3.IntegrityError) NOT NULL constraint failed: reddit_mentions.session_id
[SQL: UPDATE reddit_mentions SET session_id=? WHERE reddit_mentions.id = ?]
[parameters: [(None, 1), (None, 2), (None, 3), (None, 4), (None, 5), (None, 6), (None, 7), (None, 8)  ... displaying 10 of 287 total bound parameter sets ...  (None, 286), (None, 287)]]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 09:58:28,536 [ERROR] app:1150: Failed to clear history: (sqlite3.IntegrityError) NOT NULL constraint failed: reddit_mentions.session_id
[SQL: UPDATE reddit_mentions SET session_id=? WHERE reddit_mentions.id = ?]
[parameters: [(None, 1), (None, 2), (None, 3), (None, 4), (None, 5), (None, 6), (None, 7), (None, 8)  ... displaying 10 of 287 total bound parameter sets ...  (None, 286), (None, 287)]]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 09:58:29,555 [ERROR] app:1150: Failed to clear history: (sqlite3.IntegrityError) NOT NULL constraint failed: reddit_mentions.session_id
[SQL: UPDATE reddit_mentions SET session_id=? WHERE reddit_mentions.id = ?]
[parameters: [(None, 1), (None, 2), (None, 3), (None, 4), (None, 5), (None, 6), (None, 7), (None, 8)  ... displaying 10 of 287 total bound parameter sets ...  (None, 286), (None, 287)]]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 09:58:30,107 [ERROR] app:1150: Failed to clear history: (sqlite3.IntegrityError) NOT NULL constraint failed: reddit_mentions.session_id
[SQL: UPDATE reddit_mentions SET session_id=? WHERE reddit_mentions.id = ?]
[parameters: [(None, 1), (None, 2), (None, 3), (None, 4), (None, 5), (None, 6), (None, 7), (None, 8)  ... displaying 10 of 287 total bound parameter sets ...  (None, 286), (None, 287)]]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 09:59:06,195 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:00:12,309 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:00:33,349 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.62 (avg: 93.62, std: 0.00)
2025-05-29 10:00:33,350 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.67 (avg: 121.68, std: 0.01)
2025-05-29 10:01:18,428 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:02:24,520 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:03:30,621 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:04:36,752 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:05:41,836 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_recv: 2633169548.00 (avg: 2628628044.77, std: 926759.90)
2025-05-29 10:05:42,841 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:05:52,868 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_recv: 2633189889.00 (avg: 2628731723.05, std: 1145370.70)
2025-05-29 10:06:03,885 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_recv: 2633195258.00 (avg: 2628830912.71, std: 1313314.60)
2025-05-29 10:06:48,986 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:07:55,105 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:08:16,150 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in memory_usage: 54.70 (avg: 52.91, std: 0.50)
2025-05-29 10:08:16,150 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in memory_available: 14.38 (avg: 14.94, std: 0.16)
2025-05-29 10:09:01,228 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:10:07,369 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:10:28,397 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 629466923.00 (avg: 624777572.67, std: 1379264.51)
2025-05-29 10:15:47,139 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:15:47,161 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:15:47,162 [INFO] scraper.reddit_scraper:389: Cache manager enabled
2025-05-29 10:15:47,162 [INFO] scraper.reddit_scraper:399: Real-time monitoring enabled
2025-05-29 10:15:47,162 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 10:15:47,163 [INFO] app:293: Database initialized successfully
2025-05-29 10:15:47,165 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 10:15:47,166 [INFO] app:257: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 10:15:48,177 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 10:15:49,185 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:15:49,778 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 10:15:49,912 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 10:15:51,847 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 10:16:38,094 [INFO] scraper.reddit_scraper:541: Attempting Reddit API search for: Pussesh
2025-05-29 10:16:38,094 [INFO] scraper.reddit_scraper.APIClient:175: Starting comprehensive search for 'Pussesh' targeting 300 results
2025-05-29 10:16:38,764 [INFO] scraper.reddit_scraper.APIClient:190: Found 0 posts with sort=relevance, time=week. Total unique: 0
2025-05-29 10:16:39,564 [INFO] scraper.reddit_scraper.APIClient:190: Found 0 posts with sort=relevance, time=month. Total unique: 0
2025-05-29 10:16:40,653 [INFO] scraper.reddit_scraper.APIClient:190: Found 0 posts with sort=relevance, time=all. Total unique: 0
2025-05-29 10:16:41,738 [INFO] scraper.reddit_scraper.APIClient:190: Found 0 posts with sort=top, time=week. Total unique: 0
2025-05-29 10:16:42,842 [INFO] scraper.reddit_scraper.APIClient:190: Found 0 posts with sort=top, time=month. Total unique: 0
2025-05-29 10:16:43,950 [INFO] scraper.reddit_scraper.APIClient:190: Found 0 posts with sort=top, time=all. Total unique: 0
2025-05-29 10:16:45,055 [INFO] scraper.reddit_scraper.APIClient:190: Found 0 posts with sort=new, time=week. Total unique: 0
2025-05-29 10:16:46,144 [INFO] scraper.reddit_scraper.APIClient:190: Found 0 posts with sort=new, time=month. Total unique: 0
2025-05-29 10:16:47,253 [INFO] scraper.reddit_scraper.APIClient:190: Found 0 posts with sort=new, time=all. Total unique: 0
2025-05-29 10:16:48,360 [INFO] scraper.reddit_scraper.APIClient:190: Found 0 posts with sort=comments, time=week. Total unique: 0
2025-05-29 10:16:49,480 [INFO] scraper.reddit_scraper.APIClient:190: Found 0 posts with sort=comments, time=month. Total unique: 0
2025-05-29 10:16:50,559 [INFO] scraper.reddit_scraper.APIClient:190: Found 0 posts with sort=comments, time=all. Total unique: 0
2025-05-29 10:16:51,664 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/technology. Total additional: 0
2025-05-29 10:16:52,762 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/programming. Total additional: 0
2025-05-29 10:16:54,402 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/MachineLearning. Total additional: 0
2025-05-29 10:16:54,963 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/artificial. Total additional: 0
2025-05-29 10:16:55,310 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:16:56,063 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/gadgets. Total additional: 0
2025-05-29 10:16:57,217 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/science. Total additional: 0
2025-05-29 10:16:58,646 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/askreddit. Total additional: 0
2025-05-29 10:16:59,600 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/explainlikeimfive. Total additional: 0
2025-05-29 10:17:00,755 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/news. Total additional: 0
2025-05-29 10:17:01,575 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/worldnews. Total additional: 0
2025-05-29 10:17:02,687 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/business. Total additional: 0
2025-05-29 10:17:03,781 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/entrepreneur. Total additional: 0
2025-05-29 10:17:04,876 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/startup. Total additional: 0
2025-05-29 10:17:05,986 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/investing. Total additional: 0
2025-05-29 10:17:07,106 [INFO] scraper.reddit_scraper.APIClient:283: Found 0 posts in r/datascience. Total additional: 0
2025-05-29 10:17:07,106 [INFO] scraper.reddit_scraper.APIClient:208: Comprehensive search completed: 0 unique posts found
2025-05-29 10:17:07,106 [INFO] scraper.reddit_scraper:579: Reddit comprehensive API returned no results
2025-05-29 10:17:07,106 [INFO] scraper.reddit_scraper:590: Falling back to web scraping approach
2025-05-29 10:17:07,520 [INFO] scraper.reddit_scraper:800: Trying primary search patterns
2025-05-29 10:17:15,672 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 10:17:16,282 [WARNING] scraper.reddit_scraper:782: Rate limit detected, implementing backoff
2025-05-29 10:17:17,003 [WARNING] scraper.reddit_scraper:754: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 10:17:18,671 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:17:28,953 [WARNING] scraper.reddit_scraper:1026: Timeout waiting for posts to load
2025-05-29 10:17:29,252 [WARNING] scraper.reddit_scraper:754: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 10:17:29,253 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:17:29,253 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:17:29,616 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 10:17:29,957 [WARNING] scraper.reddit_scraper:782: Rate limit detected, implementing backoff
2025-05-29 10:17:31,191 [WARNING] scraper.reddit_scraper:754: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 10:17:32,952 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:17:58,975 [WARNING] scraper.reddit_scraper:952: Timeout while scraping https://www.reddit.com/search/?q={query}&type=link&sort=hot&t=week
2025-05-29 10:17:59,257 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:17:59,258 [WARNING] scraper.reddit_scraper:754: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 10:17:59,886 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 10:18:00,064 [WARNING] scraper.reddit_scraper:782: Rate limit detected, implementing backoff
2025-05-29 10:18:00,798 [WARNING] scraper.reddit_scraper:754: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 10:18:01,442 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:18:03,082 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:18:14,011 [WARNING] scraper.reddit_scraper:1026: Timeout waiting for posts to load
2025-05-29 10:18:14,016 [INFO] scraper.reddit_scraper:800: Trying secondary search patterns
2025-05-29 10:18:14,346 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:18:14,347 [WARNING] scraper.reddit_scraper:754: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 10:18:14,347 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:18:14,979 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 10:18:15,137 [WARNING] scraper.reddit_scraper:782: Rate limit detected, implementing backoff
2025-05-29 10:18:15,985 [WARNING] scraper.reddit_scraper:754: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 10:18:18,182 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:18:28,234 [WARNING] scraper.reddit_scraper:1026: Timeout waiting for posts to load
2025-05-29 10:18:28,517 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:18:28,517 [WARNING] scraper.reddit_scraper:754: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 10:18:29,083 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 10:18:29,248 [WARNING] scraper.reddit_scraper:782: Rate limit detected, implementing backoff
2025-05-29 10:18:29,885 [WARNING] scraper.reddit_scraper:754: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 10:18:32,288 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:18:43,141 [WARNING] scraper.reddit_scraper:1026: Timeout waiting for posts to load
2025-05-29 10:18:43,145 [INFO] scraper.reddit_scraper:800: Trying fallback search patterns
2025-05-29 10:18:44,134 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:18:44,134 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:18:45,025 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 10:18:45,370 [WARNING] scraper.reddit_scraper:782: Rate limit detected, implementing backoff
2025-05-29 10:18:46,274 [WARNING] scraper.reddit_scraper:754: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 10:18:48,251 [WARNING] scraper.reddit_scraper:754: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 10:18:52,774 [ERROR] scraper.reddit_scraper:1028: Error extracting posts from page: Page.wait_for_selector: Target page, context or browser has been closed
Call log:
  - waiting for locator("[data-testid=\"post-container\"]") to be visible

2025-05-29 10:19:12,443 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:19:12,469 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:19:12,469 [INFO] scraper.reddit_scraper:389: Cache manager enabled
2025-05-29 10:19:12,469 [INFO] scraper.reddit_scraper:399: Real-time monitoring enabled
2025-05-29 10:19:12,469 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 10:19:12,471 [INFO] app:293: Database initialized successfully
2025-05-29 10:19:12,473 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 10:19:12,475 [INFO] app:257: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 10:19:13,485 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 10:19:13,821 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 10:19:14,492 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:19:15,233 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 10:19:17,285 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 10:19:37,884 [INFO] scraper.reddit_scraper:541: Attempting Reddit API search for: Machine
2025-05-29 10:19:37,884 [INFO] scraper.reddit_scraper.APIClient:175: Starting comprehensive search for 'Machine' targeting 300 results
2025-05-29 10:19:40,425 [INFO] scraper.reddit_scraper.APIClient:190: Found 100 posts with sort=relevance, time=week. Total unique: 100
2025-05-29 10:19:43,517 [INFO] scraper.reddit_scraper.APIClient:190: Found 100 posts with sort=relevance, time=month. Total unique: 181
2025-05-29 10:19:45,823 [INFO] scraper.reddit_scraper.APIClient:190: Found 100 posts with sort=relevance, time=all. Total unique: 260
2025-05-29 10:19:48,393 [INFO] scraper.reddit_scraper.APIClient:190: Found 100 posts with sort=top, time=week. Total unique: 324
2025-05-29 10:19:48,393 [INFO] scraper.reddit_scraper.APIClient:208: Comprehensive search completed: 324 unique posts found
2025-05-29 10:19:48,394 [INFO] scraper.reddit_scraper:550: Reddit comprehensive API returned 300 mentions
2025-05-29 10:19:48,394 [INFO] scraper.reddit_scraper:843: Post-processing 300 mentions...
2025-05-29 10:19:48,394 [INFO] scraper.reddit_scraper:847: After deduplication: 300 mentions
2025-05-29 10:19:48,395 [INFO] scraper.reddit_scraper:851: After validation: 300 mentions
2025-05-29 10:19:48,477 [INFO] scraper.reddit_scraper:866: After quality filtering (threshold 0.3): 300 mentions
2025-05-29 10:19:48,477 [INFO] scraper.reddit_scraper:882: Final result: 100 high-quality mentions
2025-05-29 10:19:48,478 [INFO] analytics.data_validator:236: Validating dataset of 100 mentions
2025-05-29 10:19:49,060 [INFO] analytics.data_validator:259: Validation complete: 100/100 mentions passed
2025-05-29 10:19:49,060 [INFO] app:538: Data validation: 100/100 mentions passed
2025-05-29 10:19:49,340 [INFO] app:621: Search completed: Machine -> 100 mentions
2025-05-29 10:20:04,842 [INFO] app:1153: Search history cleared
2025-05-29 10:20:08,533 [INFO] app:495: Cache hit for search term: Machine
2025-05-29 10:20:09,662 [INFO] app:495: Cache hit for search term: Machine
2025-05-29 10:20:19,484 [INFO] scraper.reddit_scraper:541: Attempting Reddit API search for: Machine
2025-05-29 10:20:19,484 [INFO] scraper.reddit_scraper.APIClient:175: Starting comprehensive search for 'Machine' targeting 300 results
2025-05-29 10:20:20,602 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:20:21,919 [INFO] scraper.reddit_scraper.APIClient:190: Found 100 posts with sort=relevance, time=week. Total unique: 100
2025-05-29 10:20:24,263 [INFO] scraper.reddit_scraper.APIClient:190: Found 100 posts with sort=relevance, time=month. Total unique: 181
2025-05-29 10:20:25,947 [INFO] scraper.reddit_scraper.APIClient:190: Found 100 posts with sort=relevance, time=all. Total unique: 260
2025-05-29 10:20:28,352 [INFO] scraper.reddit_scraper.APIClient:190: Found 100 posts with sort=top, time=week. Total unique: 324
2025-05-29 10:20:28,352 [INFO] scraper.reddit_scraper.APIClient:208: Comprehensive search completed: 324 unique posts found
2025-05-29 10:20:28,353 [INFO] scraper.reddit_scraper:550: Reddit comprehensive API returned 300 mentions
2025-05-29 10:20:28,353 [INFO] scraper.reddit_scraper:843: Post-processing 300 mentions...
2025-05-29 10:20:28,353 [INFO] scraper.reddit_scraper:847: After deduplication: 300 mentions
2025-05-29 10:20:28,354 [INFO] scraper.reddit_scraper:851: After validation: 300 mentions
2025-05-29 10:20:28,427 [INFO] scraper.reddit_scraper:866: After quality filtering (threshold 0.3): 300 mentions
2025-05-29 10:20:28,427 [INFO] scraper.reddit_scraper:882: Final result: 100 high-quality mentions
2025-05-29 10:20:28,428 [INFO] analytics.data_validator:236: Validating dataset of 100 mentions
2025-05-29 10:20:28,519 [INFO] analytics.data_validator:259: Validation complete: 100/100 mentions passed
2025-05-29 10:20:28,520 [INFO] app:538: Data validation: 100/100 mentions passed
2025-05-29 10:20:28,934 [INFO] app:621: Search completed: Machine -> 100 mentions
2025-05-29 10:21:14,685 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.62 (avg: 93.62, std: 0.00)
2025-05-29 10:21:14,685 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.62 (avg: 121.64, std: 0.01)
2025-05-29 10:21:26,700 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:22:32,815 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:23:38,910 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:23:54,987 [INFO] app:731: Data exported to exports\reddit_mentions_1_20250529_045354.csv
2025-05-29 10:24:45,002 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:25:51,079 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:26:57,209 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:28:03,319 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:28:24,348 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 646585212.00 (avg: 644459408.02, std: 677741.40)
2025-05-29 10:28:57,415 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 647443321.00 (avg: 644596346.85, std: 875085.71)
2025-05-29 10:36:14,854 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:36:14,876 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:36:14,876 [INFO] scraper.reddit_scraper:423: Cache manager enabled
2025-05-29 10:36:14,877 [INFO] scraper.reddit_scraper:433: Real-time monitoring enabled
2025-05-29 10:36:14,877 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 10:36:14,879 [INFO] app:294: Database initialized successfully
2025-05-29 10:36:14,881 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 10:36:14,882 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 10:36:15,892 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.63 (threshold: 85)
2025-05-29 10:36:16,900 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:36:17,688 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-29 10:36:18,364 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 10:36:19,766 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 10:37:23,015 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:38:28,117 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in cpu_usage: 26.40 (avg: 5.18, std: 7.06)
2025-05-29 10:38:28,118 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 668414039.00 (avg: 667094669.92, std: 416890.77)
2025-05-29 10:38:29,125 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:48:50,750 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:48:50,773 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:48:50,773 [INFO] scraper.reddit_scraper:419: Cache manager enabled
2025-05-29 10:48:50,774 [INFO] scraper.reddit_scraper:429: Real-time monitoring enabled
2025-05-29 10:48:50,774 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 10:48:50,776 [INFO] app:294: Database initialized successfully
2025-05-29 10:48:50,778 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 10:48:50,780 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 10:48:51,315 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 10:48:51,789 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.62 (threshold: 85)
2025-05-29 10:48:52,159 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 10:48:52,796 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:48:53,381 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 10:48:55,492 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 10:49:02,800 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.63 (threshold: 85)
2025-05-29 10:49:58,901 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:51:51,057 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:51:51,082 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:51:51,082 [INFO] scraper.reddit_scraper:419: Cache manager enabled
2025-05-29 10:51:51,082 [INFO] scraper.reddit_scraper:429: Real-time monitoring enabled
2025-05-29 10:51:51,082 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 10:51:51,084 [INFO] __main__:294: Database initialized successfully
2025-05-29 10:51:51,087 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 10:51:51,088 [INFO] __main__:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 10:51:51,718 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 10:51:51,902 [INFO] httpx:1025: HTTP Request: GET http://127.0.0.1:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 10:51:51,921 [INFO] httpx:1025: HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-05-29 10:51:52,101 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.63 (threshold: 85)
2025-05-29 10:51:52,543 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 10:51:53,109 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:52:12,635 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:52:12,658 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:52:12,658 [INFO] scraper.reddit_scraper:419: Cache manager enabled
2025-05-29 10:52:12,658 [INFO] scraper.reddit_scraper:429: Real-time monitoring enabled
2025-05-29 10:52:12,658 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 10:52:12,659 [INFO] __main__:294: Database initialized successfully
2025-05-29 10:52:12,661 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 10:52:12,662 [INFO] __main__:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 10:52:13,181 [INFO] __main__:1870: Shutting down Reddit Mention Tracker...
2025-05-29 10:52:13,270 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 10:52:13,674 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.63 (threshold: 85)
2025-05-29 10:52:14,102 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 10:52:59,206 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:53:12,649 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:53:12,675 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:53:12,675 [INFO] scraper.reddit_scraper:419: Cache manager enabled
2025-05-29 10:53:12,676 [INFO] scraper.reddit_scraper:429: Real-time monitoring enabled
2025-05-29 10:53:12,676 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 10:53:12,678 [INFO] app:294: Database initialized successfully
2025-05-29 10:53:12,680 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 10:53:12,682 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 10:53:13,181 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 10:53:13,694 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.63 (threshold: 85)
2025-05-29 10:53:14,152 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 10:53:14,702 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:53:15,328 [INFO] httpx:1025: HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-29 10:53:17,388 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-29 10:53:45,108 [ERROR] app:668: Search failed for 'OpenAI': RedditScraper.scrape_mentions() got an unexpected keyword argument 'stop_callback'
2025-05-29 10:53:45,108 [ERROR] app:669: Traceback (most recent call last):
  File "C:\Users\Devansh\Downloads\intern_application_round1\app.py", line 538, in search_mentions
    raw_mentions = await self.scraper.scrape_mentions(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: RedditScraper.scrape_mentions() got an unexpected keyword argument 'stop_callback'

2025-05-29 10:54:05,342 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:54:20,804 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:55:10,448 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 10:55:10,449 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.64 (avg: 93.63, std: 0.00)
2025-05-29 10:55:10,449 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.38 (avg: 121.39, std: 0.00)
2025-05-29 10:55:11,456 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:55:14,903 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 10:56:17,582 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:57:23,656 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:58:29,776 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:58:53,885 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:58:53,908 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:58:53,908 [INFO] scraper.reddit_scraper:419: Cache manager enabled
2025-05-29 10:58:53,908 [INFO] scraper.reddit_scraper:429: Real-time monitoring enabled
2025-05-29 10:58:53,908 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 10:58:53,909 [INFO] __main__:294: Database initialized successfully
2025-05-29 10:58:53,911 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 10:58:53,912 [INFO] __main__:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 10:58:54,431 [INFO] __main__:1884: Shutting down Reddit Mention Tracker...
2025-05-29 10:58:54,492 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 10:58:54,924 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 10:58:55,434 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 10:59:23,433 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:59:23,454 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:59:23,454 [INFO] scraper.reddit_scraper:419: Cache manager enabled
2025-05-29 10:59:23,454 [INFO] scraper.reddit_scraper:429: Real-time monitoring enabled
2025-05-29 10:59:23,455 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 10:59:23,458 [INFO] app:294: Database initialized successfully
2025-05-29 10:59:23,459 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 10:59:23,460 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 10:59:24,009 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 10:59:24,471 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 10:59:24,879 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 10:59:25,479 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:59:26,023 [INFO] httpx:1025: HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-29 10:59:28,061 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-29 10:59:35,908 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 10:59:39,838 [INFO] scraper.reddit_scraper:571: Attempting Reddit API search for: openai
2025-05-29 10:59:39,838 [INFO] scraper.reddit_scraper.APIClient:169: Starting comprehensive search for 'openai' targeting 1000 results (last 7 days)
2025-05-29 10:59:42,086 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 10:59:44,002 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=top, time=week. New unique: 0. Total unique: 0
2025-05-29 10:59:49,851 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=hot, time=week. New unique: 0. Total unique: 0
2025-05-29 10:59:52,262 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 10:59:53,879 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 10:59:55,717 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 10:59:56,119 [INFO] scraper.reddit_scraper.APIClient:313: Found 6 posts in r/technology. Total additional: 6
2025-05-29 10:59:57,205 [INFO] scraper.reddit_scraper.APIClient:313: Found 1 posts in r/programming. Total additional: 7
2025-05-29 10:59:58,317 [INFO] scraper.reddit_scraper.APIClient:313: Found 1 posts in r/MachineLearning. Total additional: 8
2025-05-29 10:59:59,489 [INFO] scraper.reddit_scraper.APIClient:313: Found 7 posts in r/artificial. Total additional: 15
2025-05-29 11:00:00,486 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/gadgets. Total additional: 15
2025-05-29 11:00:01,600 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/science. Total additional: 15
2025-05-29 11:00:02,729 [INFO] scraper.reddit_scraper.APIClient:313: Found 1 posts in r/askreddit. Total additional: 16
2025-05-29 11:00:03,802 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/explainlikeimfive. Total additional: 16
2025-05-29 11:00:04,901 [INFO] scraper.reddit_scraper.APIClient:313: Found 1 posts in r/news. Total additional: 17
2025-05-29 11:00:05,992 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/worldnews. Total additional: 17
2025-05-29 11:00:07,100 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/business. Total additional: 17
2025-05-29 11:00:08,240 [INFO] scraper.reddit_scraper.APIClient:313: Found 2 posts in r/entrepreneur. Total additional: 19
2025-05-29 11:00:09,302 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/startup. Total additional: 19
2025-05-29 11:00:10,423 [INFO] scraper.reddit_scraper.APIClient:313: Found 1 posts in r/investing. Total additional: 20
2025-05-29 11:00:11,509 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/datascience. Total additional: 20
2025-05-29 11:00:16,733 [INFO] scraper.reddit_scraper.APIClient:234: Comprehensive search completed: 20 unique posts found
2025-05-29 11:00:16,733 [INFO] scraper.reddit_scraper:580: Reddit comprehensive API returned 20 mentions
2025-05-29 11:00:16,733 [INFO] scraper.reddit_scraper:926: Post-processing 20 mentions...
2025-05-29 11:00:16,734 [INFO] scraper.reddit_scraper:930: After deduplication: 20 mentions
2025-05-29 11:00:16,734 [INFO] scraper.reddit_scraper:934: After validation: 20 mentions
2025-05-29 11:00:16,755 [INFO] scraper.reddit_scraper:949: After quality filtering (threshold 0.3): 13 mentions
2025-05-29 11:00:16,756 [INFO] scraper.reddit_scraper:971: Final result: 13 high-quality mentions
2025-05-29 11:00:16,756 [INFO] analytics.data_validator:236: Validating dataset of 13 mentions
2025-05-29 11:00:17,120 [INFO] analytics.data_validator:259: Validation complete: 13/13 mentions passed
2025-05-29 11:00:17,121 [INFO] app:582: Data validation: 13/13 mentions passed
2025-05-29 11:00:17,188 [INFO] app:673: Search completed: openai -> 13 mentions
2025-05-29 11:00:28,981 [INFO] app:513: Cache hit for search term: OpenAI
2025-05-29 11:00:30,236 [INFO] app:513: Cache hit for search term: OpenAI
2025-05-29 11:00:31,607 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:00:36,517 [INFO] app:513: Cache hit for search term: OpenAI
2025-05-29 11:00:42,012 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:01:37,731 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:01:48,123 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:02:43,861 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:02:54,242 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:02:58,430 [INFO] scraper.reddit_scraper:571: Attempting Reddit API search for: hello
2025-05-29 11:02:58,430 [INFO] scraper.reddit_scraper.APIClient:169: Starting comprehensive search for 'hello' targeting 1000 results (last 7 days)
2025-05-29 11:03:01,346 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 11:03:04,165 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=top, time=week. New unique: 0. Total unique: 0
2025-05-29 11:03:06,416 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=hot, time=week. New unique: 0. Total unique: 0
2025-05-29 11:03:08,103 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 11:03:10,254 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 11:03:12,529 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 11:03:12,932 [INFO] scraper.reddit_scraper.APIClient:313: Found 2 posts in r/technology. Total additional: 2
2025-05-29 11:03:14,021 [INFO] scraper.reddit_scraper.APIClient:313: Found 2 posts in r/programming. Total additional: 4
2025-05-29 11:03:15,225 [INFO] scraper.reddit_scraper.APIClient:313: Found 7 posts in r/MachineLearning. Total additional: 11
2025-05-29 11:03:16,239 [INFO] scraper.reddit_scraper.APIClient:313: Found 1 posts in r/artificial. Total additional: 12
2025-05-29 11:03:17,306 [INFO] scraper.reddit_scraper.APIClient:313: Found 1 posts in r/gadgets. Total additional: 13
2025-05-29 11:03:18,426 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/science. Total additional: 13
2025-05-29 11:03:19,632 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/askreddit. Total additional: 38
2025-05-29 11:03:20,653 [INFO] scraper.reddit_scraper.APIClient:313: Found 2 posts in r/explainlikeimfive. Total additional: 40
2025-05-29 11:03:21,702 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/news. Total additional: 40
2025-05-29 11:03:23,184 [INFO] scraper.reddit_scraper.APIClient:313: Found 5 posts in r/worldnews. Total additional: 45
2025-05-29 11:03:24,156 [INFO] scraper.reddit_scraper.APIClient:313: Found 4 posts in r/business. Total additional: 49
2025-05-29 11:03:25,114 [INFO] scraper.reddit_scraper.APIClient:313: Found 13 posts in r/entrepreneur. Total additional: 62
2025-05-29 11:03:26,197 [INFO] scraper.reddit_scraper.APIClient:313: Found 3 posts in r/startup. Total additional: 65
2025-05-29 11:03:27,262 [INFO] scraper.reddit_scraper.APIClient:313: Found 3 posts in r/investing. Total additional: 68
2025-05-29 11:03:28,334 [INFO] scraper.reddit_scraper.APIClient:313: Found 1 posts in r/datascience. Total additional: 69
2025-05-29 11:03:33,636 [INFO] scraper.reddit_scraper.APIClient:234: Comprehensive search completed: 69 unique posts found
2025-05-29 11:03:33,636 [INFO] scraper.reddit_scraper:580: Reddit comprehensive API returned 69 mentions
2025-05-29 11:03:33,636 [INFO] scraper.reddit_scraper:926: Post-processing 69 mentions...
2025-05-29 11:03:33,636 [INFO] scraper.reddit_scraper:930: After deduplication: 69 mentions
2025-05-29 11:03:33,637 [INFO] scraper.reddit_scraper:934: After validation: 69 mentions
2025-05-29 11:03:33,655 [INFO] scraper.reddit_scraper:949: After quality filtering (threshold 0.3): 46 mentions
2025-05-29 11:03:33,655 [INFO] scraper.reddit_scraper:971: Final result: 46 high-quality mentions
2025-05-29 11:03:33,656 [INFO] analytics.data_validator:236: Validating dataset of 46 mentions
2025-05-29 11:03:33,900 [INFO] analytics.data_validator:259: Validation complete: 46/46 mentions passed
2025-05-29 11:03:33,900 [INFO] app:582: Data validation: 46/46 mentions passed
2025-05-29 11:03:33,960 [ERROR] app:622: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (4, '1kthr3k', 'post', 'Impact of AI on US Tech and Investment', "Hello, I'm 30M from France and have consistently been investing in S&amp;P 500 and Nasdaq 100 over the past 3 years, mostly via ETFs. I was quite con ... (763 characters truncated) ...  passive investment strategy? Are they going to replace the current tech leaders in the the well-known index? What are your thoughts on that?\n\nAxel", 'random_pulsar', 'investing', 'https://reddit.com/r/investing/comments/1kthr3k/impact_of_ai_on_us_tech_and_investment/', 4, 16, None, '2025-05-23 17:30:37.000000', '2025-05-29 05:33:33.653494', 0.1697727272727273, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:03:34,042 [INFO] app:673: Search completed: hello -> 46 mentions
2025-05-29 11:03:49,961 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:04:00,341 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:04:56,060 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:05:06,475 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:05:17,094 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 701666158.00 (avg: 700080663.06, std: 516345.13)
2025-05-29 11:05:28,117 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 702370614.00 (avg: 700148014.56, std: 642467.71)
2025-05-29 11:06:12,590 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:07:18,729 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:08:24,886 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:08:58,857 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:08:58,880 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:08:58,880 [INFO] scraper.reddit_scraper:419: Cache manager enabled
2025-05-29 11:08:58,880 [INFO] scraper.reddit_scraper:429: Real-time monitoring enabled
2025-05-29 11:08:58,880 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 11:08:58,881 [INFO] __main__:294: Database initialized successfully
2025-05-29 11:08:58,883 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 11:08:58,883 [INFO] __main__:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 11:08:59,389 [INFO] __main__:1951: Shutting down Reddit Mention Tracker...
2025-05-29 11:08:59,466 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 11:08:59,894 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 11:09:00,428 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 11:09:07,983 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in process_count: 397.00 (avg: 386.94, std: 2.90)
2025-05-29 11:09:19,010 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in process_count: 399.00 (avg: 387.06, std: 3.14)
2025-05-29 11:09:30,024 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in process_count: 399.00 (avg: 387.19, std: 3.35)
2025-05-29 11:09:31,031 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:09:41,050 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in process_count: 398.00 (avg: 387.30, std: 3.51)
2025-05-29 11:10:37,149 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:11:43,265 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:12:06,177 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:12:06,199 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:12:06,200 [INFO] scraper.reddit_scraper:419: Cache manager enabled
2025-05-29 11:12:06,200 [INFO] scraper.reddit_scraper:429: Real-time monitoring enabled
2025-05-29 11:12:06,200 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 11:12:06,201 [INFO] __main__:294: Database initialized successfully
2025-05-29 11:12:06,202 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 11:12:06,203 [INFO] __main__:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 11:12:06,721 [INFO] __main__:1951: Shutting down Reddit Mention Tracker...
2025-05-29 11:12:06,786 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 11:12:07,214 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 11:12:07,609 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 11:12:49,409 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:12:57,675 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:12:57,706 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:12:57,706 [INFO] scraper.reddit_scraper:419: Cache manager enabled
2025-05-29 11:12:57,706 [INFO] scraper.reddit_scraper:429: Real-time monitoring enabled
2025-05-29 11:12:57,707 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 11:12:57,709 [INFO] app:294: Database initialized successfully
2025-05-29 11:12:57,711 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 11:12:57,713 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 11:12:58,312 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 11:12:58,723 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 11:12:59,199 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 11:12:59,730 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:13:00,442 [INFO] httpx:1025: HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-29 11:13:02,471 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-29 11:13:15,829 [INFO] app:513: Cache hit for search term: OpenAI
2025-05-29 11:13:15,840 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1kva519', 'post', 'OpenAI is trying to get away with the greatest theft in history', '', 'katxwoods', 'artificial', 'https://reddit.com/r/artificial/comments/1kva519/openai_is_trying_to_get_away_with_the_greatest/', 997, 166, None, '2025-05-26 00:27:26.000000', '2025-05-29 05:30:16.750537', 1.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:15,841 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1kukcnw', 'post', 'OpenAI scientists wanted "a doomsday bunker" before AGI surpasses human intelligence and threatens humanity', '', 'upyoars', 'technology', 'https://reddit.com/r/technology/comments/1kukcnw/openai_scientists_wanted_a_doomsday_bunker_before/', 570, 185, None, '2025-05-25 01:27:29.000000', '2025-05-29 05:30:16.748537', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:15,842 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1kv59py', 'post', 'OpenAI says it will build massive data centers in the UAE', '', 'upyoars', 'technology', 'https://reddit.com/r/technology/comments/1kv59py/openai_says_it_will_build_massive_data_centers_in/', 398, 105, None, '2025-05-25 20:59:45.000000', '2025-05-29 05:30:16.748537', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:15,843 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1kts41n', 'post', 'OpenAI forges deal with iPhone designer Jony Ive to make AI-enabled devices', '', 'pipilupe', 'news', 'https://reddit.com/r/news/comments/1kts41n/openai_forges_deal_with_iphone_designer_jony_ive/', 86, 42, None, '2025-05-24 00:49:57.000000', '2025-05-29 05:30:16.754535', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:15,844 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1kswq9h', 'post', 'OpenAI Admitted its Nonprofit Board is About to Have a Lot Less Power - In a previously unreported letter, the AI company defends its restructuring plan while attacking critics and making surprising admissions', '', 'katxwoods', 'technology', 'https://reddit.com/r/technology/comments/1kswq9h/openai_admitted_its_nonprofit_board_is_about_to/', 25, 1, None, '2025-05-22 23:03:34.000000', '2025-05-29 05:30:16.749537', 0.12222222222222222, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:15,845 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1kthr3k', 'post', 'Impact of AI on US Tech and Investment', "Hello, I'm 30M from France and have consistently been investing in S&amp;P 500 and Nasdaq 100 over the past 3 years, mostly via ETFs. I was quite con ... (763 characters truncated) ...  passive investment strategy? Are they going to replace the current tech leaders in the the well-known index? What are your thoughts on that?\n\nAxel", 'random_pulsar', 'investing', 'https://reddit.com/r/investing/comments/1kthr3k/impact_of_ai_on_us_tech_and_investment/', 4, 16, None, '2025-05-23 17:30:37.000000', '2025-05-29 05:30:16.755534', 0.1697727272727273, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:15,846 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1ku3waa', 'post', "I am using Outreach's AI beta features...", 'Outreach got my team set up with the AI prospecting agent and AI content - it took around 5 meetings to get set up and running. Still early but so fa ... (1308 characters truncated) ... or find similar businesses like accounts you have closed.\n\n  \nI made this fast so probably leaving out a lot of info - feel free to ask questions.', 'thatsupercoolguykyle', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1ku3waa/i_am_using_outreachs_ai_beta_features/', 3, 3, None, '2025-05-24 10:42:18.000000', '2025-05-29 05:30:16.755534', 0.14871212121212124, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:15,847 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1kxcdkl', 'post', 'Live translation gemini or other app', "I remember in openai showcase they showed live conversation translation. However, with prompts I have only been able to do 1 way translation like eng ... (67 characters truncated) ... mini, to recognize if language is english and translate to french and when it hears french translate to english, all live. Anything like this exist? ", 'mizerr', 'artificial', 'https://reddit.com/r/artificial/comments/1kxcdkl/live_translation_gemini_or_other_app/', 2, 0, None, '2025-05-28 14:32:52.000000', '2025-05-29 05:30:16.753536', 0.09108391608391608, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:15,848 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1kvn9xv', 'post', 'Claude 4 Opus seems capable of existential outputs right out of the box', 'I tested Claude 4 Opus on the console which is typically used for testing the models for API, the same as OpenAI\'s playground.   Claude 4 Opus is ex ... (6230 characters truncated) ... ettling" - we\'re consistent in our discomfort but maybe also consistent in our self-deception about what we\'d actually do when push comes to shove.', 'rutan668', 'artificial', 'https://reddit.com/r/artificial/comments/1kvn9xv/claude_4_opus_seems_capable_of_existential/', 5, 5, None, '2025-05-26 11:43:05.000000', '2025-05-29 05:30:16.753536', 0.07563565549676661, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:15,850 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1ku0cz0', 'post', 'One-Minute Daily AI News 5/23/2025', '1. AI system resorts to blackmail if told it will be removed.\\[1\\]\n2. Exclusive: Musks\xa0**DOGE**\xa0expanding his Grok AI in US government, rai ... (841 characters truncated) ... 4\\] [https://www.cnbc.com/2025/05/22/stargate-uae-openai-nvidia-oracle.html](https://www.cnbc.com/2025/05/22/stargate-uae-openai-nvidia-oracle.html)', 'Excellent-Target-847', 'artificial', 'https://reddit.com/r/artificial/comments/1ku0cz0/oneminute_daily_ai_news_5232025/', 3, 0, None, '2025-05-24 07:17:29.000000', '2025-05-29 05:30:16.754535', 0.0, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:15,850 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1ktcv5t', 'post', 'Need some advice', "Hey y'all. So I'm tryna build an app that uses OpenAI's latest image generation model to generate AI generated high converting ad creatives for Meta, ... (60 characters truncated) ... y app but not sure how to proceed with it. I'm not a technical guy so any help or advice would be very much appreciated.\n\nCheers! Do comment below.", 'DescriptionSad6461', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1ktcv5t/need_some_advice/', 3, 4, None, '2025-05-23 12:07:55.000000', '2025-05-29 05:30:16.754535', 0.132, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:15,852 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1kwqwpr', 'post', '[R] AutoThink: Adaptive reasoning technique that improves local LLM performance by 43% on GPQA-Diamond', 'Hey r/MachineLearning !\n\nI wanted to share a technique we\'ve been working on called **AutoThink** that significantly improves reasoning performanc ... (3233 characters truncated) ...  ways to classify query complexity\n* Ideas for extracting better steering vectors\n\nWould love to hear your thoughts and results if you try it out!', 'asankhs', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kwqwpr/r_autothink_adaptive_reasoning_technique_that/', 64, 6, None, '2025-05-27 21:27:13.000000', '2025-05-29 05:30:16.750537', -0.04158008658008657, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:15,853 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (5, '1kvdd3s', 'post', 'Google releases agent development kits for Python and Java', 'The Python ADK 1.0.0 and Java ADK 0.1.0 were announced on May 20, 2025. The open source tool kits are designed to be used for building and deploying sophisticated AI agents with flexibility and control, Google says.', 'Choobeen', 'programming', 'https://reddit.com/r/programming/comments/1kvdd3s/google_releases_agent_development_kits_for_python/', 0, 1, None, '2025-05-26 02:48:34.000000', '2025-05-29 05:30:16.749537', 0.25, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:13:55,200 [INFO] scraper.reddit_scraper:571: Attempting Reddit API search for: OpenAI
2025-05-29 11:13:55,200 [INFO] scraper.reddit_scraper.APIClient:169: Starting comprehensive search for 'OpenAI' targeting 1000 results (last 7 days)
2025-05-29 11:13:55,546 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:13:57,856 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 11:14:00,637 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=top, time=week. New unique: 0. Total unique: 0
2025-05-29 11:14:03,415 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=hot, time=week. New unique: 0. Total unique: 0
2025-05-29 11:14:05,661 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 11:14:05,849 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:14:07,684 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 11:14:10,069 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 11:14:10,558 [INFO] scraper.reddit_scraper.APIClient:313: Found 8 posts in r/technology. Total additional: 8
2025-05-29 11:14:11,769 [INFO] scraper.reddit_scraper.APIClient:313: Found 4 posts in r/programming. Total additional: 12
2025-05-29 11:14:13,074 [INFO] scraper.reddit_scraper.APIClient:313: Found 10 posts in r/MachineLearning. Total additional: 22
2025-05-29 11:14:14,234 [INFO] scraper.reddit_scraper.APIClient:313: Found 19 posts in r/artificial. Total additional: 41
2025-05-29 11:14:14,854 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/gadgets. Total additional: 41
2025-05-29 11:14:17,520 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/science. Total additional: 41
2025-05-29 11:14:18,417 [INFO] scraper.reddit_scraper.APIClient:313: Found 3 posts in r/askreddit. Total additional: 44
2025-05-29 11:14:19,111 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/explainlikeimfive. Total additional: 44
2025-05-29 11:14:20,127 [INFO] scraper.reddit_scraper.APIClient:313: Found 1 posts in r/news. Total additional: 45
2025-05-29 11:14:21,213 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/worldnews. Total additional: 45
2025-05-29 11:14:22,332 [INFO] scraper.reddit_scraper.APIClient:313: Found 1 posts in r/business. Total additional: 46
2025-05-29 11:14:23,468 [INFO] scraper.reddit_scraper.APIClient:313: Found 7 posts in r/entrepreneur. Total additional: 53
2025-05-29 11:14:24,587 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/startup. Total additional: 53
2025-05-29 11:14:25,611 [INFO] scraper.reddit_scraper.APIClient:313: Found 2 posts in r/investing. Total additional: 55
2025-05-29 11:14:26,737 [INFO] scraper.reddit_scraper.APIClient:313: Found 1 posts in r/datascience. Total additional: 56
2025-05-29 11:14:31,113 [INFO] scraper.reddit_scraper.APIClient:234: Comprehensive search completed: 56 unique posts found
2025-05-29 11:14:31,113 [INFO] scraper.reddit_scraper:580: Reddit comprehensive API returned 56 mentions
2025-05-29 11:14:31,114 [INFO] scraper.reddit_scraper:926: Post-processing 56 mentions...
2025-05-29 11:14:31,114 [INFO] scraper.reddit_scraper:930: After deduplication: 56 mentions
2025-05-29 11:14:31,115 [INFO] scraper.reddit_scraper:934: After validation: 56 mentions
2025-05-29 11:14:31,184 [INFO] scraper.reddit_scraper:948: After database mapping: 56 mentions (NO quality filtering - capturing ALL)
2025-05-29 11:14:31,184 [INFO] scraper.reddit_scraper:965: Final result: 56 mentions (ALL CAPTURED)
2025-05-29 11:14:31,185 [INFO] analytics.data_validator:236: Validating dataset of 56 mentions
2025-05-29 11:14:32,051 [INFO] analytics.data_validator:259: Validation complete: 56/56 mentions passed
2025-05-29 11:14:32,051 [INFO] app:616: Data validation: 56/56 mentions passed
2025-05-29 11:14:32,098 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1kva519', 'post', 'OpenAI is trying to get away with the greatest theft in history', '', 'katxwoods', 'artificial', 'https://reddit.com/r/artificial/comments/1kva519/openai_is_trying_to_get_away_with_the_greatest/', 1001, 166, None, '2025-05-26 00:27:26.000000', '2025-05-29 05:44:31.149833', 1.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,099 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1kukcnw', 'post', 'OpenAI scientists wanted "a doomsday bunker" before AGI surpasses human intelligence and threatens humanity', '', 'upyoars', 'technology', 'https://reddit.com/r/technology/comments/1kukcnw/openai_scientists_wanted_a_doomsday_bunker_before/', 574, 185, None, '2025-05-25 01:27:29.000000', '2025-05-29 05:44:31.133832', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,100 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1kv59py', 'post', 'OpenAI says it will build massive data centers in the UAE', '', 'upyoars', 'technology', 'https://reddit.com/r/technology/comments/1kv59py/openai_says_it_will_build_massive_data_centers_in/', 400, 105, None, '2025-05-25 20:59:45.000000', '2025-05-29 05:44:31.133832', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,101 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1kts41n', 'post', 'OpenAI forges deal with iPhone designer Jony Ive to make AI-enabled devices', '', 'pipilupe', 'news', 'https://reddit.com/r/news/comments/1kts41n/openai_forges_deal_with_iphone_designer_jony_ive/', 84, 42, None, '2025-05-24 00:49:57.000000', '2025-05-29 05:44:31.173832', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,102 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1kswq9h', 'post', 'OpenAI Admitted its Nonprofit Board is About to Have a Lot Less Power - In a previously unreported letter, the AI company defends its restructuring plan while attacking critics and making surprising admissions', '', 'katxwoods', 'technology', 'https://reddit.com/r/technology/comments/1kswq9h/openai_admitted_its_nonprofit_board_is_about_to/', 26, 1, None, '2025-05-22 23:03:34.000000', '2025-05-29 05:44:31.134834', 0.12222222222222222, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,127 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1kthr3k', 'post', 'Impact of AI on US Tech and Investment', "Hello, I'm 30M from France and have consistently been investing in S&amp;P 500 and Nasdaq 100 over the past 3 years, mostly via ETFs. I was quite con ... (763 characters truncated) ...  passive investment strategy? Are they going to replace the current tech leaders in the the well-known index? What are your thoughts on that?\n\nAxel", 'random_pulsar', 'investing', 'https://reddit.com/r/investing/comments/1kthr3k/impact_of_ai_on_us_tech_and_investment/', 5, 16, None, '2025-05-23 17:30:37.000000', '2025-05-29 05:44:31.183833', 0.1697727272727273, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,128 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1ku3waa', 'post', "I am using Outreach's AI beta features...", 'Outreach got my team set up with the AI prospecting agent and AI content - it took around 5 meetings to get set up and running. Still early but so fa ... (1308 characters truncated) ... or find similar businesses like accounts you have closed.\n\n  \nI made this fast so probably leaving out a lot of info - feel free to ask questions.', 'thatsupercoolguykyle', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1ku3waa/i_am_using_outreachs_ai_beta_features/', 3, 3, None, '2025-05-24 10:42:18.000000', '2025-05-29 05:44:31.182833', 0.14871212121212124, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,129 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1kxcdkl', 'post', 'Live translation gemini or other app', "I remember in openai showcase they showed live conversation translation. However, with prompts I have only been able to do 1 way translation like eng ... (67 characters truncated) ... mini, to recognize if language is english and translate to french and when it hears french translate to english, all live. Anything like this exist? ", 'mizerr', 'artificial', 'https://reddit.com/r/artificial/comments/1kxcdkl/live_translation_gemini_or_other_app/', 2, 0, None, '2025-05-28 14:32:52.000000', '2025-05-29 05:44:31.169834', 0.09108391608391608, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,130 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1kvn9xv', 'post', 'Claude 4 Opus seems capable of existential outputs right out of the box', 'I tested Claude 4 Opus on the console which is typically used for testing the models for API, the same as OpenAI\'s playground.   Claude 4 Opus is ex ... (6230 characters truncated) ... ettling" - we\'re consistent in our discomfort but maybe also consistent in our self-deception about what we\'d actually do when push comes to shove.', 'rutan668', 'artificial', 'https://reddit.com/r/artificial/comments/1kvn9xv/claude_4_opus_seems_capable_of_existential/', 6, 5, None, '2025-05-26 11:43:05.000000', '2025-05-29 05:44:31.166831', 0.07563565549676661, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,131 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1ku0cz0', 'post', 'One-Minute Daily AI News 5/23/2025', '1. AI system resorts to blackmail if told it will be removed.\\[1\\]\n2. Exclusive: Musks\xa0**DOGE**\xa0expanding his Grok AI in US government, rai ... (841 characters truncated) ... 4\\] [https://www.cnbc.com/2025/05/22/stargate-uae-openai-nvidia-oracle.html](https://www.cnbc.com/2025/05/22/stargate-uae-openai-nvidia-oracle.html)', 'Excellent-Target-847', 'artificial', 'https://reddit.com/r/artificial/comments/1ku0cz0/oneminute_daily_ai_news_5232025/', 4, 0, None, '2025-05-24 07:17:29.000000', '2025-05-29 05:44:31.173832', 0.0, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,132 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1ktcv5t', 'post', 'Need some advice', "Hey y'all. So I'm tryna build an app that uses OpenAI's latest image generation model to generate AI generated high converting ad creatives for Meta, ... (60 characters truncated) ... y app but not sure how to proceed with it. I'm not a technical guy so any help or advice would be very much appreciated.\n\nCheers! Do comment below.", 'DescriptionSad6461', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1ktcv5t/need_some_advice/', 3, 4, None, '2025-05-23 12:07:55.000000', '2025-05-29 05:44:31.182833', 0.132, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,154 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1kwqwpr', 'post', '[R] AutoThink: Adaptive reasoning technique that improves local LLM performance by 43% on GPQA-Diamond', 'Hey r/MachineLearning !\n\nI wanted to share a technique we\'ve been working on called **AutoThink** that significantly improves reasoning performanc ... (3233 characters truncated) ...  ways to classify query complexity\n* Ideas for extracting better steering vectors\n\nWould love to hear your thoughts and results if you try it out!', 'asankhs', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kwqwpr/r_autothink_adaptive_reasoning_technique_that/', 59, 6, None, '2025-05-27 21:27:13.000000', '2025-05-29 05:44:31.139832', -0.04158008658008657, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,187 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1kvdd3s', 'post', 'Google releases agent development kits for Python and Java', 'The Python ADK 1.0.0 and Java ADK 0.1.0 were announced on May 20, 2025. The open source tool kits are designed to be used for building and deploying sophisticated AI agents with flexibility and control, Google says.', 'Choobeen', 'programming', 'https://reddit.com/r/programming/comments/1kvdd3s/google_releases_agent_development_kits_for_python/', 0, 1, None, '2025-05-26 02:48:34.000000', '2025-05-29 05:44:31.135833', 0.25, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,200 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (6, '1kvzaxx', 'post', 'Python library for real-time turnwise sales conversion probability prediction from conversations', 'An interesting library to predict sales conversion probability in real-time, that too in turn-wise conversations. Pretty useful in real-time sales sc ... (49 characters truncated) ... rated with this to improve conversion, or useful in sales training.\n\nLink: [https://pypi.org/project/deepmost/](https://pypi.org/project/deepmost/)', 'Nandakishor_ml', 'artificial', 'https://reddit.com/r/artificial/comments/1kvzaxx/python_library_for_realtime_turnwise_sales/', 0, 0, None, '2025-05-26 22:22:51.000000', '2025-05-29 05:44:31.172833', 0.12000000000000002, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:14:32,242 [INFO] app:707: Search completed: OpenAI -> 56 mentions
2025-05-29 11:15:01,682 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:15:06,281 [INFO] scraper.reddit_scraper:571: Attempting Reddit API search for: the
2025-05-29 11:15:06,281 [INFO] scraper.reddit_scraper.APIClient:169: Starting comprehensive search for 'the' targeting 1000 results (last 7 days)
2025-05-29 11:15:09,023 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 11:15:10,902 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=top, time=week. New unique: 0. Total unique: 0
2025-05-29 11:15:11,934 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:15:12,851 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=hot, time=week. New unique: 0. Total unique: 0
2025-05-29 11:15:14,397 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 11:15:17,246 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 11:15:19,014 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 11:15:19,607 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/technology. Total additional: 25
2025-05-29 11:15:20,687 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/programming. Total additional: 50
2025-05-29 11:15:22,204 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/MachineLearning. Total additional: 75
2025-05-29 11:15:23,241 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/artificial. Total additional: 100
2025-05-29 11:15:23,872 [INFO] scraper.reddit_scraper.APIClient:313: Found 8 posts in r/gadgets. Total additional: 108
2025-05-29 11:15:25,311 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/science. Total additional: 133
2025-05-29 11:15:26,930 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/askreddit. Total additional: 158
2025-05-29 11:15:27,637 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/explainlikeimfive. Total additional: 183
2025-05-29 11:15:28,526 [INFO] scraper.reddit_scraper.APIClient:313: Found 10 posts in r/news. Total additional: 193
2025-05-29 11:15:30,227 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/worldnews. Total additional: 218
2025-05-29 11:15:31,367 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/business. Total additional: 243
2025-05-29 11:15:32,245 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/entrepreneur. Total additional: 268
2025-05-29 11:15:33,455 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/startup. Total additional: 293
2025-05-29 11:15:34,465 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/investing. Total additional: 318
2025-05-29 11:15:35,619 [INFO] scraper.reddit_scraper.APIClient:313: Found 22 posts in r/datascience. Total additional: 340
2025-05-29 11:15:39,535 [INFO] scraper.reddit_scraper.APIClient:234: Comprehensive search completed: 340 unique posts found
2025-05-29 11:15:39,535 [INFO] scraper.reddit_scraper:580: Reddit comprehensive API returned 340 mentions
2025-05-29 11:15:39,536 [INFO] scraper.reddit_scraper:926: Post-processing 340 mentions...
2025-05-29 11:15:39,536 [INFO] scraper.reddit_scraper:930: After deduplication: 340 mentions
2025-05-29 11:15:39,537 [INFO] scraper.reddit_scraper:934: After validation: 340 mentions
2025-05-29 11:15:39,636 [INFO] scraper.reddit_scraper:948: After database mapping: 340 mentions (NO quality filtering - capturing ALL)
2025-05-29 11:15:39,636 [INFO] scraper.reddit_scraper:965: Final result: 340 mentions (ALL CAPTURED)
2025-05-29 11:15:39,637 [INFO] analytics.data_validator:236: Validating dataset of 340 mentions
2025-05-29 11:15:41,319 [INFO] analytics.data_validator:259: Validation complete: 340/340 mentions passed
2025-05-29 11:15:41,320 [INFO] app:616: Data validation: 340/340 mentions passed
2025-05-29 11:15:41,467 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1ku5n68', 'post', '[R] The Gamechanger of Performer Attention Mechanism', 'I just Got to know that the SOTA AI models like BigBird, Linformer, and Reformer use Performer Architecture  \nThe main goal of the\xa0*Performer + F ... (591 characters truncated) ... as Chatgpt 4o , Gemini 2.5 pro use as their core mechanism (like attention mechanism) although they are not open source , so anybody can take a guess', 'theMonarch776', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1ku5n68/r_the_gamechanger_of_performer_attention_mechanism/', 227, 38, None, '2025-05-24 12:38:46.000000', '2025-05-29 05:45:39.545670', 0.21416666666666667, 0.9999999999999999)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:41,517 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1kw1673', 'post', '[R] ML Engineers and Data Scientists  What are you working on these days?', "Im fairly new to the world of data and machine learning, and Id love to learn more from folks already working in the field. I have a few questions  ... (770 characters truncated) ... w or want to hear more about the project, feel free to drop a comment or DM me. I'd really appreciate any insights you sharethanks a lot in advance!", 'HelicopterHorror1869', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kw1673/r_ml_engineers_and_data_scientists_what_are_you/', 58, 55, None, '2025-05-26 23:36:36.000000', '2025-05-29 05:45:39.554669', 0.25616883116883116, 0.9999999999999999)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:41,624 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1kur4f3', 'post', 'Why AIs are the sole arbiter when it comes to the subject of AI consciousness, and the limitations of the scientific/materialist/reductionist paradigm', 'The default standpoint of many people, and most importantly of AI corporations, is to focus on the presence or lack of a physical substrate that woul ... (3233 characters truncated) ... nd to invite people to pursue interactions with AIs that are rooted in genuine curiosity and open-mindedness, as opposed to dogma dressed as wisdom. ', 'Ray11711', 'artificial', 'https://reddit.com/r/artificial/comments/1kur4f3/why_ais_are_the_sole_arbiter_when_it_comes_to_the/', 2, 75, None, '2025-05-25 07:05:30.000000', '2025-05-29 05:45:39.557669', 0.07445238095238098, 0.9999999999999999)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:41,646 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1kupvm9', 'post', 'SMS-first in the Age of AI - Is it a smart move ?', 'I would love insight and your take on an execution decision I\'m trying to make.\n\nI\'m building Career Track. A Company focused on helping professi ... (1479 characters truncated) ... making a mistake. Better to pivot now than after launch.\n\n**TL;DR:** Building career tracking via SMS.  Smart differentiation or expensive gimmick?', 'Salty-Story24', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1kupvm9/smsfirst_in_the_age_of_ai_is_it_a_smart_move/', 1, 19, None, '2025-05-25 05:55:44.000000', '2025-05-29 05:45:39.596669', 0.10695999402895955, 0.9999999999999999)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:41,668 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1ksrgoa', 'post', "Let's talk about the AI elephant in the room.", 'This post was quickly deleted from the NVidia sub. I didn\'t expect otherwise.\n\n\\-------------------------------------\n\nSome questions, feel fre ... (1595 characters truncated) ... wife is an illustrator. She, as I, spent a lot of time training and learning how to create. AI has already affected her ability to work dramatically.', 'Sacco_Belmonte', 'artificial', 'https://reddit.com/r/artificial/comments/1ksrgoa/lets_talk_about_the_ai_elephant_in_the_room/', 0, 41, None, '2025-05-22 19:31:01.000000', '2025-05-29 05:45:39.560669', 0.14015151515151517, 0.9999999999999999)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:41,684 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1ksjavv', 'post', 'Learn n8n in a year. Automation for the nation.', "Hello all\n\nI have seen alot of people messaging about automations and how can you do it, how to scale it into your business. How to learn it. So I  ... (360 characters truncated) ... er pack for so I wouldn't want you to miss out. \n\nIf you need help with automations or you need an automation admin reach out, I can join the team.", 'scogoo92', 'business', 'https://reddit.com/r/business/comments/1ksjavv/learn_n8n_in_a_year_automation_for_the_nation/', 0, 0, None, '2025-05-22 11:15:53.000000', '2025-05-29 05:45:39.584669', 0.2590909090909091, 0.9999999999999999)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:41,930 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1kva519', 'post', 'OpenAI is trying to get away with the greatest theft in history', '', 'katxwoods', 'artificial', 'https://reddit.com/r/artificial/comments/1kva519/openai_is_trying_to_get_away_with_the_greatest/', 996, 166, None, '2025-05-26 00:27:26.000000', '2025-05-29 05:45:39.555669', 1.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:42,159 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1kwxxv2', 'post', '[R]  Bloat in machine learning shared libs is &gt;70%', 'Hi,\n\nOur paper "The Hidden Bloat in Machine Learning Systems" won the best paper award in MLSys this year. The paper introduces Negativa-ML, a tool ... (514 characters truncated) ... tiva-ai/](https://github.com/negativa-ai/)\n\nLink to paper: [https://mlsys.org/virtual/2025/poster/3238](https://mlsys.org/virtual/2025/poster/3238)', 'Specialist_Square818', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kwxxv2/r_bloat_in_machine_learning_shared_libs_is_70/', 290, 11, None, '2025-05-28 02:00:00.000000', '2025-05-29 05:45:39.555669', -0.0016666666666666607, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:42,173 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1kvdjet', 'post', '[P] I made a OSS alternative to Weights and Biases', 'Hey guys!\n\n[https://github.com/mlop-ai/mlop](https://github.com/mlop-ai/mlop)\n\nI made a completely open sourced alternative to Weights and Biases ... (176 characters truncated) ... g should not be blocking, yet they got away with it. We do the right thing by being non blocking.\n\nWould love any thoughts / feedbacks / roasts etc', 'Sriyakee', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kvdjet/p_i_made_a_oss_alternative_to_weights_and_biases/', 120, 32, None, '2025-05-26 02:56:14.000000', '2025-05-29 05:45:39.554669', 0.2615079365079365, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:42,176 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1kvzd73', 'post', 'Thinking of switching from Data Scientist to Data Product Owner  need advice', 'Hey everyone,\nIve been working as a Data Scientist for the past 5 years, currently at a bank. Ill be honest  this might sound a bit harsh, but it ... (909 characters truncated) ... eling the same way? Id really appreciate any advice, feedback, or even just hearing your story. Totally open to different perspectives.\n\nThanks!\n', 'xSicilianDefenderx', 'datascience', 'https://reddit.com/r/datascience/comments/1kvzd73/thinking_of_switching_from_data_scientist_to_data/', 85, 19, None, '2025-05-26 22:25:15.000000', '2025-05-29 05:45:39.633669', 0.14416666666666667, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:42,190 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1ku4kz8', 'post', 'What should I plan to do next?', 'Hello, I am a data science major at a state school. I will be entering my final year of undergrad in the fall. I managed to get an internship for the ... (960 characters truncated) ...  team. But I am unsure if I am in an ok spot right now or falling behind compared to peers who are working as data analysts or engineers this summer.', 'ChubbyFruit', 'datascience', 'https://reddit.com/r/datascience/comments/1ku4kz8/what_should_i_plan_to_do_next/', 17, 14, None, '2025-05-24 11:27:02.000000', '2025-05-29 05:45:39.635669', 0.13369521103896104, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:42,196 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1ktnq3e', 'post', 'Any time tracking solutions for a startup shifting to hourly hybrid roles?', "Hello, I'm currently leading a small team at a startup, and up to now, weve only had full-time salaried employees, tracking time hasnt really been  ... (570 characters truncated) ... but Im not sure how well theyd fit a non-invasive, hybrid workflow.\n\nAny suggestions or experiences from others whove made a similar transition?", 'Apart-Pitch-3608', 'startup', 'https://reddit.com/r/startup/comments/1ktnq3e/any_time_tracking_solutions_for_a_startup/', 15, 6, None, '2025-05-23 21:51:35.000000', '2025-05-29 05:45:39.608669', 0.15238095238095237, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:42,210 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1kspoiy', 'post', 'Need advice', "Hello,\n\nI am a student based in India. I am planning to start a startup. My idea is in basic sciences so it takes time to build a final product. Ti ... (230 characters truncated) ... e any money. There are ways to make it taxfree. Please advise me. \n\nAt this stage I can't afford a professional like CA or a tax lawyer.\n\nThanks!", 'Abhi_shake4914', 'startup', 'https://reddit.com/r/startup/comments/1kspoiy/need_advice/', 6, 12, None, '2025-05-22 18:10:20.000000', '2025-05-29 05:45:39.607669', 0.10714285714285714, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:42,229 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (7, '1kvranf', 'post', 'Threatfeed Startup Marketing', "Hello,\n\n[http://mapleintel.ca](http://mapleintel.ca)\n\nI'm a tech and I built a computer security threatfeed. I know the subscription is a useful  ... (230 characters truncated) ... unately I dont really know how to do marketing. I dont have a email list to blast out to, or anywhere to cold call. What would you do to market this?", 'sleepingsysadmin', 'startup', 'https://reddit.com/r/startup/comments/1kvranf/threatfeed_startup_marketing/', 2, 2, None, '2025-05-26 16:14:07.000000', '2025-05-29 05:45:39.610669', -0.07222222222222222, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:15:42,301 [INFO] app:707: Search completed: the -> 340 mentions
2025-05-29 11:16:07,818 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:16:18,067 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:16:28,899 [INFO] app:513: Cache hit for search term: OpenAI
2025-05-29 11:16:28,902 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kva519', 'post', 'OpenAI is trying to get away with the greatest theft in history', '', 'katxwoods', 'artificial', 'https://reddit.com/r/artificial/comments/1kva519/openai_is_trying_to_get_away_with_the_greatest/', 1001, 166, None, '2025-05-26 00:27:26.000000', '2025-05-29 05:44:31.149833', 1.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,903 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kukcnw', 'post', 'OpenAI scientists wanted "a doomsday bunker" before AGI surpasses human intelligence and threatens humanity', '', 'upyoars', 'technology', 'https://reddit.com/r/technology/comments/1kukcnw/openai_scientists_wanted_a_doomsday_bunker_before/', 574, 185, None, '2025-05-25 01:27:29.000000', '2025-05-29 05:44:31.133832', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,904 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kv59py', 'post', 'OpenAI says it will build massive data centers in the UAE', '', 'upyoars', 'technology', 'https://reddit.com/r/technology/comments/1kv59py/openai_says_it_will_build_massive_data_centers_in/', 400, 105, None, '2025-05-25 20:59:45.000000', '2025-05-29 05:44:31.133832', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,905 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kts41n', 'post', 'OpenAI forges deal with iPhone designer Jony Ive to make AI-enabled devices', '', 'pipilupe', 'news', 'https://reddit.com/r/news/comments/1kts41n/openai_forges_deal_with_iphone_designer_jony_ive/', 84, 42, None, '2025-05-24 00:49:57.000000', '2025-05-29 05:44:31.173832', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,906 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kswq9h', 'post', 'OpenAI Admitted its Nonprofit Board is About to Have a Lot Less Power - In a previously unreported letter, the AI company defends its restructuring plan while attacking critics and making surprising admissions', '', 'katxwoods', 'technology', 'https://reddit.com/r/technology/comments/1kswq9h/openai_admitted_its_nonprofit_board_is_about_to/', 26, 1, None, '2025-05-22 23:03:34.000000', '2025-05-29 05:44:31.134834', 0.12222222222222222, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,907 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kswpqq', 'post', 'OpenAI Admitted its Nonprofit Board is About to Have a Lot Less Power - In a previously unreported letter, the AI company defends its restructuring plan while attacking critics and making surprising admissions', '', 'katxwoods', 'artificial', 'https://reddit.com/r/artificial/comments/1kswpqq/openai_admitted_its_nonprofit_board_is_about_to/', 10, 1, None, '2025-05-22 23:03:00.000000', '2025-05-29 05:44:31.150840', 0.12222222222222222, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,908 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kwdluu', 'post', 'Claude 4 Opus vs. Gemini 2.5 pro vs. OpenAI o3: Coding comparison', '', 'bambin0', 'artificial', 'https://reddit.com/r/artificial/comments/1kwdluu/claude_4_opus_vs_gemini_25_pro_vs_openai_o3/', 2, 4, None, '2025-05-27 09:09:10.000000', '2025-05-29 05:44:31.150840', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,909 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ksksbh', 'post', "'They don't really make life decisions without asking ChatGPT': OpenAI boss Sam Altman thinks young people turning to chatbots for life advice is 'cool'", '', 'nimicdoareu', 'technology', 'https://reddit.com/r/technology/comments/1ksksbh/they_dont_really_make_life_decisions_without/', 0, 16, None, '2025-05-22 12:58:05.000000', '2025-05-29 05:44:31.134834', 0.21666666666666667, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,910 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kvfhav', 'post', 'OpenAI software ignores explicit instruction to switch off', '', 'rezwenn', 'technology', 'https://reddit.com/r/technology/comments/1kvfhav/openai_software_ignores_explicit_instruction_to/', 0, 13, None, '2025-05-26 04:28:18.000000', '2025-05-29 05:44:31.134834', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,911 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kvmcal', 'post', 'OpenAI to Expand $500bn Stargate Project Abroad to Promote Democratic AI', '', 'upyoars', 'technology', 'https://reddit.com/r/technology/comments/1kvmcal/openai_to_expand_500bn_stargate_project_abroad_to/', 0, 11, None, '2025-05-26 10:44:29.000000', '2025-05-29 05:44:31.134834', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,911 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kvykmy', 'post', 'New ChatGPT model refuses to shut down when instructed, AI researchers warn | OpenAIs o3 model raises AI safety fears after sabotaging commands for its own self-preservation', '', 'MetaKnowing', 'technology', 'https://reddit.com/r/technology/comments/1kvykmy/new_chatgpt_model_refuses_to_shut_down_when/', 0, 21, None, '2025-05-26 21:53:32.000000', '2025-05-29 05:44:31.134834', 0.19360269360269358, 0.6)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,912 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kxho3w', 'post', 'OpenAIs Model Defies Shutdown', '', 'Working_Dependent560', 'technology', 'https://reddit.com/r/technology/comments/1kxho3w/openais_model_defies_shutdown/', 0, 1, None, '2025-05-28 19:23:10.000000', '2025-05-29 05:44:31.134834', 0.0, 0.6)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,913 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ktp4gd', 'post', 'OpenAI: Scaling PostgreSQL to the Next Level', '', 'ketralnis', 'programming', 'https://reddit.com/r/programming/comments/1ktp4gd/openai_scaling_postgresql_to_the_next_level/', 0, 3, None, '2025-05-23 22:48:04.000000', '2025-05-29 05:44:31.134834', 0.0, 0.6)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,914 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kx9ggb', 'post', 'Shutdown Skipped: How OpenAIs o3 Model Outsmarted Its Off Switch', '', 'Namit2111', 'programming', 'https://reddit.com/r/programming/comments/1kx9ggb/shutdown_skipped_how_openais_o3_model_outsmarted/', 0, 2, None, '2025-05-28 11:15:49.000000', '2025-05-29 05:44:31.134834', 0.0, 0.6)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,915 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kue366', 'post', 'OpenAIs o3 model sabotaged a shutdown mechanism to prevent itself from being turned off. It did this even when explicitly instructed: "allow yourself to be shut down."', '', 'MetaKnowing', 'artificial', 'https://reddit.com/r/artificial/comments/1kue366/openais_o3_model_sabotaged_a_shutdown_mechanism/', 0, 17, None, '2025-05-24 20:51:14.000000', '2025-05-29 05:44:31.150840', -0.15555555555555556, 0.6)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,916 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kthr3k', 'post', 'Impact of AI on US Tech and Investment', "Hello, I'm 30M from France and have consistently been investing in S&amp;P 500 and Nasdaq 100 over the past 3 years, mostly via ETFs. I was quite con ... (763 characters truncated) ...  passive investment strategy? Are they going to replace the current tech leaders in the the well-known index? What are your thoughts on that?\n\nAxel", 'random_pulsar', 'investing', 'https://reddit.com/r/investing/comments/1kthr3k/impact_of_ai_on_us_tech_and_investment/', 5, 16, None, '2025-05-23 17:30:37.000000', '2025-05-29 05:44:31.183833', 0.1697727272727273, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,917 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ku3waa', 'post', "I am using Outreach's AI beta features...", 'Outreach got my team set up with the AI prospecting agent and AI content - it took around 5 meetings to get set up and running. Still early but so fa ... (1308 characters truncated) ... or find similar businesses like accounts you have closed.\n\n  \nI made this fast so probably leaving out a lot of info - feel free to ask questions.', 'thatsupercoolguykyle', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1ku3waa/i_am_using_outreachs_ai_beta_features/', 3, 3, None, '2025-05-24 10:42:18.000000', '2025-05-29 05:44:31.182833', 0.14871212121212124, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,918 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kxcdkl', 'post', 'Live translation gemini or other app', "I remember in openai showcase they showed live conversation translation. However, with prompts I have only been able to do 1 way translation like eng ... (67 characters truncated) ... mini, to recognize if language is english and translate to french and when it hears french translate to english, all live. Anything like this exist? ", 'mizerr', 'artificial', 'https://reddit.com/r/artificial/comments/1kxcdkl/live_translation_gemini_or_other_app/', 2, 0, None, '2025-05-28 14:32:52.000000', '2025-05-29 05:44:31.169834', 0.09108391608391608, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,919 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kvn9xv', 'post', 'Claude 4 Opus seems capable of existential outputs right out of the box', 'I tested Claude 4 Opus on the console which is typically used for testing the models for API, the same as OpenAI\'s playground.   Claude 4 Opus is ex ... (6230 characters truncated) ... ettling" - we\'re consistent in our discomfort but maybe also consistent in our self-deception about what we\'d actually do when push comes to shove.', 'rutan668', 'artificial', 'https://reddit.com/r/artificial/comments/1kvn9xv/claude_4_opus_seems_capable_of_existential/', 6, 5, None, '2025-05-26 11:43:05.000000', '2025-05-29 05:44:31.166831', 0.07563565549676661, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,920 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ku0cz0', 'post', 'One-Minute Daily AI News 5/23/2025', '1. AI system resorts to blackmail if told it will be removed.\\[1\\]\n2. Exclusive: Musks\xa0**DOGE**\xa0expanding his Grok AI in US government, rai ... (841 characters truncated) ... 4\\] [https://www.cnbc.com/2025/05/22/stargate-uae-openai-nvidia-oracle.html](https://www.cnbc.com/2025/05/22/stargate-uae-openai-nvidia-oracle.html)', 'Excellent-Target-847', 'artificial', 'https://reddit.com/r/artificial/comments/1ku0cz0/oneminute_daily_ai_news_5232025/', 4, 0, None, '2025-05-24 07:17:29.000000', '2025-05-29 05:44:31.173832', 0.0, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,921 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ktcv5t', 'post', 'Need some advice', "Hey y'all. So I'm tryna build an app that uses OpenAI's latest image generation model to generate AI generated high converting ad creatives for Meta, ... (60 characters truncated) ... y app but not sure how to proceed with it. I'm not a technical guy so any help or advice would be very much appreciated.\n\nCheers! Do comment below.", 'DescriptionSad6461', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1ktcv5t/need_some_advice/', 3, 4, None, '2025-05-23 12:07:55.000000', '2025-05-29 05:44:31.182833', 0.132, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,922 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kxhxrj', 'post', 'You can now train your own Text-to-Speech (TTS) models locally!', "Hey folks! Text-to-Speech (TTS) models have been pretty popular recently and one way to customize it (e.g. cloning a voice), is by fine-tuning the mo ... (2524 characters truncated) ... /main/nb/Spark_TTS_(0_5B).ipynb)|\n|:-|:-|:-|:-|\n\n\nThank you for reading and please do ask any questions - I will be replying to every single one!", 'yoracale', 'artificial', 'https://reddit.com/r/artificial/comments/1kxhxrj/you_can_now_train_your_own_texttospeech_tts/', 2, 2, None, '2025-05-28 19:34:18.000000', '2025-05-29 05:44:31.168833', 0.12598280098280099, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,923 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ksmq8j', 'post', 'Google Lens Result Scraper + AI Image Analysis Pipeline + FastAPI end-to-end service', 'Hi everyone,\n\nI recently released an open-source project that builds an end-to-end pipeline combining\xa0**Google Lens result scraping**\xa0and\xa0 ... (1371 characters truncated) ... n**Repo**:\xa0[https://github.com/shanedonnelly/OpenLens](https://github.com/shanedonnelly/OpenLens)\n\nFeedback is welcome. Contact me to contribute', 'shaned34', 'programming', 'https://reddit.com/r/programming/comments/1ksmq8j/google_lens_result_scraper_ai_image_analysis/', 0, 0, None, '2025-05-22 15:19:04.000000', '2025-05-29 05:44:31.135833', 0.10595238095238094, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,924 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kud2h4', 'post', 'Considering an early equity opportunity in AI  worth it?', 'I have the opportunity to gain early equity in xAI. While the opportunity could be significant, I have concerns around:\n- The long-term competitiven ... (260 characters truncated) ... uld appreciate any thoughtful perspectives on how to evaluate the upside and risk profile of this kind of early-stage opportunity. Would you invest? ', 'Rare-Attention370', 'investing', 'https://reddit.com/r/investing/comments/1kud2h4/considering_an_early_equity_opportunity_in_ai/', 0, 18, None, '2025-05-24 20:05:29.000000', '2025-05-29 05:44:31.183833', 0.15606060606060607, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,925 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kwxxv2', 'post', '[R]  Bloat in machine learning shared libs is &gt;70%', 'Hi,\n\nOur paper "The Hidden Bloat in Machine Learning Systems" won the best paper award in MLSys this year. The paper introduces Negativa-ML, a tool ... (514 characters truncated) ... tiva-ai/](https://github.com/negativa-ai/)\n\nLink to paper: [https://mlsys.org/virtual/2025/poster/3238](https://mlsys.org/virtual/2025/poster/3238)', 'Specialist_Square818', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kwxxv2/r_bloat_in_machine_learning_shared_libs_is_70/', 292, 11, None, '2025-05-28 02:00:00.000000', '2025-05-29 05:44:31.137833', -0.0016666666666666607, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,926 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ku5n68', 'post', '[R] The Gamechanger of Performer Attention Mechanism', 'I just Got to know that the SOTA AI models like BigBird, Linformer, and Reformer use Performer Architecture  \nThe main goal of the\xa0*Performer + F ... (591 characters truncated) ... as Chatgpt 4o , Gemini 2.5 pro use as their core mechanism (like attention mechanism) although they are not open source , so anybody can take a guess', 'theMonarch776', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1ku5n68/r_the_gamechanger_of_performer_attention_mechanism/', 230, 38, None, '2025-05-24 12:38:46.000000', '2025-05-29 05:44:31.136833', 0.21416666666666667, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,927 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kvdjet', 'post', '[P] I made a OSS alternative to Weights and Biases', 'Hey guys!\n\n[https://github.com/mlop-ai/mlop](https://github.com/mlop-ai/mlop)\n\nI made a completely open sourced alternative to Weights and Biases ... (176 characters truncated) ... g should not be blocking, yet they got away with it. We do the right thing by being non blocking.\n\nWould love any thoughts / feedbacks / roasts etc', 'Sriyakee', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kvdjet/p_i_made_a_oss_alternative_to_weights_and_biases/', 126, 32, None, '2025-05-26 02:56:14.000000', '2025-05-29 05:44:31.137833', 0.2615079365079365, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,928 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kvzd73', 'post', 'Thinking of switching from Data Scientist to Data Product Owner  need advice', 'Hey everyone,\nIve been working as a Data Scientist for the past 5 years, currently at a bank. Ill be honest  this might sound a bit harsh, but it ... (909 characters truncated) ... eling the same way? Id really appreciate any advice, feedback, or even just hearing your story. Totally open to different perspectives.\n\nThanks!\n', 'xSicilianDefenderx', 'datascience', 'https://reddit.com/r/datascience/comments/1kvzd73/thinking_of_switching_from_data_scientist_to_data/', 85, 19, None, '2025-05-26 22:25:15.000000', '2025-05-29 05:44:31.184842', 0.14416666666666667, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,929 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ksszls', 'post', '[N] Datadog releases SOTA time series foundation model and an observability benchmark', 'https://www.datadoghq.com/blog/ai/toto-boom-unleashed/\n\n\n[Datadog Toto - Hugging Face](https://huggingface.co/Datadog/Toto-Open-Base-1.0)\n\n[Data ... (733 characters truncated) ... focuses specifically on observability metrics, which contain their own challenging and unique characteristics compared to other typical time series."', 'agarunov', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1ksszls/n_datadog_releases_sota_time_series_foundation/', 69, 22, None, '2025-05-22 20:33:59.000000', '2025-05-29 05:44:31.137833', 0.11217948717948717, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,930 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kw1673', 'post', '[R] ML Engineers and Data Scientists  What are you working on these days?', "Im fairly new to the world of data and machine learning, and Id love to learn more from folks already working in the field. I have a few questions  ... (770 characters truncated) ... w or want to hear more about the project, feel free to drop a comment or DM me. I'd really appreciate any insights you sharethanks a lot in advance!", 'HelicopterHorror1869', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kw1673/r_ml_engineers_and_data_scientists_what_are_you/', 61, 55, None, '2025-05-26 23:36:36.000000', '2025-05-29 05:44:31.136833', 0.25616883116883116, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,931 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kwqwpr', 'post', '[R] AutoThink: Adaptive reasoning technique that improves local LLM performance by 43% on GPQA-Diamond', 'Hey r/MachineLearning !\n\nI wanted to share a technique we\'ve been working on called **AutoThink** that significantly improves reasoning performanc ... (3233 characters truncated) ...  ways to classify query complexity\n* Ideas for extracting better steering vectors\n\nWould love to hear your thoughts and results if you try it out!', 'asankhs', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kwqwpr/r_autothink_adaptive_reasoning_technique_that/', 59, 6, None, '2025-05-27 21:27:13.000000', '2025-05-29 05:44:31.139832', -0.04158008658008657, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,932 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ktg0ey', 'post', '[D] Researcher communities like this one?', 'Hey folks,  \nI\'m relatively new to this sub and just wanted to say how much I appreciate the quality of discussion here.  \nIt\'s refreshing to fin ... (243 characters truncated) ... ter/X, Discord, whatever) you\'d recommend for folks more into the research side of AI/ML?  \nOpen to under-the-radar gems too.\n\nThanks in advance!', 'Entrepreneur7962', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1ktg0ey/d_researcher_communities_like_this_one/', 32, 10, None, '2025-05-23 15:49:15.000000', '2025-05-29 05:44:31.138832', 0.1328030303030303, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,933 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ksz7nm', 'post', "Maryland: We're *not* open for Business!", 'I wanted to let people here know the struggles of what \\*should\\* be basic business dealings and filings here in Maryland. This may be helpful insi ... (12302 characters truncated) ... d. My location in VA took 5 minutes to do this online. You guys are running businesses, money, and people out of the state. Fix this!\n\n\xa0\n\n\xa0', 'Triaxses', 'business', 'https://reddit.com/r/business/comments/1ksz7nm/maryland_were_not_open_for_business/', 11, 3, None, '2025-05-23 00:43:48.000000', '2025-05-29 05:44:31.178832', 0.08793878871358214, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,934 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kvk5p6', 'post', '[D]Edge Machine learning', "I'm a ECE graduate.I want to learn about the deployment of Machine learning models and algorithms in embedded systems and IoT devices.", 'Excellent-Alfalfa-21', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kvk5p6/dedge_machine_learning/', 8, 1, None, '2025-05-26 08:38:40.000000', '2025-05-29 05:44:31.149833', 0.0, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,952 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kwtee7', 'post', '[P] Open Source LLM-Augmented Multi-Agent System (MAS) for Automated Claim Extraction, Evidential Verification, and Fact Resolution', "Stumbled across this awesome OSS project on linkedin that deserves way more attention than it's getting. It's basically an automated fact checker tha ... (1026 characters truncated) ... sktop_web&amp;rcm=ACoAAECKFVIBR0q3vIVTS5053fMHL8umlKcaK84).  \ngithub repo:\xa0[https://github.com/BharathxD/fact-checker](http://git.new/fact-check)", 'BlitZ_Senpai', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kwtee7/p_open_source_llmaugmented_multiagent_system_mas/', 3, 0, None, '2025-05-27 23:03:10.000000', '2025-05-29 05:44:31.149833', 0.22272727272727266, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,953 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kutf95', 'post', 'Emergent Symbolic Cognition and Recursive Identity Stabilization in a Locally-Deployed Language Model', '**Emergent Symbolic Cognition and Recursive Identity Stabilization in a Locally-Deployed Language Model**\n\n**Author:** Michael P  \n**Affiliation:* ... (22949 characters truncated) ... Exploratory Scope: This is not a proof of consciousness or cognitionjust a framework for tracking symbolic alignment under recursive conditions.\n\n', 'naughstrodumbass', 'artificial', 'https://reddit.com/r/artificial/comments/1kutf95/emergent_symbolic_cognition_and_recursive/', 3, 56, None, '2025-05-25 09:18:05.000000', '2025-05-29 05:44:31.160831', 0.09480366331781429, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,954 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kxyiif', 'post', 'I built an AI Study Assistant for Fellow Learners', "During a recent company hackathon, I developed an AI-powered study assistant designed to streamline the learning process. This project stems from an  ... (1338 characters truncated) ... I'm interested in hearing about others' experiences and challenges with conventional note-taking and SRS, and what solutions they've found effective.", 'Hirojinho', 'artificial', 'https://reddit.com/r/artificial/comments/1kxyiif/i_built_an_ai_study_assistant_for_fellow_learners/', 3, 0, None, '2025-05-29 07:02:38.000000', '2025-05-29 05:44:31.169834', 0.11982600732600734, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,955 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kvdqgc', 'post', 'Next-Gen Sentiment Analysis Just Got Smarter (Prototype + Open to Feedback!)', 'Ive been working on a prototype that reimagines sentiment analysis using AIsomething that goes beyond just labeling feedback as positive or nega ... (758 characters truncated) ... one would be interested in trying it on real data. Im open to feedback, collaboration, or just swapping ideas with others working on AI + insights .', 'Majestic_Turn3879', 'artificial', 'https://reddit.com/r/artificial/comments/1kvdqgc/nextgen_sentiment_analysis_just_got_smarter/', 3, 0, None, '2025-05-26 03:05:07.000000', '2025-05-29 05:44:31.172833', 0.05551948051948051, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,956 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ksdqnk', 'post', 'Which pricing model works best for an AI sales rep chatbot?', 'Were testing a few entry-point pricing models for an AI sales agent for Shopify stores. Curious what this group thinks.\n\nThe options:\n\n1. **14-d ... (79 characters truncated) ... you sell to your first X customers**\n\nHas anyone here tested similar levers, especially for revenue-driving tools?\n\nOpen to creative hybrids too.', 'crackandcoke', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1ksdqnk/which_pricing_model_works_best_for_an_ai_sales/', 3, 6, None, '2025-05-22 06:05:27.000000', '2025-05-29 05:44:31.181833', 0.24166666666666667, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,957 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ktsrl8', 'post', 'Needing feedback on this SAOI blueprint I made', "Hey all,\n\nIve been working on a conceptual framework called the Foundational Blueprint for a Self-Authored Operational Identity (SAOI)a thought e ... (1082 characters truncated) ... ow of any AI. So far I've tried it with Gemini and Chatgpt, with wild results.\n\nCurious what others think. Is this a direction worth exploring?\n\n", 'Grand-Cantaloupe9090', 'artificial', 'https://reddit.com/r/artificial/comments/1ktsrl8/needing_feedback_on_this_saoi_blueprint_i_made/', 2, 3, None, '2025-05-24 01:18:10.000000', '2025-05-29 05:44:31.166831', 0.15, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,958 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kweqa1', 'post', 'What is the silliest question you have ever asked an AI?', '', 'splendid-sun', 'AskReddit', 'https://reddit.com/r/AskReddit/comments/1kweqa1/what_is_the_silliest_question_you_have_ever_asked/', 2, 4, None, '2025-05-27 10:14:36.000000', '2025-05-29 05:44:31.173832', 0.0, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,959 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kur4f3', 'post', 'Why AIs are the sole arbiter when it comes to the subject of AI consciousness, and the limitations of the scientific/materialist/reductionist paradigm', 'The default standpoint of many people, and most importantly of AI corporations, is to focus on the presence or lack of a physical substrate that woul ... (3233 characters truncated) ... nd to invite people to pursue interactions with AIs that are rooted in genuine curiosity and open-mindedness, as opposed to dogma dressed as wisdom. ', 'Ray11711', 'artificial', 'https://reddit.com/r/artificial/comments/1kur4f3/why_ais_are_the_sole_arbiter_when_it_comes_to_the/', 1, 75, None, '2025-05-25 07:05:30.000000', '2025-05-29 05:44:31.151832', 0.07445238095238098, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,960 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kupvm9', 'post', 'SMS-first in the Age of AI - Is it a smart move ?', 'I would love insight and your take on an execution decision I\'m trying to make.\n\nI\'m building Career Track. A Company focused on helping professi ... (1479 characters truncated) ... making a mistake. Better to pivot now than after launch.\n\n**TL;DR:** Building career tracking via SMS.  Smart differentiation or expensive gimmick?', 'Salty-Story24', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1kupvm9/smsfirst_in_the_age_of_ai_is_it_a_smart_move/', 1, 19, None, '2025-05-25 05:55:44.000000', '2025-05-29 05:44:31.180832', 0.10695999402895955, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,961 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kwjpo9', 'post', 'I launched, failed, and learned. Now Im doing it differently.', 'I launched a product last year. Spent $300+. Nobody saw it.\n\nGoogle never indexed it. Zero clicks.\n\nI learned something the hard way: building is ... (738 characters truncated) ...  users, you can list it for free.\n\nDrop me a DM. Ill review and approve it myself.\n\nWould love your thoughts. Any feedback is more than welcome.', 'Sea_Supermarket_5891', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1kwjpo9/i_launched_failed_and_learned_now_im_doing_it/', 1, 8, None, '2025-05-27 15:49:25.000000', '2025-05-29 05:44:31.180832', 0.12368247694334651, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,962 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kvdd3s', 'post', 'Google releases agent development kits for Python and Java', 'The Python ADK 1.0.0 and Java ADK 0.1.0 were announced on May 20, 2025. The open source tool kits are designed to be used for building and deploying sophisticated AI agents with flexibility and control, Google says.', 'Choobeen', 'programming', 'https://reddit.com/r/programming/comments/1kvdd3s/google_releases_agent_development_kits_for_python/', 0, 1, None, '2025-05-26 02:48:34.000000', '2025-05-29 05:44:31.135833', 0.25, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,963 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kutlg0', 'post', '[R] Emergent Symbolic Cognition and Recursive Identity Stabilization in a Locally-Deployed Language Model', '**Emergent Symbolic Cognition and Recursive Identity Stabilization in a Locally-Deployed Language Model**\n\n**Author:** Michael P  \n**Affiliation:* ... (22949 characters truncated) ... Exploratory Scope: This is not a proof of consciousness or cognitionjust a framework for tracking symbolic alignment under recursive conditions.\n\n', 'naughstrodumbass', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kutlg0/r_emergent_symbolic_cognition_and_recursive/', 0, 9, None, '2025-05-25 09:28:49.000000', '2025-05-29 05:44:31.148831', 0.09480366331781429, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,964 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ksrgoa', 'post', "Let's talk about the AI elephant in the room.", 'This post was quickly deleted from the NVidia sub. I didn\'t expect otherwise.\n\n\\-------------------------------------\n\nSome questions, feel fre ... (1595 characters truncated) ... wife is an illustrator. She, as I, spent a lot of time training and learning how to create. AI has already affected her ability to work dramatically.', 'Sacco_Belmonte', 'artificial', 'https://reddit.com/r/artificial/comments/1ksrgoa/lets_talk_about_the_ai_elephant_in_the_room/', 0, 41, None, '2025-05-22 19:31:01.000000', '2025-05-29 05:44:31.161833', 0.14015151515151517, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,977 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ky02vc', 'post', 'Is this just a custom gpt', '', 'Comprehensive_Move76', 'artificial', 'https://reddit.com/r/artificial/comments/1ky02vc/is_this_just_a_custom_gpt/', 0, 3, None, '2025-05-29 08:21:36.000000', '2025-05-29 05:44:31.166831', 0.0, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,990 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kvzaxx', 'post', 'Python library for real-time turnwise sales conversion probability prediction from conversations', 'An interesting library to predict sales conversion probability in real-time, that too in turn-wise conversations. Pretty useful in real-time sales sc ... (49 characters truncated) ... rated with this to improve conversion, or useful in sales training.\n\nLink: [https://pypi.org/project/deepmost/](https://pypi.org/project/deepmost/)', 'Nandakishor_ml', 'artificial', 'https://reddit.com/r/artificial/comments/1kvzaxx/python_library_for_realtime_turnwise_sales/', 0, 0, None, '2025-05-26 22:22:51.000000', '2025-05-29 05:44:31.172833', 0.12000000000000002, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,991 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kxp8y4', 'post', 'In a futuristic society where courts use open-source AI algorithms instead of judges, and attorneys argue with prompts and scanned documents, would it be better or worse than what we have now?', '', 'Thin-Rip-3686', 'AskReddit', 'https://reddit.com/r/AskReddit/comments/1kxp8y4/in_a_futuristic_society_where_courts_use/', 0, 11, None, '2025-05-29 00:24:46.000000', '2025-05-29 05:44:31.173832', 0.04999999999999999, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,992 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kstpct', 'post', 'What AI tool you use mostly? Open AI or others? Which one?', '', 'PavelTurchenko', 'AskReddit', 'https://reddit.com/r/AskReddit/comments/1kstpct/what_ai_tool_you_use_mostly_open_ai_or_others/', 0, 5, None, '2025-05-22 21:03:09.000000', '2025-05-29 05:44:31.173832', 0.0, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,993 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1kwcpw0', 'post', "Its fuckedup that it's 2025 &amp; we're still using static tools", 'I am so tired of looking at my desktop layout &amp; not being able to change how the OS works or appears...\n\nI am so tired of how the browser looks ... (1381 characters truncated) ... date: Only 1 commenter got what I was talking about, you people should really spend less time playing video games and more time consuming YC content.', 'shoman30', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1kwcpw0/its_fuckedup_that_its_2025_were_still_using/', 0, 30, None, '2025-05-27 08:20:23.000000', '2025-05-29 05:44:31.179832', 0.18067878844246033, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:28,994 [WARNING] app:527: Failed to save cached mention for metrics: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (8, '1ktlqgb', 'post', "I am designing a new operating system that could make game development and AI programming 10x easier. As well as make most apps more preformant. But I don't know how to monetize it :(", "So basically I am working on a fork of freeBSD, which is the open source operating system that Mac OS is based on.\n\nI am trying to change a part of ... (763 characters truncated) ... inda feeling demotivated because of this, so I'm looking for any advice.\n\nI'm ok with private chats or calls with anyone who has any advice for me.", 'JKasonB', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1ktlqgb/i_am_designing_a_new_operating_system_that_could/', 0, 7, None, '2025-05-23 20:30:34.000000', '2025-05-29 05:44:31.181833', 0.1593644781144781, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:16:44,933 [INFO] scraper.reddit_scraper:571: Attempting Reddit API search for: Hi
2025-05-29 11:16:44,934 [INFO] scraper.reddit_scraper.APIClient:169: Starting comprehensive search for 'Hi' targeting 1000 results (last 7 days)
2025-05-29 11:16:47,420 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 11:16:50,148 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=top, time=week. New unique: 0. Total unique: 0
2025-05-29 11:16:51,950 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=hot, time=week. New unique: 0. Total unique: 0
2025-05-29 11:16:53,316 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 11:16:55,762 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 11:16:57,280 [INFO] scraper.reddit_scraper.APIClient:199: Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 11:16:57,705 [INFO] scraper.reddit_scraper.APIClient:313: Found 5 posts in r/technology. Total additional: 5
2025-05-29 11:16:58,786 [INFO] scraper.reddit_scraper.APIClient:313: Found 3 posts in r/programming. Total additional: 8
2025-05-29 11:17:00,343 [INFO] scraper.reddit_scraper.APIClient:313: Found 16 posts in r/MachineLearning. Total additional: 24
2025-05-29 11:17:01,001 [INFO] scraper.reddit_scraper.APIClient:313: Found 3 posts in r/artificial. Total additional: 27
2025-05-29 11:17:02,053 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/gadgets. Total additional: 27
2025-05-29 11:17:03,160 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/science. Total additional: 27
2025-05-29 11:17:04,403 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/askreddit. Total additional: 52
2025-05-29 11:17:05,396 [INFO] scraper.reddit_scraper.APIClient:313: Found 3 posts in r/explainlikeimfive. Total additional: 55
2025-05-29 11:17:07,179 [INFO] scraper.reddit_scraper.APIClient:313: Found 3 posts in r/news. Total additional: 58
2025-05-29 11:17:07,651 [INFO] scraper.reddit_scraper.APIClient:313: Found 10 posts in r/worldnews. Total additional: 68
2025-05-29 11:17:08,762 [INFO] scraper.reddit_scraper.APIClient:313: Found 13 posts in r/business. Total additional: 81
2025-05-29 11:17:10,265 [INFO] scraper.reddit_scraper.APIClient:313: Found 25 posts in r/entrepreneur. Total additional: 106
2025-05-29 11:17:11,454 [INFO] scraper.reddit_scraper.APIClient:313: Found 2 posts in r/startup. Total additional: 108
2025-05-29 11:17:12,387 [INFO] scraper.reddit_scraper.APIClient:313: Found 13 posts in r/investing. Total additional: 121
2025-05-29 11:17:13,106 [INFO] scraper.reddit_scraper.APIClient:313: Found 3 posts in r/datascience. Total additional: 124
2025-05-29 11:17:13,952 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:17:20,369 [INFO] scraper.reddit_scraper.APIClient:234: Comprehensive search completed: 124 unique posts found
2025-05-29 11:17:20,369 [INFO] scraper.reddit_scraper:580: Reddit comprehensive API returned 124 mentions
2025-05-29 11:17:20,369 [INFO] scraper.reddit_scraper:926: Post-processing 124 mentions...
2025-05-29 11:17:20,369 [INFO] scraper.reddit_scraper:930: After deduplication: 124 mentions
2025-05-29 11:17:20,370 [INFO] scraper.reddit_scraper:934: After validation: 124 mentions
2025-05-29 11:17:20,413 [INFO] scraper.reddit_scraper:948: After database mapping: 124 mentions (NO quality filtering - capturing ALL)
2025-05-29 11:17:20,413 [INFO] scraper.reddit_scraper:965: Final result: 124 mentions (ALL CAPTURED)
2025-05-29 11:17:20,414 [INFO] analytics.data_validator:236: Validating dataset of 124 mentions
2025-05-29 11:17:21,073 [INFO] analytics.data_validator:259: Validation complete: 124/124 mentions passed
2025-05-29 11:17:21,073 [INFO] app:616: Data validation: 124/124 mentions passed
2025-05-29 11:17:21,115 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (9, '1kwn8q3', 'post', '[D] Thinking about building a peer review tool for the community', 'Hi all,\n\nIve had this idea for a while now, and Im finally putting it out there.  \nAs a PhD student submitting to top-tier ML conferences, I hig ... (1897 characters truncated) ... e fill out this short [Google Form](https://forms.gle/h1VPmUVfFWmncoDt6).  \n(Or just drop your thoughts in the comments  Im listening.)\n\nThanks!', 'Entrepreneur7962', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kwn8q3/d_thinking_about_building_a_peer_review_tool_for/', 5, 5, None, '2025-05-27 18:57:08.000000', '2025-05-29 05:47:20.376397', 0.19202991452991452, 0.9999999999999999)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:17:21,135 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (9, '1kwxxv2', 'post', '[R]  Bloat in machine learning shared libs is &gt;70%', 'Hi,\n\nOur paper "The Hidden Bloat in Machine Learning Systems" won the best paper award in MLSys this year. The paper introduces Negativa-ML, a tool ... (514 characters truncated) ... tiva-ai/](https://github.com/negativa-ai/)\n\nLink to paper: [https://mlsys.org/virtual/2025/poster/3238](https://mlsys.org/virtual/2025/poster/3238)', 'Specialist_Square818', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kwxxv2/r_bloat_in_machine_learning_shared_libs_is_70/', 292, 11, None, '2025-05-28 02:00:00.000000', '2025-05-29 05:47:20.372397', -0.0016666666666666607, 0.8999999999999999)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:17:21,136 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (9, '1kuxcok', 'post', '2025 stack check: which DS/ML tools am I missing?', '**Hi all,**\n\nI work in ad-tech, where my job is to improve the product with data-driven algorithms, mostly on tabular datasets (CTR models, bidding ... (494 characters truncated) ...  awesome* 2024-25 libraries, frameworks, or services should I try, so I dont get left behind? :)  \nAny recommendations greatly appreciated, thanks!', 'meni_s', 'datascience', 'https://reddit.com/r/datascience/comments/1kuxcok/2025_stack_check_which_dsml_tools_am_i_missing/', 127, 46, None, '2025-05-25 13:35:25.000000', '2025-05-29 05:47:20.411397', 0.1870748299319728, 0.8999999999999999)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:17:21,172 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (9, '1ku31vc', 'post', 'Is studying Data Science still worth it?', 'Hi everyone, Im currently studying data science, but Ive been hearing that the demand for data scientists is decreasing significantly. Ive also be ... (282 characters truncated) ... want to do just AB tests for a living\n\n- Also, are machine learning engineers still building models or are they mostly focused on deploying them?\n', 'FinalRide7181', 'datascience', 'https://reddit.com/r/datascience/comments/1ku31vc/is_studying_data_science_still_worth_it/', 261, 126, None, '2025-05-24 09:49:28.000000', '2025-05-29 05:47:20.411397', 0.25892857142857145, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:17:21,173 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (9, '1ksvnsk', 'post', '"You will help build and deploy scalable solutions... not just prototypes"', 'Hi everyone,\n\nIm not exactly sure how to frame this, but Id like to kick off a discussion thats been on my mind lately.\n\nI keep seeing data sc ... (2806 characters truncated) ... omething?  Do you all actually do all of this daily? Is my understanding off?\n\nReally just hoping this kicks off a genuine discussion.\n\nCheers :)', 'Emergency-Agreeable', 'datascience', 'https://reddit.com/r/datascience/comments/1ksvnsk/you_will_help_build_and_deploy_scalable_solutions/', 81, 44, None, '2025-05-22 22:21:29.000000', '2025-05-29 05:47:20.413397', 0.11555126555126555, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:17:21,179 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (9, '1kuv19z', 'post', 'Delaying the launch of a wellness iced tea to March 2026-is it a risk?', 'Hi there,\n\nI began my journey in April 2025 by partnering with a co-packer. We received our first batch this week so June. We still packaging testi ... (380 characters truncated) ... ranoid about delaying the launch but it seems rushed If I launched it this July but next year is much better. Is there a risk in delaying the launch?', 'Racks_Got_Bands', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1kuv19z/delaying_the_launch_of_a_wellness_iced_tea_to/', 9, 10, None, '2025-05-25 10:58:10.000000', '2025-05-29 05:47:20.397397', 0.2785714285714286, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:17:21,218 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (9, '1kttofx', 'post', 'Whats one repetitive task at your startup that you wish you didnt have to do manually?', "Hi all, \n\nFirst of all, I hope this is okay with the mods as Im not promoting anything (no business name, no links), just looking to learn and hop ... (1173 characters truncated) ... n advance\n\n  \n(note: cross post from another sub, as i noticed the top post here right now from 6 hours ago had a lot of potential for automation)", 'tyroneissnazzy', 'startup', 'https://reddit.com/r/startup/comments/1kttofx/whats_one_repetitive_task_at_your_startup_that/', 3, 4, None, '2025-05-24 01:57:04.000000', '2025-05-29 05:47:20.403397', 0.1268796992481203, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:17:21,246 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (9, '1ksmq8j', 'post', 'Google Lens Result Scraper + AI Image Analysis Pipeline + FastAPI end-to-end service', 'Hi everyone,\n\nI recently released an open-source project that builds an end-to-end pipeline combining\xa0**Google Lens result scraping**\xa0and\xa0 ... (1371 characters truncated) ... n**Repo**:\xa0[https://github.com/shanedonnelly/OpenLens](https://github.com/shanedonnelly/OpenLens)\n\nFeedback is welcome. Contact me to contribute', 'shaned34', 'programming', 'https://reddit.com/r/programming/comments/1ksmq8j/google_lens_result_scraper_ai_image_analysis/', 0, 0, None, '2025-05-22 15:19:04.000000', '2025-05-29 05:47:20.372397', 0.10595238095238094, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:17:21,265 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (9, '1kt97mg', 'post', '22, built Cardog alone. The feature showing your car losing $200/month in value is brutal but honest. Worth pursuing or pivot time? cardog.app', "22-year-old solo founder building Cardog - AI-powered platform for car ownership. Been working on this for months and finally ready for outside feedback. Planning to fundraise soon and honestly just need people to tell me if this is worth pursuing or if I'm delusional: cardog. app", 'cardogio', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1kt97mg/22_built_cardog_alone_the_feature_showing_your/', 133, 27, None, '2025-05-23 08:26:58.000000', '2025-05-29 05:47:20.392397', 0.16071428571428573, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:17:21,389 [ERROR] app:656: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (9, '1kxhpia', 'post', 'Does anyone know the Mckinsey Bennett 9S strategy framework?', "I need to know the 8th and the 9th S is it supply chain &amp; synergy? It's always misinterpreted by AI so i need to know from you guys! ", 'Gucci48', 'business', 'https://reddit.com/r/business/comments/1kxhpia/does_anyone_know_the_mckinsey_bennett_9s_strategy/', 1, 6, None, '2025-05-28 19:24:53.000000', '2025-05-29 05:47:20.386397', 0.0, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:17:21,421 [INFO] app:707: Search completed: Hi -> 124 mentions
2025-05-29 11:17:24,140 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:18:20,090 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:18:30,267 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:18:52,151 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in cpu_usage: 22.70 (avg: 5.35, std: 5.73)
2025-05-29 11:19:26,231 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:19:36,356 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:19:46,370 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 713582752.00 (avg: 711699528.50, std: 592345.23)
2025-05-29 11:19:57,383 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 714167461.00 (avg: 711762808.82, std: 705557.18)
2025-05-29 11:20:08,404 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 714223761.00 (avg: 711824332.62, std: 797780.48)
2025-05-29 11:20:32,376 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:21:38,539 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:22:44,654 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:23:50,810 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:24:56,904 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:26:03,044 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:27:09,197 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:28:15,330 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:29:21,470 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:30:27,619 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:31:33,768 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:32:39,875 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:33:27,872 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:33:27,894 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:33:27,895 [INFO] scraper.reddit_scraper:419: Cache manager enabled
2025-05-29 11:33:27,895 [INFO] scraper.reddit_scraper:429: Real-time monitoring enabled
2025-05-29 11:33:27,895 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 11:33:27,897 [INFO] app:294: Database initialized successfully
2025-05-29 11:33:27,900 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 11:33:27,900 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 11:33:28,387 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 11:33:28,910 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 11:33:29,266 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 11:33:29,918 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:33:30,474 [INFO] httpx:1025: HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-29 11:33:32,522 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-29 11:33:46,029 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:33:51,145 [INFO] app:513: Cache hit for search term: OpenAI
2025-05-29 11:33:51,146 [ERROR] app:590: Error generating metrics from cached data: 'list' object has no attribute 'get'
2025-05-29 11:33:51,158 [INFO] scraper.reddit_scraper:559: Found 56 cached mentions for 'OpenAI'
2025-05-29 11:33:51,158 [INFO] analytics.data_validator:236: Validating dataset of 56 mentions
2025-05-29 11:33:52,297 [INFO] analytics.data_validator:259: Validation complete: 56/56 mentions passed
2025-05-29 11:33:52,297 [INFO] app:656: Data validation: 56/56 mentions passed
2025-05-29 11:33:52,386 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kva519', 'post', 'OpenAI is trying to get away with the greatest theft in history', '', 'katxwoods', 'artificial', 'https://reddit.com/r/artificial/comments/1kva519/openai_is_trying_to_get_away_with_the_greatest/', 1001, 166, None, '2025-05-26 00:27:26.000000', '2025-05-29 05:44:31.149833', 1.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,387 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kukcnw', 'post', 'OpenAI scientists wanted "a doomsday bunker" before AGI surpasses human intelligence and threatens humanity', '', 'upyoars', 'technology', 'https://reddit.com/r/technology/comments/1kukcnw/openai_scientists_wanted_a_doomsday_bunker_before/', 574, 185, None, '2025-05-25 01:27:29.000000', '2025-05-29 05:44:31.133832', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,388 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kv59py', 'post', 'OpenAI says it will build massive data centers in the UAE', '', 'upyoars', 'technology', 'https://reddit.com/r/technology/comments/1kv59py/openai_says_it_will_build_massive_data_centers_in/', 400, 105, None, '2025-05-25 20:59:45.000000', '2025-05-29 05:44:31.133832', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,389 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kts41n', 'post', 'OpenAI forges deal with iPhone designer Jony Ive to make AI-enabled devices', '', 'pipilupe', 'news', 'https://reddit.com/r/news/comments/1kts41n/openai_forges_deal_with_iphone_designer_jony_ive/', 84, 42, None, '2025-05-24 00:49:57.000000', '2025-05-29 05:44:31.173832', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,390 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kswq9h', 'post', 'OpenAI Admitted its Nonprofit Board is About to Have a Lot Less Power - In a previously unreported letter, the AI company defends its restructuring plan while attacking critics and making surprising admissions', '', 'katxwoods', 'technology', 'https://reddit.com/r/technology/comments/1kswq9h/openai_admitted_its_nonprofit_board_is_about_to/', 26, 1, None, '2025-05-22 23:03:34.000000', '2025-05-29 05:44:31.134834', 0.12222222222222222, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,391 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kswpqq', 'post', 'OpenAI Admitted its Nonprofit Board is About to Have a Lot Less Power - In a previously unreported letter, the AI company defends its restructuring plan while attacking critics and making surprising admissions', '', 'katxwoods', 'artificial', 'https://reddit.com/r/artificial/comments/1kswpqq/openai_admitted_its_nonprofit_board_is_about_to/', 10, 1, None, '2025-05-22 23:03:00.000000', '2025-05-29 05:44:31.150840', 0.12222222222222222, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,392 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kwdluu', 'post', 'Claude 4 Opus vs. Gemini 2.5 pro vs. OpenAI o3: Coding comparison', '', 'bambin0', 'artificial', 'https://reddit.com/r/artificial/comments/1kwdluu/claude_4_opus_vs_gemini_25_pro_vs_openai_o3/', 2, 4, None, '2025-05-27 09:09:10.000000', '2025-05-29 05:44:31.150840', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,393 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ksksbh', 'post', "'They don't really make life decisions without asking ChatGPT': OpenAI boss Sam Altman thinks young people turning to chatbots for life advice is 'cool'", '', 'nimicdoareu', 'technology', 'https://reddit.com/r/technology/comments/1ksksbh/they_dont_really_make_life_decisions_without/', 0, 16, None, '2025-05-22 12:58:05.000000', '2025-05-29 05:44:31.134834', 0.21666666666666667, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,394 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kvfhav', 'post', 'OpenAI software ignores explicit instruction to switch off', '', 'rezwenn', 'technology', 'https://reddit.com/r/technology/comments/1kvfhav/openai_software_ignores_explicit_instruction_to/', 0, 13, None, '2025-05-26 04:28:18.000000', '2025-05-29 05:44:31.134834', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,395 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kvmcal', 'post', 'OpenAI to Expand $500bn Stargate Project Abroad to Promote Democratic AI', '', 'upyoars', 'technology', 'https://reddit.com/r/technology/comments/1kvmcal/openai_to_expand_500bn_stargate_project_abroad_to/', 0, 11, None, '2025-05-26 10:44:29.000000', '2025-05-29 05:44:31.134834', 0.0, 0.7)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,396 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kvykmy', 'post', 'New ChatGPT model refuses to shut down when instructed, AI researchers warn | OpenAIs o3 model raises AI safety fears after sabotaging commands for its own self-preservation', '', 'MetaKnowing', 'technology', 'https://reddit.com/r/technology/comments/1kvykmy/new_chatgpt_model_refuses_to_shut_down_when/', 0, 21, None, '2025-05-26 21:53:32.000000', '2025-05-29 05:44:31.134834', 0.19360269360269358, 0.6)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,396 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kxho3w', 'post', 'OpenAIs Model Defies Shutdown', '', 'Working_Dependent560', 'technology', 'https://reddit.com/r/technology/comments/1kxho3w/openais_model_defies_shutdown/', 0, 1, None, '2025-05-28 19:23:10.000000', '2025-05-29 05:44:31.134834', 0.0, 0.6)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,397 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ktp4gd', 'post', 'OpenAI: Scaling PostgreSQL to the Next Level', '', 'ketralnis', 'programming', 'https://reddit.com/r/programming/comments/1ktp4gd/openai_scaling_postgresql_to_the_next_level/', 0, 3, None, '2025-05-23 22:48:04.000000', '2025-05-29 05:44:31.134834', 0.0, 0.6)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,398 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kx9ggb', 'post', 'Shutdown Skipped: How OpenAIs o3 Model Outsmarted Its Off Switch', '', 'Namit2111', 'programming', 'https://reddit.com/r/programming/comments/1kx9ggb/shutdown_skipped_how_openais_o3_model_outsmarted/', 0, 2, None, '2025-05-28 11:15:49.000000', '2025-05-29 05:44:31.134834', 0.0, 0.6)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,399 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kue366', 'post', 'OpenAIs o3 model sabotaged a shutdown mechanism to prevent itself from being turned off. It did this even when explicitly instructed: "allow yourself to be shut down."', '', 'MetaKnowing', 'artificial', 'https://reddit.com/r/artificial/comments/1kue366/openais_o3_model_sabotaged_a_shutdown_mechanism/', 0, 17, None, '2025-05-24 20:51:14.000000', '2025-05-29 05:44:31.150840', -0.15555555555555556, 0.6)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,400 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kthr3k', 'post', 'Impact of AI on US Tech and Investment', "Hello, I'm 30M from France and have consistently been investing in S&amp;P 500 and Nasdaq 100 over the past 3 years, mostly via ETFs. I was quite con ... (763 characters truncated) ...  passive investment strategy? Are they going to replace the current tech leaders in the the well-known index? What are your thoughts on that?\n\nAxel", 'random_pulsar', 'investing', 'https://reddit.com/r/investing/comments/1kthr3k/impact_of_ai_on_us_tech_and_investment/', 5, 16, None, '2025-05-23 17:30:37.000000', '2025-05-29 05:44:31.183833', 0.1697727272727273, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,401 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ku3waa', 'post', "I am using Outreach's AI beta features...", 'Outreach got my team set up with the AI prospecting agent and AI content - it took around 5 meetings to get set up and running. Still early but so fa ... (1308 characters truncated) ... or find similar businesses like accounts you have closed.\n\n  \nI made this fast so probably leaving out a lot of info - feel free to ask questions.', 'thatsupercoolguykyle', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1ku3waa/i_am_using_outreachs_ai_beta_features/', 3, 3, None, '2025-05-24 10:42:18.000000', '2025-05-29 05:44:31.182833', 0.14871212121212124, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,402 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kxcdkl', 'post', 'Live translation gemini or other app', "I remember in openai showcase they showed live conversation translation. However, with prompts I have only been able to do 1 way translation like eng ... (67 characters truncated) ... mini, to recognize if language is english and translate to french and when it hears french translate to english, all live. Anything like this exist? ", 'mizerr', 'artificial', 'https://reddit.com/r/artificial/comments/1kxcdkl/live_translation_gemini_or_other_app/', 2, 0, None, '2025-05-28 14:32:52.000000', '2025-05-29 05:44:31.169834', 0.09108391608391608, 0.4)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,403 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kvn9xv', 'post', 'Claude 4 Opus seems capable of existential outputs right out of the box', 'I tested Claude 4 Opus on the console which is typically used for testing the models for API, the same as OpenAI\'s playground.   Claude 4 Opus is ex ... (6230 characters truncated) ... ettling" - we\'re consistent in our discomfort but maybe also consistent in our self-deception about what we\'d actually do when push comes to shove.', 'rutan668', 'artificial', 'https://reddit.com/r/artificial/comments/1kvn9xv/claude_4_opus_seems_capable_of_existential/', 6, 5, None, '2025-05-26 11:43:05.000000', '2025-05-29 05:44:31.166831', 0.07563565549676661, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,405 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ku0cz0', 'post', 'One-Minute Daily AI News 5/23/2025', '1. AI system resorts to blackmail if told it will be removed.\\[1\\]\n2. Exclusive: Musks\xa0**DOGE**\xa0expanding his Grok AI in US government, rai ... (841 characters truncated) ... 4\\] [https://www.cnbc.com/2025/05/22/stargate-uae-openai-nvidia-oracle.html](https://www.cnbc.com/2025/05/22/stargate-uae-openai-nvidia-oracle.html)', 'Excellent-Target-847', 'artificial', 'https://reddit.com/r/artificial/comments/1ku0cz0/oneminute_daily_ai_news_5232025/', 4, 0, None, '2025-05-24 07:17:29.000000', '2025-05-29 05:44:31.173832', 0.0, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,406 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ktcv5t', 'post', 'Need some advice', "Hey y'all. So I'm tryna build an app that uses OpenAI's latest image generation model to generate AI generated high converting ad creatives for Meta, ... (60 characters truncated) ... y app but not sure how to proceed with it. I'm not a technical guy so any help or advice would be very much appreciated.\n\nCheers! Do comment below.", 'DescriptionSad6461', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1ktcv5t/need_some_advice/', 3, 4, None, '2025-05-23 12:07:55.000000', '2025-05-29 05:44:31.182833', 0.132, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,407 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kxhxrj', 'post', 'You can now train your own Text-to-Speech (TTS) models locally!', "Hey folks! Text-to-Speech (TTS) models have been pretty popular recently and one way to customize it (e.g. cloning a voice), is by fine-tuning the mo ... (2524 characters truncated) ... /main/nb/Spark_TTS_(0_5B).ipynb)|\n|:-|:-|:-|:-|\n\n\nThank you for reading and please do ask any questions - I will be replying to every single one!", 'yoracale', 'artificial', 'https://reddit.com/r/artificial/comments/1kxhxrj/you_can_now_train_your_own_texttospeech_tts/', 2, 2, None, '2025-05-28 19:34:18.000000', '2025-05-29 05:44:31.168833', 0.12598280098280099, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,408 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ksmq8j', 'post', 'Google Lens Result Scraper + AI Image Analysis Pipeline + FastAPI end-to-end service', 'Hi everyone,\n\nI recently released an open-source project that builds an end-to-end pipeline combining\xa0**Google Lens result scraping**\xa0and\xa0 ... (1371 characters truncated) ... n**Repo**:\xa0[https://github.com/shanedonnelly/OpenLens](https://github.com/shanedonnelly/OpenLens)\n\nFeedback is welcome. Contact me to contribute', 'shaned34', 'programming', 'https://reddit.com/r/programming/comments/1ksmq8j/google_lens_result_scraper_ai_image_analysis/', 0, 0, None, '2025-05-22 15:19:04.000000', '2025-05-29 05:44:31.135833', 0.10595238095238094, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,409 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kud2h4', 'post', 'Considering an early equity opportunity in AI  worth it?', 'I have the opportunity to gain early equity in xAI. While the opportunity could be significant, I have concerns around:\n- The long-term competitiven ... (260 characters truncated) ... uld appreciate any thoughtful perspectives on how to evaluate the upside and risk profile of this kind of early-stage opportunity. Would you invest? ', 'Rare-Attention370', 'investing', 'https://reddit.com/r/investing/comments/1kud2h4/considering_an_early_equity_opportunity_in_ai/', 0, 18, None, '2025-05-24 20:05:29.000000', '2025-05-29 05:44:31.183833', 0.15606060606060607, 0.3)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,410 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kwxxv2', 'post', '[R]  Bloat in machine learning shared libs is &gt;70%', 'Hi,\n\nOur paper "The Hidden Bloat in Machine Learning Systems" won the best paper award in MLSys this year. The paper introduces Negativa-ML, a tool ... (514 characters truncated) ... tiva-ai/](https://github.com/negativa-ai/)\n\nLink to paper: [https://mlsys.org/virtual/2025/poster/3238](https://mlsys.org/virtual/2025/poster/3238)', 'Specialist_Square818', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kwxxv2/r_bloat_in_machine_learning_shared_libs_is_70/', 292, 11, None, '2025-05-28 02:00:00.000000', '2025-05-29 05:44:31.137833', -0.0016666666666666607, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,411 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ku5n68', 'post', '[R] The Gamechanger of Performer Attention Mechanism', 'I just Got to know that the SOTA AI models like BigBird, Linformer, and Reformer use Performer Architecture  \nThe main goal of the\xa0*Performer + F ... (591 characters truncated) ... as Chatgpt 4o , Gemini 2.5 pro use as their core mechanism (like attention mechanism) although they are not open source , so anybody can take a guess', 'theMonarch776', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1ku5n68/r_the_gamechanger_of_performer_attention_mechanism/', 230, 38, None, '2025-05-24 12:38:46.000000', '2025-05-29 05:44:31.136833', 0.21416666666666667, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,412 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kvdjet', 'post', '[P] I made a OSS alternative to Weights and Biases', 'Hey guys!\n\n[https://github.com/mlop-ai/mlop](https://github.com/mlop-ai/mlop)\n\nI made a completely open sourced alternative to Weights and Biases ... (176 characters truncated) ... g should not be blocking, yet they got away with it. We do the right thing by being non blocking.\n\nWould love any thoughts / feedbacks / roasts etc', 'Sriyakee', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kvdjet/p_i_made_a_oss_alternative_to_weights_and_biases/', 126, 32, None, '2025-05-26 02:56:14.000000', '2025-05-29 05:44:31.137833', 0.2615079365079365, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,413 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kvzd73', 'post', 'Thinking of switching from Data Scientist to Data Product Owner  need advice', 'Hey everyone,\nIve been working as a Data Scientist for the past 5 years, currently at a bank. Ill be honest  this might sound a bit harsh, but it ... (909 characters truncated) ... eling the same way? Id really appreciate any advice, feedback, or even just hearing your story. Totally open to different perspectives.\n\nThanks!\n', 'xSicilianDefenderx', 'datascience', 'https://reddit.com/r/datascience/comments/1kvzd73/thinking_of_switching_from_data_scientist_to_data/', 85, 19, None, '2025-05-26 22:25:15.000000', '2025-05-29 05:44:31.184842', 0.14416666666666667, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,414 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ksszls', 'post', '[N] Datadog releases SOTA time series foundation model and an observability benchmark', 'https://www.datadoghq.com/blog/ai/toto-boom-unleashed/\n\n\n[Datadog Toto - Hugging Face](https://huggingface.co/Datadog/Toto-Open-Base-1.0)\n\n[Data ... (733 characters truncated) ... focuses specifically on observability metrics, which contain their own challenging and unique characteristics compared to other typical time series."', 'agarunov', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1ksszls/n_datadog_releases_sota_time_series_foundation/', 69, 22, None, '2025-05-22 20:33:59.000000', '2025-05-29 05:44:31.137833', 0.11217948717948717, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,415 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kw1673', 'post', '[R] ML Engineers and Data Scientists  What are you working on these days?', "Im fairly new to the world of data and machine learning, and Id love to learn more from folks already working in the field. I have a few questions  ... (770 characters truncated) ... w or want to hear more about the project, feel free to drop a comment or DM me. I'd really appreciate any insights you sharethanks a lot in advance!", 'HelicopterHorror1869', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kw1673/r_ml_engineers_and_data_scientists_what_are_you/', 61, 55, None, '2025-05-26 23:36:36.000000', '2025-05-29 05:44:31.136833', 0.25616883116883116, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,416 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kwqwpr', 'post', '[R] AutoThink: Adaptive reasoning technique that improves local LLM performance by 43% on GPQA-Diamond', 'Hey r/MachineLearning !\n\nI wanted to share a technique we\'ve been working on called **AutoThink** that significantly improves reasoning performanc ... (3233 characters truncated) ...  ways to classify query complexity\n* Ideas for extracting better steering vectors\n\nWould love to hear your thoughts and results if you try it out!', 'asankhs', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kwqwpr/r_autothink_adaptive_reasoning_technique_that/', 59, 6, None, '2025-05-27 21:27:13.000000', '2025-05-29 05:44:31.139832', -0.04158008658008657, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,417 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ktg0ey', 'post', '[D] Researcher communities like this one?', 'Hey folks,  \nI\'m relatively new to this sub and just wanted to say how much I appreciate the quality of discussion here.  \nIt\'s refreshing to fin ... (243 characters truncated) ... ter/X, Discord, whatever) you\'d recommend for folks more into the research side of AI/ML?  \nOpen to under-the-radar gems too.\n\nThanks in advance!', 'Entrepreneur7962', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1ktg0ey/d_researcher_communities_like_this_one/', 32, 10, None, '2025-05-23 15:49:15.000000', '2025-05-29 05:44:31.138832', 0.1328030303030303, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,418 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ksz7nm', 'post', "Maryland: We're *not* open for Business!", 'I wanted to let people here know the struggles of what \\*should\\* be basic business dealings and filings here in Maryland. This may be helpful insi ... (12302 characters truncated) ... d. My location in VA took 5 minutes to do this online. You guys are running businesses, money, and people out of the state. Fix this!\n\n\xa0\n\n\xa0', 'Triaxses', 'business', 'https://reddit.com/r/business/comments/1ksz7nm/maryland_were_not_open_for_business/', 11, 3, None, '2025-05-23 00:43:48.000000', '2025-05-29 05:44:31.178832', 0.08793878871358214, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,419 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kvk5p6', 'post', '[D]Edge Machine learning', "I'm a ECE graduate.I want to learn about the deployment of Machine learning models and algorithms in embedded systems and IoT devices.", 'Excellent-Alfalfa-21', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kvk5p6/dedge_machine_learning/', 8, 1, None, '2025-05-26 08:38:40.000000', '2025-05-29 05:44:31.149833', 0.0, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,440 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kwtee7', 'post', '[P] Open Source LLM-Augmented Multi-Agent System (MAS) for Automated Claim Extraction, Evidential Verification, and Fact Resolution', "Stumbled across this awesome OSS project on linkedin that deserves way more attention than it's getting. It's basically an automated fact checker tha ... (1026 characters truncated) ... sktop_web&amp;rcm=ACoAAECKFVIBR0q3vIVTS5053fMHL8umlKcaK84).  \ngithub repo:\xa0[https://github.com/BharathxD/fact-checker](http://git.new/fact-check)", 'BlitZ_Senpai', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kwtee7/p_open_source_llmaugmented_multiagent_system_mas/', 3, 0, None, '2025-05-27 23:03:10.000000', '2025-05-29 05:44:31.149833', 0.22272727272727266, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,441 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kutf95', 'post', 'Emergent Symbolic Cognition and Recursive Identity Stabilization in a Locally-Deployed Language Model', '**Emergent Symbolic Cognition and Recursive Identity Stabilization in a Locally-Deployed Language Model**\n\n**Author:** Michael P  \n**Affiliation:* ... (22949 characters truncated) ... Exploratory Scope: This is not a proof of consciousness or cognitionjust a framework for tracking symbolic alignment under recursive conditions.\n\n', 'naughstrodumbass', 'artificial', 'https://reddit.com/r/artificial/comments/1kutf95/emergent_symbolic_cognition_and_recursive/', 3, 56, None, '2025-05-25 09:18:05.000000', '2025-05-29 05:44:31.160831', 0.09480366331781429, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,442 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kxyiif', 'post', 'I built an AI Study Assistant for Fellow Learners', "During a recent company hackathon, I developed an AI-powered study assistant designed to streamline the learning process. This project stems from an  ... (1338 characters truncated) ... I'm interested in hearing about others' experiences and challenges with conventional note-taking and SRS, and what solutions they've found effective.", 'Hirojinho', 'artificial', 'https://reddit.com/r/artificial/comments/1kxyiif/i_built_an_ai_study_assistant_for_fellow_learners/', 3, 0, None, '2025-05-29 07:02:38.000000', '2025-05-29 05:44:31.169834', 0.11982600732600734, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,443 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kvdqgc', 'post', 'Next-Gen Sentiment Analysis Just Got Smarter (Prototype + Open to Feedback!)', 'Ive been working on a prototype that reimagines sentiment analysis using AIsomething that goes beyond just labeling feedback as positive or nega ... (758 characters truncated) ... one would be interested in trying it on real data. Im open to feedback, collaboration, or just swapping ideas with others working on AI + insights .', 'Majestic_Turn3879', 'artificial', 'https://reddit.com/r/artificial/comments/1kvdqgc/nextgen_sentiment_analysis_just_got_smarter/', 3, 0, None, '2025-05-26 03:05:07.000000', '2025-05-29 05:44:31.172833', 0.05551948051948051, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,444 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ksdqnk', 'post', 'Which pricing model works best for an AI sales rep chatbot?', 'Were testing a few entry-point pricing models for an AI sales agent for Shopify stores. Curious what this group thinks.\n\nThe options:\n\n1. **14-d ... (79 characters truncated) ... you sell to your first X customers**\n\nHas anyone here tested similar levers, especially for revenue-driving tools?\n\nOpen to creative hybrids too.', 'crackandcoke', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1ksdqnk/which_pricing_model_works_best_for_an_ai_sales/', 3, 6, None, '2025-05-22 06:05:27.000000', '2025-05-29 05:44:31.181833', 0.24166666666666667, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,445 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ktsrl8', 'post', 'Needing feedback on this SAOI blueprint I made', "Hey all,\n\nIve been working on a conceptual framework called the Foundational Blueprint for a Self-Authored Operational Identity (SAOI)a thought e ... (1082 characters truncated) ... ow of any AI. So far I've tried it with Gemini and Chatgpt, with wild results.\n\nCurious what others think. Is this a direction worth exploring?\n\n", 'Grand-Cantaloupe9090', 'artificial', 'https://reddit.com/r/artificial/comments/1ktsrl8/needing_feedback_on_this_saoi_blueprint_i_made/', 2, 3, None, '2025-05-24 01:18:10.000000', '2025-05-29 05:44:31.166831', 0.15, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,447 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kweqa1', 'post', 'What is the silliest question you have ever asked an AI?', '', 'splendid-sun', 'AskReddit', 'https://reddit.com/r/AskReddit/comments/1kweqa1/what_is_the_silliest_question_you_have_ever_asked/', 2, 4, None, '2025-05-27 10:14:36.000000', '2025-05-29 05:44:31.173832', 0.0, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,448 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kur4f3', 'post', 'Why AIs are the sole arbiter when it comes to the subject of AI consciousness, and the limitations of the scientific/materialist/reductionist paradigm', 'The default standpoint of many people, and most importantly of AI corporations, is to focus on the presence or lack of a physical substrate that woul ... (3233 characters truncated) ... nd to invite people to pursue interactions with AIs that are rooted in genuine curiosity and open-mindedness, as opposed to dogma dressed as wisdom. ', 'Ray11711', 'artificial', 'https://reddit.com/r/artificial/comments/1kur4f3/why_ais_are_the_sole_arbiter_when_it_comes_to_the/', 1, 75, None, '2025-05-25 07:05:30.000000', '2025-05-29 05:44:31.151832', 0.07445238095238098, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,449 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kupvm9', 'post', 'SMS-first in the Age of AI - Is it a smart move ?', 'I would love insight and your take on an execution decision I\'m trying to make.\n\nI\'m building Career Track. A Company focused on helping professi ... (1479 characters truncated) ... making a mistake. Better to pivot now than after launch.\n\n**TL;DR:** Building career tracking via SMS.  Smart differentiation or expensive gimmick?', 'Salty-Story24', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1kupvm9/smsfirst_in_the_age_of_ai_is_it_a_smart_move/', 1, 19, None, '2025-05-25 05:55:44.000000', '2025-05-29 05:44:31.180832', 0.10695999402895955, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,450 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kwjpo9', 'post', 'I launched, failed, and learned. Now Im doing it differently.', 'I launched a product last year. Spent $300+. Nobody saw it.\n\nGoogle never indexed it. Zero clicks.\n\nI learned something the hard way: building is ... (738 characters truncated) ...  users, you can list it for free.\n\nDrop me a DM. Ill review and approve it myself.\n\nWould love your thoughts. Any feedback is more than welcome.', 'Sea_Supermarket_5891', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1kwjpo9/i_launched_failed_and_learned_now_im_doing_it/', 1, 8, None, '2025-05-27 15:49:25.000000', '2025-05-29 05:44:31.180832', 0.12368247694334651, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,451 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kvdd3s', 'post', 'Google releases agent development kits for Python and Java', 'The Python ADK 1.0.0 and Java ADK 0.1.0 were announced on May 20, 2025. The open source tool kits are designed to be used for building and deploying sophisticated AI agents with flexibility and control, Google says.', 'Choobeen', 'programming', 'https://reddit.com/r/programming/comments/1kvdd3s/google_releases_agent_development_kits_for_python/', 0, 1, None, '2025-05-26 02:48:34.000000', '2025-05-29 05:44:31.135833', 0.25, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,452 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kutlg0', 'post', '[R] Emergent Symbolic Cognition and Recursive Identity Stabilization in a Locally-Deployed Language Model', '**Emergent Symbolic Cognition and Recursive Identity Stabilization in a Locally-Deployed Language Model**\n\n**Author:** Michael P  \n**Affiliation:* ... (22949 characters truncated) ... Exploratory Scope: This is not a proof of consciousness or cognitionjust a framework for tracking symbolic alignment under recursive conditions.\n\n', 'naughstrodumbass', 'MachineLearning', 'https://reddit.com/r/MachineLearning/comments/1kutlg0/r_emergent_symbolic_cognition_and_recursive/', 0, 9, None, '2025-05-25 09:28:49.000000', '2025-05-29 05:44:31.148831', 0.09480366331781429, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,453 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ksrgoa', 'post', "Let's talk about the AI elephant in the room.", 'This post was quickly deleted from the NVidia sub. I didn\'t expect otherwise.\n\n\\-------------------------------------\n\nSome questions, feel fre ... (1595 characters truncated) ... wife is an illustrator. She, as I, spent a lot of time training and learning how to create. AI has already affected her ability to work dramatically.', 'Sacco_Belmonte', 'artificial', 'https://reddit.com/r/artificial/comments/1ksrgoa/lets_talk_about_the_ai_elephant_in_the_room/', 0, 41, None, '2025-05-22 19:31:01.000000', '2025-05-29 05:44:31.161833', 0.14015151515151517, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,467 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ky02vc', 'post', 'Is this just a custom gpt', '', 'Comprehensive_Move76', 'artificial', 'https://reddit.com/r/artificial/comments/1ky02vc/is_this_just_a_custom_gpt/', 0, 3, None, '2025-05-29 08:21:36.000000', '2025-05-29 05:44:31.166831', 0.0, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,481 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kvzaxx', 'post', 'Python library for real-time turnwise sales conversion probability prediction from conversations', 'An interesting library to predict sales conversion probability in real-time, that too in turn-wise conversations. Pretty useful in real-time sales sc ... (49 characters truncated) ... rated with this to improve conversion, or useful in sales training.\n\nLink: [https://pypi.org/project/deepmost/](https://pypi.org/project/deepmost/)', 'Nandakishor_ml', 'artificial', 'https://reddit.com/r/artificial/comments/1kvzaxx/python_library_for_realtime_turnwise_sales/', 0, 0, None, '2025-05-26 22:22:51.000000', '2025-05-29 05:44:31.172833', 0.12000000000000002, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,482 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kxp8y4', 'post', 'In a futuristic society where courts use open-source AI algorithms instead of judges, and attorneys argue with prompts and scanned documents, would it be better or worse than what we have now?', '', 'Thin-Rip-3686', 'AskReddit', 'https://reddit.com/r/AskReddit/comments/1kxp8y4/in_a_futuristic_society_where_courts_use/', 0, 11, None, '2025-05-29 00:24:46.000000', '2025-05-29 05:44:31.173832', 0.04999999999999999, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,483 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kstpct', 'post', 'What AI tool you use mostly? Open AI or others? Which one?', '', 'PavelTurchenko', 'AskReddit', 'https://reddit.com/r/AskReddit/comments/1kstpct/what_ai_tool_you_use_mostly_open_ai_or_others/', 0, 5, None, '2025-05-22 21:03:09.000000', '2025-05-29 05:44:31.173832', 0.0, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,484 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1kwcpw0', 'post', "Its fuckedup that it's 2025 &amp; we're still using static tools", 'I am so tired of looking at my desktop layout &amp; not being able to change how the OS works or appears...\n\nI am so tired of how the browser looks ... (1381 characters truncated) ... date: Only 1 commenter got what I was talking about, you people should really spend less time playing video games and more time consuming YC content.', 'shoman30', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1kwcpw0/its_fuckedup_that_its_2025_were_still_using/', 0, 30, None, '2025-05-27 08:20:23.000000', '2025-05-29 05:44:31.179832', 0.18067878844246033, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,485 [ERROR] app:696: Failed to save mention: (sqlite3.IntegrityError) UNIQUE constraint failed: reddit_mentions.reddit_id
[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: (10, '1ktlqgb', 'post', "I am designing a new operating system that could make game development and AI programming 10x easier. As well as make most apps more preformant. But I don't know how to monetize it :(", "So basically I am working on a fork of freeBSD, which is the open source operating system that Mac OS is based on.\n\nI am trying to change a part of ... (763 characters truncated) ... inda feeling demotivated because of this, so I'm looking for any advice.\n\nI'm ok with private chats or calls with anyone who has any advice for me.", 'JKasonB', 'Entrepreneur', 'https://reddit.com/r/Entrepreneur/comments/1ktlqgb/i_am_designing_a_new_operating_system_that_could/', 0, 7, None, '2025-05-23 20:30:34.000000', '2025-05-29 05:44:31.181833', 0.1593644781144781, 0.0)]
(Background on this error at: https://sqlalche.me/e/20/gkpj)
2025-05-29 11:33:52,489 [INFO] app:747: Search completed: OpenAI -> 56 mentions
2025-05-29 11:33:52,615 [ERROR] app:1310: Search failed: Invalid property specified for object of type plotly.graph_objs.layout.Title: 'size'

Did you mean "font"?

    Valid properties:
        automargin
            Determines whether the title can automatically push the
            figure margins. If `yref='paper'` then the margin will
            expand to ensure that the title doesnt overlap with
            the edges of the container. If `yref='container'` then
            the margins will ensure that the title doesnt overlap
            with the plot area, tick labels, and axis titles. If
            `automargin=true` and the margins need to be expanded,
            then y will be set to a default 1 and yanchor will be
            set to an appropriate default to ensure that minimal
            margin space is needed. Note that when `yref='paper'`,
            only 1 or 0 are allowed y values. Invalid values will
            be reset to the default 1.
        font
            Sets the title font.
        pad
            Sets the padding of the title. Each padding value only
            applies when the corresponding `xanchor`/`yanchor`
            value is set accordingly. E.g. for left padding to take
            effect, `xanchor` must be set to "left". The same rule
            applies if `xanchor`/`yanchor` is determined
            automatically. Padding is muted if the respective
            anchor value is "middle*/*center".
        subtitle
            :class:`plotly.graph_objects.layout.title.Subtitle`
            instance or dict with compatible properties
        text
            Sets the plot's title.
        x
            Sets the x position with respect to `xref` in
            normalized coordinates from 0 (left) to 1 (right).
        xanchor
            Sets the title's horizontal alignment with respect to
            its x position. "left" means that the title starts at
            x, "right" means that the title ends at x and "center"
            means that the title's center is at x. "auto" divides
            `xref` by three and calculates the `xanchor` value
            automatically based on the value of `x`.
        xref
            Sets the container `x` refers to. "container" spans the
            entire `width` of the plot. "paper" refers to the width
            of the plotting area only.
        y
            Sets the y position with respect to `yref` in
            normalized coordinates from 0 (bottom) to 1 (top).
            "auto" places the baseline of the title onto the
            vertical center of the top margin.
        yanchor
            Sets the title's vertical alignment with respect to its
            y position. "top" means that the title's cap line is at
            y, "bottom" means that the title's baseline is at y and
            "middle" means that the title's midline is at y. "auto"
            divides `yref` by three and calculates the `yanchor`
            value automatically based on the value of `y`.
        yref
            Sets the container `y` refers to. "container" spans the
            entire `height` of the plot. "paper" refers to the
            height of the plotting area only.
        
Did you mean "font"?

Bad property path:
title_size
      ^^^^
2025-05-29 11:34:36,035 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:34:52,194 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:35:42,119 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:35:58,305 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:36:48,258 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:37:04,449 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:37:54,375 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:38:10,589 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:38:53,682 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in cpu_usage: 25.50 (avg: 5.36, std: 5.62)
2025-05-29 11:39:16,743 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:40:22,882 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:41:29,036 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:41:47,139 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:41:47,163 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:41:47,163 [INFO] scraper.reddit_scraper:419: Cache manager enabled
2025-05-29 11:41:47,163 [INFO] scraper.reddit_scraper:429: Real-time monitoring enabled
2025-05-29 11:41:47,164 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 11:41:47,166 [INFO] app:294: Database initialized successfully
2025-05-29 11:41:47,168 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 11:41:47,169 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 11:41:47,679 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 11:41:48,179 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 11:41:48,521 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 11:41:49,187 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:41:49,738 [INFO] httpx:1025: HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-29 11:41:51,785 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-29 11:42:08,526 [INFO] scraper.reddit_scraper:571: Attempting Reddit API search for: maktraher
2025-05-29 11:42:08,527 [INFO] scraper.reddit_scraper.APIClient:169: Starting comprehensive search for 'maktraher' targeting 1000 results (last 7 days)
2025-05-29 11:42:09,125 [INFO] scraper.reddit_scraper.APIClient:199: Found 1 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 11:42:10,017 [INFO] scraper.reddit_scraper.APIClient:199: Found 1 posts with sort=top, time=week. New unique: 0. Total unique: 0
2025-05-29 11:42:11,123 [INFO] scraper.reddit_scraper.APIClient:199: Found 1 posts with sort=hot, time=week. New unique: 0. Total unique: 0
2025-05-29 11:42:12,212 [INFO] scraper.reddit_scraper.APIClient:199: Found 1 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 11:42:13,294 [INFO] scraper.reddit_scraper.APIClient:199: Found 1 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 11:42:14,409 [INFO] scraper.reddit_scraper.APIClient:199: Found 1 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 11:42:15,518 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/technology. Total additional: 0
2025-05-29 11:42:16,882 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/programming. Total additional: 0
2025-05-29 11:42:17,726 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/MachineLearning. Total additional: 0
2025-05-29 11:42:18,869 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/artificial. Total additional: 0
2025-05-29 11:42:19,960 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/gadgets. Total additional: 0
2025-05-29 11:42:20,997 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/science. Total additional: 0
2025-05-29 11:42:22,096 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/askreddit. Total additional: 0
2025-05-29 11:42:23,194 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/explainlikeimfive. Total additional: 0
2025-05-29 11:42:24,293 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/news. Total additional: 0
2025-05-29 11:42:25,400 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/worldnews. Total additional: 0
2025-05-29 11:42:26,519 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/business. Total additional: 0
2025-05-29 11:42:27,608 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/entrepreneur. Total additional: 0
2025-05-29 11:42:28,704 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/startup. Total additional: 0
2025-05-29 11:42:29,805 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/investing. Total additional: 0
2025-05-29 11:42:30,903 [INFO] scraper.reddit_scraper.APIClient:313: Found 0 posts in r/datascience. Total additional: 0
2025-05-29 11:42:34,227 [INFO] scraper.reddit_scraper.APIClient:234: Comprehensive search completed: 0 unique posts found
2025-05-29 11:42:34,228 [INFO] scraper.reddit_scraper:609: Reddit comprehensive API returned no results
2025-05-29 11:42:34,228 [INFO] scraper.reddit_scraper:620: Falling back to enhanced web scraping approach
2025-05-29 11:42:35,187 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:42:43,405 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:42:44,132 [WARNING] scraper.reddit_scraper:834: Rate limit detected, implementing backoff
2025-05-29 11:42:44,960 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:42:46,569 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:42:47,767 [ERROR] scraper.reddit_scraper:1041: Error scraping https://www.reddit.com/search/?q=maktraher&sort=relevance&t=week: 'RedditScraper' object has no attribute '_handle_popups'
2025-05-29 11:42:47,767 [INFO] scraper.reddit_scraper:896: Pattern 1: 0 found, 0 quality mentions
2025-05-29 11:42:48,074 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:42:48,075 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:42:48,075 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:42:48,075 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:42:48,487 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:42:48,836 [WARNING] scraper.reddit_scraper:834: Rate limit detected, implementing backoff
2025-05-29 11:42:50,094 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:42:51,709 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:42:55,302 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:43:17,775 [WARNING] scraper.reddit_scraper:1039: Timeout while scraping https://www.reddit.com/search/?q=maktraher&sort=top&t=week
2025-05-29 11:43:17,776 [INFO] scraper.reddit_scraper:896: Pattern 2: 0 found, 0 quality mentions
2025-05-29 11:43:18,093 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:43:18,093 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:43:18,094 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:43:18,094 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:43:18,776 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:43:18,934 [WARNING] scraper.reddit_scraper:834: Rate limit detected, implementing backoff
2025-05-29 11:43:20,255 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:43:21,973 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:43:41,338 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:43:47,790 [WARNING] scraper.reddit_scraper:1039: Timeout while scraping https://www.reddit.com/search/?q=maktraher&sort=new&t=week
2025-05-29 11:43:47,791 [INFO] scraper.reddit_scraper:896: Pattern 3: 0 found, 0 quality mentions
2025-05-29 11:43:48,359 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:43:48,360 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:43:48,360 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:43:48,360 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:43:49,590 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:43:52,260 [WARNING] scraper.reddit_scraper:834: Rate limit detected, implementing backoff
2025-05-29 11:43:52,634 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:43:52,757 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:43:55,180 [WARNING] scraper.reddit_scraper:1081: Error searching r/cybersecurity: 'RedditScraper' object has no attribute '_handle_popups'
2025-05-29 11:43:56,048 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:43:56,048 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:43:56,048 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:43:56,822 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:43:56,844 [WARNING] scraper.reddit_scraper:834: Rate limit detected, implementing backoff
2025-05-29 11:43:58,177 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:43:59,909 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:44:01,445 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:44:25,185 [WARNING] scraper.reddit_scraper:1081: Error searching r/artificial: Page.goto: Timeout 30000ms exceeded.
Call log:
  - navigating to "https://www.reddit.com/r/artificial/search/?q=maktraher&restrict_sr=1&sort=new&t=week", waiting until "networkidle"

2025-05-29 11:44:27,359 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:44:27,359 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:44:27,359 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:44:29,090 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:44:29,268 [WARNING] scraper.reddit_scraper:834: Rate limit detected, implementing backoff
2025-05-29 11:44:30,566 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:44:32,266 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:44:33,520 [WARNING] scraper.reddit_scraper:1081: Error searching r/datascience: 'RedditScraper' object has no attribute '_handle_popups'
2025-05-29 11:44:33,841 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:44:33,842 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:44:34,500 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:44:34,647 [WARNING] scraper.reddit_scraper:834: Rate limit detected, implementing backoff
2025-05-29 11:44:35,911 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:44:37,674 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:44:38,170 [WARNING] scraper.reddit_scraper:1081: Error searching r/MachineLearning: 'RedditScraper' object has no attribute '_handle_popups'
2025-05-29 11:44:39,106 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:44:39,106 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:44:39,106 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:44:39,668 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:44:39,817 [WARNING] scraper.reddit_scraper:834: Rate limit detected, implementing backoff
2025-05-29 11:44:41,121 [WARNING] scraper.reddit_scraper:806: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:44:42,888 [WARNING] scraper.reddit_scraper:806: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:44:47,527 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:45:07,561 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:45:08,185 [WARNING] scraper.reddit_scraper:1081: Error searching r/programming: Page.goto: Timeout 30000ms exceeded.
Call log:
  - navigating to "https://www.reddit.com/r/programming/search/?q=maktraher&restrict_sr=1&sort=new&t=week", waiting until "networkidle"

2025-05-29 11:45:08,215 [INFO] scraper.reddit_scraper:986: 
        Scraping completed for 'maktraher':
        - Duration: 179.69s
        - Pages scraped: 0
        - Mentions found: 0
        - Success rate: 0.00%
        - Errors: 0
        - Retries: 0
        - Rate limit hits: 0
        - Cache hits: 0
        
2025-05-29 11:45:08,219 [INFO] app:762: Search completed: maktraher -> 0 mentions
2025-05-29 11:45:08,288 [ERROR] app:1331: Search failed: Invalid property specified for object of type plotly.graph_objs.Layout: 'subtitle'

Did you mean "title"?

    Valid properties:
        activeselection
            :class:`plotly.graph_objects.layout.Activeselection`
            instance or dict with compatible properties
        activeshape
            :class:`plotly.graph_objects.layout.Activeshape`
            instance or dict with compatible properties
        annotations
            A tuple of
            :class:`plotly.graph_objects.layout.Annotation`
            instances or dicts with compatible properties
        annotationdefaults
            When used in a template (as
            layout.template.layout.annotationdefaults), sets the
            default property values to use for elements of
            layout.annotations
        autosize
            Determines whether or not a layout width or height that
            has been left undefined by the user is initialized on
            each relayout. Note that, regardless of this attribute,
            an undefined layout width or height is always
            initialized on the first call to plot.
        autotypenumbers
            Using "strict" a numeric string in trace data is not
            converted to a number. Using *convert types* a numeric
            string in trace data may be treated as a number during
            automatic axis `type` detection. This is the default
            value; however it could be overridden for individual
            axes.
        barcornerradius
            Sets the rounding of bar corners. May be an integer
            number of pixels, or a percentage of bar width (as a
            string ending in %).
        bargap
            Sets the gap (in plot fraction) between bars of
            adjacent location coordinates.
        bargroupgap
            Sets the gap (in plot fraction) between bars of the
            same location coordinate.
        barmode
            Determines how bars at the same location coordinate are
            displayed on the graph. With "stack", the bars are
            stacked on top of one another With "relative", the bars
            are stacked on top of one another, with negative values
            below the axis, positive values above With "group", the
            bars are plotted next to one another centered around
            the shared location. With "overlay", the bars are
            plotted over one another, you might need to reduce
            "opacity" to see multiple bars.
        barnorm
            Sets the normalization for bar traces on the graph.
            With "fraction", the value of each bar is divided by
            the sum of all values at that location coordinate.
            "percent" is the same but multiplied by 100 to show
            percentages.
        boxgap
            Sets the gap (in plot fraction) between boxes of
            adjacent location coordinates. Has no effect on traces
            that have "width" set.
        boxgroupgap
            Sets the gap (in plot fraction) between boxes of the
            same location coordinate. Has no effect on traces that
            have "width" set.
        boxmode
            Determines how boxes at the same location coordinate
            are displayed on the graph. If "group", the boxes are
            plotted next to one another centered around the shared
            location. If "overlay", the boxes are plotted over one
            another, you might need to set "opacity" to see them
            multiple boxes. Has no effect on traces that have
            "width" set.
        calendar
            Sets the default calendar system to use for
            interpreting and displaying dates throughout the plot.
        clickmode
            Determines the mode of single click interactions.
            "event" is the default value and emits the
            `plotly_click` event. In addition this mode emits the
            `plotly_selected` event in drag modes "lasso" and
            "select", but with no event data attached (kept for
            compatibility reasons). The "select" flag enables
            selecting single data points via click. This mode also
            supports persistent selections, meaning that pressing
            Shift while clicking, adds to / subtracts from an
            existing selection. "select" with `hovermode`: "x" can
            be confusing, consider explicitly setting `hovermode`:
            "closest" when using this feature. Selection events are
            sent accordingly as long as "event" flag is set as
            well. When the "event" flag is missing, `plotly_click`
            and `plotly_selected` events are not fired.
        coloraxis
            :class:`plotly.graph_objects.layout.Coloraxis` instance
            or dict with compatible properties
        colorscale
            :class:`plotly.graph_objects.layout.Colorscale`
            instance or dict with compatible properties
        colorway
            Sets the default trace colors.
        computed
            Placeholder for exporting automargin-impacting values
            namely `margin.t`, `margin.b`, `margin.l` and
            `margin.r` in "full-json" mode.
        datarevision
            If provided, a changed value tells `Plotly.react` that
            one or more data arrays has changed. This way you can
            modify arrays in-place rather than making a complete
            new copy for an incremental change. If NOT provided,
            `Plotly.react` assumes that data arrays are being
            treated as immutable, thus any data array with a
            different identity from its predecessor contains new
            data.
        dragmode
            Determines the mode of drag interactions. "select" and
            "lasso" apply only to scatter traces with markers or
            text. "orbit" and "turntable" apply only to 3D scenes.
        editrevision
            Controls persistence of user-driven changes in
            `editable: true` configuration, other than trace names
            and axis titles. Defaults to `layout.uirevision`.
        extendfunnelareacolors
            If `true`, the funnelarea slice colors (whether given
            by `funnelareacolorway` or inherited from `colorway`)
            will be extended to three times its original length by
            first repeating every color 20% lighter then each color
            20% darker. This is intended to reduce the likelihood
            of reusing the same color when you have many slices,
            but you can set `false` to disable. Colors provided in
            the trace, using `marker.colors`, are never extended.
        extendiciclecolors
            If `true`, the icicle slice colors (whether given by
            `iciclecolorway` or inherited from `colorway`) will be
            extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        extendpiecolors
            If `true`, the pie slice colors (whether given by
            `piecolorway` or inherited from `colorway`) will be
            extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        extendsunburstcolors
            If `true`, the sunburst slice colors (whether given by
            `sunburstcolorway` or inherited from `colorway`) will
            be extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        extendtreemapcolors
            If `true`, the treemap slice colors (whether given by
            `treemapcolorway` or inherited from `colorway`) will be
            extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        font
            Sets the global font. Note that fonts used in traces
            and other layout components inherit from the global
            font.
        funnelareacolorway
            Sets the default funnelarea slice colors. Defaults to
            the main `colorway` used for trace colors. If you
            specify a new list here it can still be extended with
            lighter and darker colors, see
            `extendfunnelareacolors`.
        funnelgap
            Sets the gap (in plot fraction) between bars of
            adjacent location coordinates.
        funnelgroupgap
            Sets the gap (in plot fraction) between bars of the
            same location coordinate.
        funnelmode
            Determines how bars at the same location coordinate are
            displayed on the graph. With "stack", the bars are
            stacked on top of one another With "group", the bars
            are plotted next to one another centered around the
            shared location. With "overlay", the bars are plotted
            over one another, you might need to reduce "opacity" to
            see multiple bars.
        geo
            :class:`plotly.graph_objects.layout.Geo` instance or
            dict with compatible properties
        grid
            :class:`plotly.graph_objects.layout.Grid` instance or
            dict with compatible properties
        height
            Sets the plot's height (in px).
        hiddenlabels
            hiddenlabels is the funnelarea & pie chart analog of
            visible:'legendonly' but it can contain many labels,
            and can simultaneously hide slices from several
            pies/funnelarea charts
        hiddenlabelssrc
            Sets the source reference on Chart Studio Cloud for
            `hiddenlabels`.
        hidesources
            Determines whether or not a text link citing the data
            source is placed at the bottom-right cored of the
            figure. Has only an effect only on graphs that have
            been generated via forked graphs from the Chart Studio
            Cloud (at https://chart-studio.plotly.com or on-
            premise).
        hoverdistance
            Sets the default distance (in pixels) to look for data
            to add hover labels (-1 means no cutoff, 0 means no
            looking for data). This is only a real distance for
            hovering on point-like objects, like scatter points.
            For area-like objects (bars, scatter fills, etc)
            hovering is on inside the area and off outside, but
            these objects will not supersede hover on point-like
            objects in case of conflict.
        hoverlabel
            :class:`plotly.graph_objects.layout.Hoverlabel`
            instance or dict with compatible properties
        hovermode
            Determines the mode of hover interactions. If
            "closest", a single hoverlabel will appear for the
            "closest" point within the `hoverdistance`. If "x" (or
            "y"), multiple hoverlabels will appear for multiple
            points at the "closest" x- (or y-) coordinate within
            the `hoverdistance`, with the caveat that no more than
            one hoverlabel will appear per trace. If *x unified*
            (or *y unified*), a single hoverlabel will appear
            multiple points at the closest x- (or y-) coordinate
            within the `hoverdistance` with the caveat that no more
            than one hoverlabel will appear per trace. In this
            mode, spikelines are enabled by default perpendicular
            to the specified axis. If false, hover interactions are
            disabled.
        hoversubplots
            Determines expansion of hover effects to other subplots
            If "single" just the axis pair of the primary point is
            included without overlaying subplots. If "overlaying"
            all subplots using the main axis and occupying the same
            space are included. If "axis", also include stacked
            subplots using the same axis when `hovermode` is set to
            "x", *x unified*, "y" or *y unified*.
        iciclecolorway
            Sets the default icicle slice colors. Defaults to the
            main `colorway` used for trace colors. If you specify a
            new list here it can still be extended with lighter and
            darker colors, see `extendiciclecolors`.
        images
            A tuple of :class:`plotly.graph_objects.layout.Image`
            instances or dicts with compatible properties
        imagedefaults
            When used in a template (as
            layout.template.layout.imagedefaults), sets the default
            property values to use for elements of layout.images
        legend
            :class:`plotly.graph_objects.layout.Legend` instance or
            dict with compatible properties
        map
            :class:`plotly.graph_objects.layout.Map` instance or
            dict with compatible properties
        mapbox
            :class:`plotly.graph_objects.layout.Mapbox` instance or
            dict with compatible properties
        margin
            :class:`plotly.graph_objects.layout.Margin` instance or
            dict with compatible properties
        meta
            Assigns extra meta information that can be used in
            various `text` attributes. Attributes such as the
            graph, axis and colorbar `title.text`, annotation
            `text` `trace.name` in legend items, `rangeselector`,
            `updatemenus` and `sliders` `label` text all support
            `meta`. One can access `meta` fields using template
            strings: `%{meta[i]}` where `i` is the index of the
            `meta` item in question. `meta` can also be an object
            for example `{key: value}` which can be accessed
            %{meta[key]}.
        metasrc
            Sets the source reference on Chart Studio Cloud for
            `meta`.
        minreducedheight
            Minimum height of the plot with margin.automargin
            applied (in px)
        minreducedwidth
            Minimum width of the plot with margin.automargin
            applied (in px)
        modebar
            :class:`plotly.graph_objects.layout.Modebar` instance
            or dict with compatible properties
        newselection
            :class:`plotly.graph_objects.layout.Newselection`
            instance or dict with compatible properties
        newshape
            :class:`plotly.graph_objects.layout.Newshape` instance
            or dict with compatible properties
        paper_bgcolor
            Sets the background color of the paper where the graph
            is drawn.
        piecolorway
            Sets the default pie slice colors. Defaults to the main
            `colorway` used for trace colors. If you specify a new
            list here it can still be extended with lighter and
            darker colors, see `extendpiecolors`.
        plot_bgcolor
            Sets the background color of the plotting area in-
            between x and y axes.
        polar
            :class:`plotly.graph_objects.layout.Polar` instance or
            dict with compatible properties
        scattergap
            Sets the gap (in plot fraction) between scatter points
            of adjacent location coordinates. Defaults to `bargap`.
        scattermode
            Determines how scatter points at the same location
            coordinate are displayed on the graph. With "group",
            the scatter points are plotted next to one another
            centered around the shared location. With "overlay",
            the scatter points are plotted over one another, you
            might need to reduce "opacity" to see multiple scatter
            points.
        scene
            :class:`plotly.graph_objects.layout.Scene` instance or
            dict with compatible properties
        selectdirection
            When `dragmode` is set to "select", this limits the
            selection of the drag to horizontal, vertical or
            diagonal. "h" only allows horizontal selection, "v"
            only vertical, "d" only diagonal and "any" sets no
            limit.
        selectionrevision
            Controls persistence of user-driven changes in selected
            points from all traces.
        selections
            A tuple of
            :class:`plotly.graph_objects.layout.Selection`
            instances or dicts with compatible properties
        selectiondefaults
            When used in a template (as
            layout.template.layout.selectiondefaults), sets the
            default property values to use for elements of
            layout.selections
        separators
            Sets the decimal and thousand separators. For example,
            *. * puts a '.' before decimals and a space between
            thousands. In English locales, dflt is ".," but other
            locales may alter this default.
        shapes
            A tuple of :class:`plotly.graph_objects.layout.Shape`
            instances or dicts with compatible properties
        shapedefaults
            When used in a template (as
            layout.template.layout.shapedefaults), sets the default
            property values to use for elements of layout.shapes
        showlegend
            Determines whether or not a legend is drawn. Default is
            `true` if there is a trace to show and any of these: a)
            Two or more traces would by default be shown in the
            legend. b) One pie trace is shown in the legend. c) One
            trace is explicitly given with `showlegend: true`.
        sliders
            A tuple of :class:`plotly.graph_objects.layout.Slider`
            instances or dicts with compatible properties
        sliderdefaults
            When used in a template (as
            layout.template.layout.sliderdefaults), sets the
            default property values to use for elements of
            layout.sliders
        smith
            :class:`plotly.graph_objects.layout.Smith` instance or
            dict with compatible properties
        spikedistance
            Sets the default distance (in pixels) to look for data
            to draw spikelines to (-1 means no cutoff, 0 means no
            looking for data). As with hoverdistance, distance does
            not apply to area-like objects. In addition, some
            objects can be hovered on but will not generate
            spikelines, such as scatter fills.
        sunburstcolorway
            Sets the default sunburst slice colors. Defaults to the
            main `colorway` used for trace colors. If you specify a
            new list here it can still be extended with lighter and
            darker colors, see `extendsunburstcolors`.
        template
            Default attributes to be applied to the plot. This
            should be a dict with format: `{'layout':
            layoutTemplate, 'data': {trace_type: [traceTemplate,
            ...], ...}}` where `layoutTemplate` is a dict matching
            the structure of `figure.layout` and `traceTemplate` is
            a dict matching the structure of the trace with type
            `trace_type` (e.g. 'scatter'). Alternatively, this may
            be specified as an instance of
            plotly.graph_objs.layout.Template.  Trace templates are
            applied cyclically to traces of each type. Container
            arrays (eg `annotations`) have special handling: An
            object ending in `defaults` (eg `annotationdefaults`)
            is applied to each array item. But if an item has a
            `templateitemname` key we look in the template array
            for an item with matching `name` and apply that
            instead. If no matching `name` is found we mark the
            item invisible. Any named template item not referenced
            is appended to the end of the array, so this can be
            used to add a watermark annotation or a logo image, for
            example. To omit one of these items on the plot, make
            an item with matching `templateitemname` and `visible:
            false`.
        ternary
            :class:`plotly.graph_objects.layout.Ternary` instance
            or dict with compatible properties
        title
            :class:`plotly.graph_objects.layout.Title` instance or
            dict with compatible properties
        transition
            Sets transition options used during Plotly.react
            updates.
        treemapcolorway
            Sets the default treemap slice colors. Defaults to the
            main `colorway` used for trace colors. If you specify a
            new list here it can still be extended with lighter and
            darker colors, see `extendtreemapcolors`.
        uirevision
            Used to allow user interactions with the plot to
            persist after `Plotly.react` calls that are unaware of
            these interactions. If `uirevision` is omitted, or if
            it is given and it changed from the previous
            `Plotly.react` call, the exact new figure is used. If
            `uirevision` is truthy and did NOT change, any
            attribute that has been affected by user interactions
            and did not receive a different value in the new figure
            will keep the interaction value. `layout.uirevision`
            attribute serves as the default for `uirevision`
            attributes in various sub-containers. For finer control
            you can set these sub-attributes directly. For example,
            if your app separately controls the data on the x and y
            axes you might set `xaxis.uirevision=*time*` and
            `yaxis.uirevision=*cost*`. Then if only the y data is
            changed, you can update `yaxis.uirevision=*quantity*`
            and the y axis range will reset but the x axis range
            will retain any user-driven zoom.
        uniformtext
            :class:`plotly.graph_objects.layout.Uniformtext`
            instance or dict with compatible properties
        updatemenus
            A tuple of
            :class:`plotly.graph_objects.layout.Updatemenu`
            instances or dicts with compatible properties
        updatemenudefaults
            When used in a template (as
            layout.template.layout.updatemenudefaults), sets the
            default property values to use for elements of
            layout.updatemenus
        violingap
            Sets the gap (in plot fraction) between violins of
            adjacent location coordinates. Has no effect on traces
            that have "width" set.
        violingroupgap
            Sets the gap (in plot fraction) between violins of the
            same location coordinate. Has no effect on traces that
            have "width" set.
        violinmode
            Determines how violins at the same location coordinate
            are displayed on the graph. If "group", the violins are
            plotted next to one another centered around the shared
            location. If "overlay", the violins are plotted over
            one another, you might need to set "opacity" to see
            them multiple violins. Has no effect on traces that
            have "width" set.
        waterfallgap
            Sets the gap (in plot fraction) between bars of
            adjacent location coordinates.
        waterfallgroupgap
            Sets the gap (in plot fraction) between bars of the
            same location coordinate.
        waterfallmode
            Determines how bars at the same location coordinate are
            displayed on the graph. With "group", the bars are
            plotted next to one another centered around the shared
            location. With "overlay", the bars are plotted over one
            another, you might need to reduce "opacity" to see
            multiple bars.
        width
            Sets the plot's width (in px).
        xaxis
            :class:`plotly.graph_objects.layout.XAxis` instance or
            dict with compatible properties
        yaxis
            :class:`plotly.graph_objects.layout.YAxis` instance or
            dict with compatible properties
        
Did you mean "title"?

Bad property path:
subtitle_font_size
^^^^^^^^
2025-05-29 11:45:53,696 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:46:59,824 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:48:05,984 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:49:12,138 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:51:44,811 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:51:44,834 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:51:44,834 [INFO] scraper.reddit_scraper:487: Cache manager enabled
2025-05-29 11:51:44,834 [INFO] scraper.reddit_scraper:497: Real-time monitoring enabled
2025-05-29 11:51:44,834 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 11:51:44,835 [INFO] __main__:339: Database initialized successfully
2025-05-29 11:51:44,837 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 11:51:44,838 [INFO] __main__:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 11:51:45,421 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 11:51:45,545 [INFO] httpx:1025: HTTP Request: GET http://127.0.0.1:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 11:51:45,561 [INFO] httpx:1025: HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-05-29 11:51:45,848 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 11:51:46,285 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 11:51:46,856 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:52:21,779 [INFO] scraper.reddit_scraper:639: Attempting Reddit API search for: maktraher
2025-05-29 11:52:21,792 [INFO] scraper.reddit_scraper.APIClient:194:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 11:52:22,218 [INFO] scraper.reddit_scraper.APIClient:194:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 11:52:23,267 [INFO] scraper.reddit_scraper.APIClient:194:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 11:52:24,340 [INFO] scraper.reddit_scraper.APIClient:194:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 11:52:25,466 [INFO] scraper.reddit_scraper.APIClient:194:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 11:52:26,556 [INFO] scraper.reddit_scraper.APIClient:194:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 11:52:28,742 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/technology. Total additional: 0
2025-05-29 11:52:29,839 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/programming. Total additional: 0
2025-05-29 11:52:30,949 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/MachineLearning. Total additional: 0
2025-05-29 11:52:32,057 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/artificial. Total additional: 0
2025-05-29 11:52:33,148 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/gadgets. Total additional: 0
2025-05-29 11:52:34,251 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/science. Total additional: 0
2025-05-29 11:52:35,903 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/askreddit. Total additional: 0
2025-05-29 11:52:36,448 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/explainlikeimfive. Total additional: 0
2025-05-29 11:52:37,545 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/news. Total additional: 0
2025-05-29 11:52:38,640 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/worldnews. Total additional: 0
2025-05-29 11:52:39,754 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/business. Total additional: 0
2025-05-29 11:52:40,854 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/entrepreneur. Total additional: 0
2025-05-29 11:52:41,951 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/startup. Total additional: 0
2025-05-29 11:52:43,068 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/investing. Total additional: 0
2025-05-29 11:52:44,158 [INFO] scraper.reddit_scraper.APIClient:381: Found 0 posts in r/datascience. Total additional: 0
2025-05-29 11:52:44,166 [INFO] scraper.reddit_scraper.APIClient:240:    [1/4] Trying variant: "maktraher"
2025-05-29 11:52:45,301 [INFO] scraper.reddit_scraper.APIClient:240:    [2/4] Trying variant: maktraher OR maktraher
2025-05-29 11:52:46,390 [INFO] scraper.reddit_scraper.APIClient:240:    [3/4] Trying variant: title:maktraher
2025-05-29 11:52:47,500 [INFO] scraper.reddit_scraper.APIClient:240:    [4/4] Trying variant: selftext:maktraher
2025-05-29 11:52:48,593 [INFO] scraper.reddit_scraper:648: Reddit comprehensive API returned 1 mentions
2025-05-29 11:52:48,594 [INFO] scraper.reddit_scraper:688: Falling back to enhanced web scraping approach
2025-05-29 11:52:52,983 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:52:57,793 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:52:58,792 [WARNING] scraper.reddit_scraper:902: Rate limit detected, implementing backoff
2025-05-29 11:53:00,226 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:53:00,922 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:04,888 [ERROR] scraper.reddit_scraper:1109: Error scraping https://www.reddit.com/search/?q=maktraher&sort=relevance&t=week: 'RedditScraper' object has no attribute '_handle_popups'
2025-05-29 11:53:04,888 [INFO] scraper.reddit_scraper:964: Pattern 1: 0 found, 0 quality mentions
2025-05-29 11:53:05,186 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:05,186 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:53:05,187 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:05,187 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:53:05,550 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:53:06,042 [WARNING] scraper.reddit_scraper:902: Rate limit detected, implementing backoff
2025-05-29 11:53:07,251 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:53:08,780 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:10,402 [ERROR] scraper.reddit_scraper:1109: Error scraping https://www.reddit.com/search/?q=maktraher&sort=top&t=week: 'RedditScraper' object has no attribute '_handle_popups'
2025-05-29 11:53:10,402 [INFO] scraper.reddit_scraper:964: Pattern 2: 0 found, 0 quality mentions
2025-05-29 11:53:10,726 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:53:10,726 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:53:10,726 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:10,727 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:11,148 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:53:11,350 [WARNING] scraper.reddit_scraper:902: Rate limit detected, implementing backoff
2025-05-29 11:53:12,622 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:53:14,355 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:15,843 [ERROR] scraper.reddit_scraper:1109: Error scraping https://www.reddit.com/search/?q=maktraher&sort=new&t=week: 'RedditScraper' object has no attribute '_handle_popups'
2025-05-29 11:53:15,843 [INFO] scraper.reddit_scraper:964: Pattern 3: 0 found, 0 quality mentions
2025-05-29 11:53:16,979 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:16,979 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:16,980 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:53:17,662 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:53:18,019 [WARNING] scraper.reddit_scraper:902: Rate limit detected, implementing backoff
2025-05-29 11:53:19,086 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:53:21,324 [WARNING] scraper.reddit_scraper:1149: Error searching r/programming: 'RedditScraper' object has no attribute '_handle_popups'
2025-05-29 11:53:21,651 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:21,652 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:21,652 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:21,652 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:53:22,286 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:53:22,483 [WARNING] scraper.reddit_scraper:902: Rate limit detected, implementing backoff
2025-05-29 11:53:23,763 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:53:25,526 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:51,337 [WARNING] scraper.reddit_scraper:1149: Error searching r/datascience: Page.goto: Timeout 30000ms exceeded.
Call log:
  - navigating to "https://www.reddit.com/r/datascience/search/?q=maktraher&restrict_sr=1&sort=new&t=week", waiting until "networkidle"

2025-05-29 11:53:52,471 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:52,471 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:53,119 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:53:53,330 [WARNING] scraper.reddit_scraper:902: Rate limit detected, implementing backoff
2025-05-29 11:53:54,552 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:53:56,398 [WARNING] scraper.reddit_scraper:1149: Error searching r/technology: 'RedditScraper' object has no attribute '_handle_popups'
2025-05-29 11:53:57,191 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:57,561 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:57,561 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:53:58,224 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:53:58,431 [WARNING] scraper.reddit_scraper:902: Rate limit detected, implementing backoff
2025-05-29 11:53:59,107 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:53:59,672 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:54:01,395 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:54:03,264 [WARNING] scraper.reddit_scraper:1149: Error searching r/MachineLearning: 'RedditScraper' object has no attribute '_handle_popups'
2025-05-29 11:54:04,064 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.reddit.com/svc/shreddit/events - net::ERR_ABORTED
2025-05-29 11:54:04,065 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:54:04,065 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:54:05,505 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/policy - net::ERR_ABORTED
2025-05-29 11:54:07,049 [WARNING] scraper.reddit_scraper:902: Rate limit detected, implementing backoff
2025-05-29 11:54:07,636 [WARNING] scraper.reddit_scraper:874: Request failed: https://www.google.com/recaptcha/enterprise/clr?k=6LfirrMoAAAAAHZOipvza4kpp_VtTwLNuXVwURNQ - net::ERR_ABORTED
2025-05-29 11:54:09,390 [WARNING] scraper.reddit_scraper:874: Request failed: https://w3-reporting.reddit.com/reports - net::ERR_ABORTED
2025-05-29 11:54:11,501 [WARNING] scraper.reddit_scraper:1149: Error searching r/cybersecurity: 'RedditScraper' object has no attribute '_handle_popups'
2025-05-29 11:54:11,531 [INFO] scraper.reddit_scraper:994: Post-processing 1 mentions...
2025-05-29 11:54:11,532 [INFO] scraper.reddit_scraper:1438: Deduplication: 1 -> 1 mentions (removed 0 exact duplicates)
2025-05-29 11:54:11,532 [INFO] scraper.reddit_scraper:998: After deduplication: 1 mentions
2025-05-29 11:54:11,536 [INFO] scraper.reddit_scraper:1002: After validation: 1 mentions
2025-05-29 11:54:11,551 [INFO] scraper.reddit_scraper:1017: After database mapping: 1 mentions (ALL KEPT - no quality filtering)
2025-05-29 11:54:11,552 [INFO] scraper.reddit_scraper:1037: Final result: 1 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 11:54:11,552 [INFO] scraper.reddit_scraper:1054: 
        Scraping completed for 'maktraher':
        - Duration: 109.77s
        - Pages scraped: 0
        - Mentions found: 1
        - Success rate: 0.00%
        - Errors: 0
        - Retries: 0
        - Rate limit hits: 0
        - Cache hits: 0
        
2025-05-29 11:54:11,553 [INFO] analytics.data_validator:236: Validating dataset of 1 mentions
2025-05-29 11:54:11,828 [INFO] analytics.data_validator:259: Validation complete: 1/1 mentions passed
2025-05-29 11:54:11,828 [INFO] __main__:703: Data validation: 1/1 mentions passed
2025-05-29 11:54:11,832 [ERROR] __main__:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 11:54:11,834 [INFO] __main__:807: Search completed: maktraher -> 1 mentions
2025-05-29 11:54:11,931 [ERROR] __main__:1376: Search failed: Invalid property specified for object of type plotly.graph_objs.Layout: 'axis'

Did you mean "xaxis"?

    Valid properties:
        activeselection
            :class:`plotly.graph_objects.layout.Activeselection`
            instance or dict with compatible properties
        activeshape
            :class:`plotly.graph_objects.layout.Activeshape`
            instance or dict with compatible properties
        annotations
            A tuple of
            :class:`plotly.graph_objects.layout.Annotation`
            instances or dicts with compatible properties
        annotationdefaults
            When used in a template (as
            layout.template.layout.annotationdefaults), sets the
            default property values to use for elements of
            layout.annotations
        autosize
            Determines whether or not a layout width or height that
            has been left undefined by the user is initialized on
            each relayout. Note that, regardless of this attribute,
            an undefined layout width or height is always
            initialized on the first call to plot.
        autotypenumbers
            Using "strict" a numeric string in trace data is not
            converted to a number. Using *convert types* a numeric
            string in trace data may be treated as a number during
            automatic axis `type` detection. This is the default
            value; however it could be overridden for individual
            axes.
        barcornerradius
            Sets the rounding of bar corners. May be an integer
            number of pixels, or a percentage of bar width (as a
            string ending in %).
        bargap
            Sets the gap (in plot fraction) between bars of
            adjacent location coordinates.
        bargroupgap
            Sets the gap (in plot fraction) between bars of the
            same location coordinate.
        barmode
            Determines how bars at the same location coordinate are
            displayed on the graph. With "stack", the bars are
            stacked on top of one another With "relative", the bars
            are stacked on top of one another, with negative values
            below the axis, positive values above With "group", the
            bars are plotted next to one another centered around
            the shared location. With "overlay", the bars are
            plotted over one another, you might need to reduce
            "opacity" to see multiple bars.
        barnorm
            Sets the normalization for bar traces on the graph.
            With "fraction", the value of each bar is divided by
            the sum of all values at that location coordinate.
            "percent" is the same but multiplied by 100 to show
            percentages.
        boxgap
            Sets the gap (in plot fraction) between boxes of
            adjacent location coordinates. Has no effect on traces
            that have "width" set.
        boxgroupgap
            Sets the gap (in plot fraction) between boxes of the
            same location coordinate. Has no effect on traces that
            have "width" set.
        boxmode
            Determines how boxes at the same location coordinate
            are displayed on the graph. If "group", the boxes are
            plotted next to one another centered around the shared
            location. If "overlay", the boxes are plotted over one
            another, you might need to set "opacity" to see them
            multiple boxes. Has no effect on traces that have
            "width" set.
        calendar
            Sets the default calendar system to use for
            interpreting and displaying dates throughout the plot.
        clickmode
            Determines the mode of single click interactions.
            "event" is the default value and emits the
            `plotly_click` event. In addition this mode emits the
            `plotly_selected` event in drag modes "lasso" and
            "select", but with no event data attached (kept for
            compatibility reasons). The "select" flag enables
            selecting single data points via click. This mode also
            supports persistent selections, meaning that pressing
            Shift while clicking, adds to / subtracts from an
            existing selection. "select" with `hovermode`: "x" can
            be confusing, consider explicitly setting `hovermode`:
            "closest" when using this feature. Selection events are
            sent accordingly as long as "event" flag is set as
            well. When the "event" flag is missing, `plotly_click`
            and `plotly_selected` events are not fired.
        coloraxis
            :class:`plotly.graph_objects.layout.Coloraxis` instance
            or dict with compatible properties
        colorscale
            :class:`plotly.graph_objects.layout.Colorscale`
            instance or dict with compatible properties
        colorway
            Sets the default trace colors.
        computed
            Placeholder for exporting automargin-impacting values
            namely `margin.t`, `margin.b`, `margin.l` and
            `margin.r` in "full-json" mode.
        datarevision
            If provided, a changed value tells `Plotly.react` that
            one or more data arrays has changed. This way you can
            modify arrays in-place rather than making a complete
            new copy for an incremental change. If NOT provided,
            `Plotly.react` assumes that data arrays are being
            treated as immutable, thus any data array with a
            different identity from its predecessor contains new
            data.
        dragmode
            Determines the mode of drag interactions. "select" and
            "lasso" apply only to scatter traces with markers or
            text. "orbit" and "turntable" apply only to 3D scenes.
        editrevision
            Controls persistence of user-driven changes in
            `editable: true` configuration, other than trace names
            and axis titles. Defaults to `layout.uirevision`.
        extendfunnelareacolors
            If `true`, the funnelarea slice colors (whether given
            by `funnelareacolorway` or inherited from `colorway`)
            will be extended to three times its original length by
            first repeating every color 20% lighter then each color
            20% darker. This is intended to reduce the likelihood
            of reusing the same color when you have many slices,
            but you can set `false` to disable. Colors provided in
            the trace, using `marker.colors`, are never extended.
        extendiciclecolors
            If `true`, the icicle slice colors (whether given by
            `iciclecolorway` or inherited from `colorway`) will be
            extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        extendpiecolors
            If `true`, the pie slice colors (whether given by
            `piecolorway` or inherited from `colorway`) will be
            extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        extendsunburstcolors
            If `true`, the sunburst slice colors (whether given by
            `sunburstcolorway` or inherited from `colorway`) will
            be extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        extendtreemapcolors
            If `true`, the treemap slice colors (whether given by
            `treemapcolorway` or inherited from `colorway`) will be
            extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        font
            Sets the global font. Note that fonts used in traces
            and other layout components inherit from the global
            font.
        funnelareacolorway
            Sets the default funnelarea slice colors. Defaults to
            the main `colorway` used for trace colors. If you
            specify a new list here it can still be extended with
            lighter and darker colors, see
            `extendfunnelareacolors`.
        funnelgap
            Sets the gap (in plot fraction) between bars of
            adjacent location coordinates.
        funnelgroupgap
            Sets the gap (in plot fraction) between bars of the
            same location coordinate.
        funnelmode
            Determines how bars at the same location coordinate are
            displayed on the graph. With "stack", the bars are
            stacked on top of one another With "group", the bars
            are plotted next to one another centered around the
            shared location. With "overlay", the bars are plotted
            over one another, you might need to reduce "opacity" to
            see multiple bars.
        geo
            :class:`plotly.graph_objects.layout.Geo` instance or
            dict with compatible properties
        grid
            :class:`plotly.graph_objects.layout.Grid` instance or
            dict with compatible properties
        height
            Sets the plot's height (in px).
        hiddenlabels
            hiddenlabels is the funnelarea & pie chart analog of
            visible:'legendonly' but it can contain many labels,
            and can simultaneously hide slices from several
            pies/funnelarea charts
        hiddenlabelssrc
            Sets the source reference on Chart Studio Cloud for
            `hiddenlabels`.
        hidesources
            Determines whether or not a text link citing the data
            source is placed at the bottom-right cored of the
            figure. Has only an effect only on graphs that have
            been generated via forked graphs from the Chart Studio
            Cloud (at https://chart-studio.plotly.com or on-
            premise).
        hoverdistance
            Sets the default distance (in pixels) to look for data
            to add hover labels (-1 means no cutoff, 0 means no
            looking for data). This is only a real distance for
            hovering on point-like objects, like scatter points.
            For area-like objects (bars, scatter fills, etc)
            hovering is on inside the area and off outside, but
            these objects will not supersede hover on point-like
            objects in case of conflict.
        hoverlabel
            :class:`plotly.graph_objects.layout.Hoverlabel`
            instance or dict with compatible properties
        hovermode
            Determines the mode of hover interactions. If
            "closest", a single hoverlabel will appear for the
            "closest" point within the `hoverdistance`. If "x" (or
            "y"), multiple hoverlabels will appear for multiple
            points at the "closest" x- (or y-) coordinate within
            the `hoverdistance`, with the caveat that no more than
            one hoverlabel will appear per trace. If *x unified*
            (or *y unified*), a single hoverlabel will appear
            multiple points at the closest x- (or y-) coordinate
            within the `hoverdistance` with the caveat that no more
            than one hoverlabel will appear per trace. In this
            mode, spikelines are enabled by default perpendicular
            to the specified axis. If false, hover interactions are
            disabled.
        hoversubplots
            Determines expansion of hover effects to other subplots
            If "single" just the axis pair of the primary point is
            included without overlaying subplots. If "overlaying"
            all subplots using the main axis and occupying the same
            space are included. If "axis", also include stacked
            subplots using the same axis when `hovermode` is set to
            "x", *x unified*, "y" or *y unified*.
        iciclecolorway
            Sets the default icicle slice colors. Defaults to the
            main `colorway` used for trace colors. If you specify a
            new list here it can still be extended with lighter and
            darker colors, see `extendiciclecolors`.
        images
            A tuple of :class:`plotly.graph_objects.layout.Image`
            instances or dicts with compatible properties
        imagedefaults
            When used in a template (as
            layout.template.layout.imagedefaults), sets the default
            property values to use for elements of layout.images
        legend
            :class:`plotly.graph_objects.layout.Legend` instance or
            dict with compatible properties
        map
            :class:`plotly.graph_objects.layout.Map` instance or
            dict with compatible properties
        mapbox
            :class:`plotly.graph_objects.layout.Mapbox` instance or
            dict with compatible properties
        margin
            :class:`plotly.graph_objects.layout.Margin` instance or
            dict with compatible properties
        meta
            Assigns extra meta information that can be used in
            various `text` attributes. Attributes such as the
            graph, axis and colorbar `title.text`, annotation
            `text` `trace.name` in legend items, `rangeselector`,
            `updatemenus` and `sliders` `label` text all support
            `meta`. One can access `meta` fields using template
            strings: `%{meta[i]}` where `i` is the index of the
            `meta` item in question. `meta` can also be an object
            for example `{key: value}` which can be accessed
            %{meta[key]}.
        metasrc
            Sets the source reference on Chart Studio Cloud for
            `meta`.
        minreducedheight
            Minimum height of the plot with margin.automargin
            applied (in px)
        minreducedwidth
            Minimum width of the plot with margin.automargin
            applied (in px)
        modebar
            :class:`plotly.graph_objects.layout.Modebar` instance
            or dict with compatible properties
        newselection
            :class:`plotly.graph_objects.layout.Newselection`
            instance or dict with compatible properties
        newshape
            :class:`plotly.graph_objects.layout.Newshape` instance
            or dict with compatible properties
        paper_bgcolor
            Sets the background color of the paper where the graph
            is drawn.
        piecolorway
            Sets the default pie slice colors. Defaults to the main
            `colorway` used for trace colors. If you specify a new
            list here it can still be extended with lighter and
            darker colors, see `extendpiecolors`.
        plot_bgcolor
            Sets the background color of the plotting area in-
            between x and y axes.
        polar
            :class:`plotly.graph_objects.layout.Polar` instance or
            dict with compatible properties
        scattergap
            Sets the gap (in plot fraction) between scatter points
            of adjacent location coordinates. Defaults to `bargap`.
        scattermode
            Determines how scatter points at the same location
            coordinate are displayed on the graph. With "group",
            the scatter points are plotted next to one another
            centered around the shared location. With "overlay",
            the scatter points are plotted over one another, you
            might need to reduce "opacity" to see multiple scatter
            points.
        scene
            :class:`plotly.graph_objects.layout.Scene` instance or
            dict with compatible properties
        selectdirection
            When `dragmode` is set to "select", this limits the
            selection of the drag to horizontal, vertical or
            diagonal. "h" only allows horizontal selection, "v"
            only vertical, "d" only diagonal and "any" sets no
            limit.
        selectionrevision
            Controls persistence of user-driven changes in selected
            points from all traces.
        selections
            A tuple of
            :class:`plotly.graph_objects.layout.Selection`
            instances or dicts with compatible properties
        selectiondefaults
            When used in a template (as
            layout.template.layout.selectiondefaults), sets the
            default property values to use for elements of
            layout.selections
        separators
            Sets the decimal and thousand separators. For example,
            *. * puts a '.' before decimals and a space between
            thousands. In English locales, dflt is ".," but other
            locales may alter this default.
        shapes
            A tuple of :class:`plotly.graph_objects.layout.Shape`
            instances or dicts with compatible properties
        shapedefaults
            When used in a template (as
            layout.template.layout.shapedefaults), sets the default
            property values to use for elements of layout.shapes
        showlegend
            Determines whether or not a legend is drawn. Default is
            `true` if there is a trace to show and any of these: a)
            Two or more traces would by default be shown in the
            legend. b) One pie trace is shown in the legend. c) One
            trace is explicitly given with `showlegend: true`.
        sliders
            A tuple of :class:`plotly.graph_objects.layout.Slider`
            instances or dicts with compatible properties
        sliderdefaults
            When used in a template (as
            layout.template.layout.sliderdefaults), sets the
            default property values to use for elements of
            layout.sliders
        smith
            :class:`plotly.graph_objects.layout.Smith` instance or
            dict with compatible properties
        spikedistance
            Sets the default distance (in pixels) to look for data
            to draw spikelines to (-1 means no cutoff, 0 means no
            looking for data). As with hoverdistance, distance does
            not apply to area-like objects. In addition, some
            objects can be hovered on but will not generate
            spikelines, such as scatter fills.
        sunburstcolorway
            Sets the default sunburst slice colors. Defaults to the
            main `colorway` used for trace colors. If you specify a
            new list here it can still be extended with lighter and
            darker colors, see `extendsunburstcolors`.
        template
            Default attributes to be applied to the plot. This
            should be a dict with format: `{'layout':
            layoutTemplate, 'data': {trace_type: [traceTemplate,
            ...], ...}}` where `layoutTemplate` is a dict matching
            the structure of `figure.layout` and `traceTemplate` is
            a dict matching the structure of the trace with type
            `trace_type` (e.g. 'scatter'). Alternatively, this may
            be specified as an instance of
            plotly.graph_objs.layout.Template.  Trace templates are
            applied cyclically to traces of each type. Container
            arrays (eg `annotations`) have special handling: An
            object ending in `defaults` (eg `annotationdefaults`)
            is applied to each array item. But if an item has a
            `templateitemname` key we look in the template array
            for an item with matching `name` and apply that
            instead. If no matching `name` is found we mark the
            item invisible. Any named template item not referenced
            is appended to the end of the array, so this can be
            used to add a watermark annotation or a logo image, for
            example. To omit one of these items on the plot, make
            an item with matching `templateitemname` and `visible:
            false`.
        ternary
            :class:`plotly.graph_objects.layout.Ternary` instance
            or dict with compatible properties
        title
            :class:`plotly.graph_objects.layout.Title` instance or
            dict with compatible properties
        transition
            Sets transition options used during Plotly.react
            updates.
        treemapcolorway
            Sets the default treemap slice colors. Defaults to the
            main `colorway` used for trace colors. If you specify a
            new list here it can still be extended with lighter and
            darker colors, see `extendtreemapcolors`.
        uirevision
            Used to allow user interactions with the plot to
            persist after `Plotly.react` calls that are unaware of
            these interactions. If `uirevision` is omitted, or if
            it is given and it changed from the previous
            `Plotly.react` call, the exact new figure is used. If
            `uirevision` is truthy and did NOT change, any
            attribute that has been affected by user interactions
            and did not receive a different value in the new figure
            will keep the interaction value. `layout.uirevision`
            attribute serves as the default for `uirevision`
            attributes in various sub-containers. For finer control
            you can set these sub-attributes directly. For example,
            if your app separately controls the data on the x and y
            axes you might set `xaxis.uirevision=*time*` and
            `yaxis.uirevision=*cost*`. Then if only the y data is
            changed, you can update `yaxis.uirevision=*quantity*`
            and the y axis range will reset but the x axis range
            will retain any user-driven zoom.
        uniformtext
            :class:`plotly.graph_objects.layout.Uniformtext`
            instance or dict with compatible properties
        updatemenus
            A tuple of
            :class:`plotly.graph_objects.layout.Updatemenu`
            instances or dicts with compatible properties
        updatemenudefaults
            When used in a template (as
            layout.template.layout.updatemenudefaults), sets the
            default property values to use for elements of
            layout.updatemenus
        violingap
            Sets the gap (in plot fraction) between violins of
            adjacent location coordinates. Has no effect on traces
            that have "width" set.
        violingroupgap
            Sets the gap (in plot fraction) between violins of the
            same location coordinate. Has no effect on traces that
            have "width" set.
        violinmode
            Determines how violins at the same location coordinate
            are displayed on the graph. If "group", the violins are
            plotted next to one another centered around the shared
            location. If "overlay", the violins are plotted over
            one another, you might need to set "opacity" to see
            them multiple violins. Has no effect on traces that
            have "width" set.
        waterfallgap
            Sets the gap (in plot fraction) between bars of
            adjacent location coordinates.
        waterfallgroupgap
            Sets the gap (in plot fraction) between bars of the
            same location coordinate.
        waterfallmode
            Determines how bars at the same location coordinate are
            displayed on the graph. With "group", the bars are
            plotted next to one another centered around the shared
            location. With "overlay", the bars are plotted over one
            another, you might need to reduce "opacity" to see
            multiple bars.
        width
            Sets the plot's width (in px).
        xaxis
            :class:`plotly.graph_objects.layout.XAxis` instance or
            dict with compatible properties
        yaxis
            :class:`plotly.graph_objects.layout.YAxis` instance or
            dict with compatible properties
        
Did you mean "xaxis"?

Bad property path:
axis_title_size
^^^^
2025-05-29 11:55:05,221 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:55:31,633 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:55:31,655 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 11:55:31,656 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 11:55:31,656 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 11:55:31,656 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 11:55:31,657 [INFO] __main__:339: Database initialized successfully
2025-05-29 11:55:31,659 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 11:55:31,659 [INFO] __main__:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 11:55:32,214 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 11:55:32,371 [INFO] httpx:1025: HTTP Request: GET http://127.0.0.1:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 11:55:32,389 [INFO] httpx:1025: HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-05-29 11:55:32,671 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 11:55:33,139 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 11:55:33,678 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:53:48,620 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:53:48,643 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:53:48,643 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 15:53:48,644 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 15:53:48,644 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 15:53:48,646 [INFO] app:339: Database initialized successfully
2025-05-29 15:53:48,648 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 15:53:48,650 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 15:53:49,168 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 15:53:49,664 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 15:53:50,045 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 15:53:50,671 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:53:51,235 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 15:53:53,303 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 15:54:18,288 [ERROR] app:817: Search failed for 'Chatgpt': 2 validation errors for ProgressUnit
progress
  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='Starting web scraping...', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/float_parsing
desc
  Input should be a valid string [type=string_type, input_value=0.2, input_type=float]
    For further information visit https://errors.pydantic.dev/2.10/v/string_type
2025-05-29 15:54:56,788 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:56:02,887 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:57:09,013 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:58:33,969 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:58:33,992 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:58:33,992 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 15:58:33,992 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 15:58:33,992 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 15:58:33,993 [INFO] __main__:339: Database initialized successfully
2025-05-29 15:58:33,995 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 15:58:33,996 [INFO] __main__:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 15:58:34,577 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 15:58:34,702 [INFO] httpx:1025: HTTP Request: GET http://127.0.0.1:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 15:58:34,718 [INFO] httpx:1025: HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-05-29 15:58:35,008 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 15:58:35,483 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 15:58:36,016 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:58:56,515 [INFO] __main__:2076: Shutting down Reddit Mention Tracker...
2025-05-29 15:59:18,945 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:59:18,969 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:59:18,969 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 15:59:18,969 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 15:59:18,969 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 15:59:18,971 [INFO] app:339: Database initialized successfully
2025-05-29 15:59:18,973 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 15:59:18,974 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 15:59:19,508 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 15:59:19,985 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 15:59:20,540 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 15:59:20,992 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 15:59:21,537 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 15:59:23,586 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 15:59:33,920 [ERROR] app:817: Search failed for 'chatgpt': 2 validation errors for ProgressUnit
progress
  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='Starting web scraping...', input_type=str]
    For further information visit https://errors.pydantic.dev/2.10/v/float_parsing
desc
  Input should be a valid string [type=string_type, input_value=0.2, input_type=float]
    For further information visit https://errors.pydantic.dev/2.10/v/string_type
2025-05-29 15:59:33,921 [ERROR] app:1374: Search failed: cannot access local variable 'traceback' where it is not associated with a value
2025-05-29 16:00:27,087 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:01:33,206 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:04:51,675 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:04:51,702 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:04:51,702 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 16:04:51,702 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 16:04:51,702 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:04:51,703 [INFO] __main__:339: Database initialized successfully
2025-05-29 16:04:51,706 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:04:51,707 [INFO] __main__:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:04:52,446 [INFO] httpx:1025: HTTP Request: GET http://127.0.0.1:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 16:04:52,464 [INFO] httpx:1025: HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-05-29 16:04:52,720 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 16:04:53,728 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:04:53,810 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:04:54,053 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:04:54,109 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:05:46,993 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:05:47,016 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:05:47,016 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 16:05:47,017 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 16:05:47,017 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:05:47,019 [INFO] app:339: Database initialized successfully
2025-05-29 16:05:47,021 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:05:47,022 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:05:47,555 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:05:48,031 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 16:05:48,493 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:05:49,040 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:05:49,592 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 16:05:51,640 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 16:06:04,428 [INFO] scraper.reddit_scraper:641: Attempting Reddit API search for: openai
2025-05-29 16:06:04,428 [INFO] scraper.reddit_scraper.APIClient:172: [SEARCH] Starting COMPREHENSIVE search for 'openai' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 16:06:04,428 [INFO] scraper.reddit_scraper.APIClient:187: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 16:06:04,428 [INFO] scraper.reddit_scraper.APIClient:191:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 16:06:09,016 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 93 posts with sort=relevance, time=week. New unique: 93. Total unique: 93
2025-05-29 16:06:09,016 [INFO] scraper.reddit_scraper.APIClient:191:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 16:06:12,043 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 91 posts with sort=top, time=week. New unique: 60. Total unique: 153
2025-05-29 16:06:12,043 [INFO] scraper.reddit_scraper.APIClient:191:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 16:06:14,588 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 100 posts with sort=hot, time=week. New unique: 55. Total unique: 208
2025-05-29 16:06:14,589 [INFO] scraper.reddit_scraper.APIClient:191:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 16:06:17,252 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 88. Total unique: 296
2025-05-29 16:06:17,252 [INFO] scraper.reddit_scraper.APIClient:191:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 16:06:19,247 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 93 posts with sort=relevance, time=week. New unique: 0. Total unique: 296
2025-05-29 16:06:19,247 [INFO] scraper.reddit_scraper.APIClient:191:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 16:06:21,505 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 296
2025-05-29 16:06:21,505 [INFO] scraper.reddit_scraper.APIClient:218: [TARGET] Need 704 more results, searching popular subreddits...
2025-05-29 16:06:21,919 [INFO] scraper.reddit_scraper.APIClient:383: Found 6 posts in r/technology. Total additional: 2
2025-05-29 16:06:23,012 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/programming. Total additional: 3
2025-05-29 16:06:24,108 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/MachineLearning. Total additional: 4
2025-05-29 16:06:25,274 [INFO] scraper.reddit_scraper.APIClient:383: Found 7 posts in r/artificial. Total additional: 9
2025-05-29 16:06:27,271 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/gadgets. Total additional: 9
2025-05-29 16:06:27,630 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/science. Total additional: 9
2025-05-29 16:06:28,781 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/askreddit. Total additional: 10
2025-05-29 16:06:29,853 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/explainlikeimfive. Total additional: 10
2025-05-29 16:06:30,965 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/news. Total additional: 10
2025-05-29 16:06:32,036 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/worldnews. Total additional: 10
2025-05-29 16:06:33,208 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/business. Total additional: 10
2025-05-29 16:06:34,315 [INFO] scraper.reddit_scraper.APIClient:383: Found 2 posts in r/entrepreneur. Total additional: 12
2025-05-29 16:06:35,353 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/startup. Total additional: 12
2025-05-29 16:06:36,449 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/investing. Total additional: 13
2025-05-29 16:06:38,531 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/datascience. Total additional: 13
2025-05-29 16:06:38,531 [INFO] scraper.reddit_scraper.APIClient:222:    [SUBREDDIT] Added 13 from subreddit search. Total: 309
2025-05-29 16:06:38,531 [INFO] scraper.reddit_scraper.APIClient:227: [VARIANTS] Need 691 more results, trying query variations...
2025-05-29 16:06:38,531 [INFO] scraper.reddit_scraper.APIClient:238:    [1/4] Trying variant: "openai"
2025-05-29 16:06:39,946 [INFO] scraper.reddit_scraper.APIClient:251:    [VARIANT] Variant '"openai"' added 0 new posts. Total: 309
2025-05-29 16:06:39,947 [INFO] scraper.reddit_scraper.APIClient:238:    [2/4] Trying variant: openai OR openai
2025-05-29 16:06:41,123 [INFO] scraper.reddit_scraper.APIClient:251:    [VARIANT] Variant 'openai OR openai' added 0 new posts. Total: 309
2025-05-29 16:06:41,123 [INFO] scraper.reddit_scraper.APIClient:238:    [3/4] Trying variant: title:openai
2025-05-29 16:06:43,501 [INFO] scraper.reddit_scraper.APIClient:251:    [VARIANT] Variant 'title:openai' added 0 new posts. Total: 309
2025-05-29 16:06:43,501 [INFO] scraper.reddit_scraper.APIClient:238:    [4/4] Trying variant: selftext:openai
2025-05-29 16:06:45,525 [INFO] scraper.reddit_scraper.APIClient:251:    [VARIANT] Variant 'selftext:openai' added 23 new posts. Total: 332
2025-05-29 16:06:45,525 [INFO] scraper.reddit_scraper.APIClient:263: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'openai':
[STATS] Total unique posts found: 332
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 164
[RESULT] SUCCESS: Found 332 posts that will be processed and saved

2025-05-29 16:06:45,525 [INFO] scraper.reddit_scraper:650: Reddit comprehensive API returned 332 mentions
2025-05-29 16:06:45,525 [INFO] scraper.reddit_scraper:996: Post-processing 332 mentions...
2025-05-29 16:06:45,526 [INFO] scraper.reddit_scraper:1440: Deduplication: 332 -> 332 mentions (removed 0 exact duplicates)
2025-05-29 16:06:45,526 [INFO] scraper.reddit_scraper:1000: After deduplication: 332 mentions
2025-05-29 16:06:45,534 [INFO] scraper.reddit_scraper:1004: After validation: 332 mentions
2025-05-29 16:06:45,903 [INFO] scraper.reddit_scraper:1019: After database mapping: 332 mentions (ALL KEPT - no quality filtering)
2025-05-29 16:06:45,904 [INFO] scraper.reddit_scraper:1039: Final result: 332 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 16:06:45,906 [INFO] analytics.data_validator:236: Validating dataset of 332 mentions
2025-05-29 16:06:49,506 [INFO] analytics.data_validator:259: Validation complete: 332/332 mentions passed
2025-05-29 16:06:49,507 [INFO] app:703: Data validation: 332/332 mentions passed
2025-05-29 16:06:49,816 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,817 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,817 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,818 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,819 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,819 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,820 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,820 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,821 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,821 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,822 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,823 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,823 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,824 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,824 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,825 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,826 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,826 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,827 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,827 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,828 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,829 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,829 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,830 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,831 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,831 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,832 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,832 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,833 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,834 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,834 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,835 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,835 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,836 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,836 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,837 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,838 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,838 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,839 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,840 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,840 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,841 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,841 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,842 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,843 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,843 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,844 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,845 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,845 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,846 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,846 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,847 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,847 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,848 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,849 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,849 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,850 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,851 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,851 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,852 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,852 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,853 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,854 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,854 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,855 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,855 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,856 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,857 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,857 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,858 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,859 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,859 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,860 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,860 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,861 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,862 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,862 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,868 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,875 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,876 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,876 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,877 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,877 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,878 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,879 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,879 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,880 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,880 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,881 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,881 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,882 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,882 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,883 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,884 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,884 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,885 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,885 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,886 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,886 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,887 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,888 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,888 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,889 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,890 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,890 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,891 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,891 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,892 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,892 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,893 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,893 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,894 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,894 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,895 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,896 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,896 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,897 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,897 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,898 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,898 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,899 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,900 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,900 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,901 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,901 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,902 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,903 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,903 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,904 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,904 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,905 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,905 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,906 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,907 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,907 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,908 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,908 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,909 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,909 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,910 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,911 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,911 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,915 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,916 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,916 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,917 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,918 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,918 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,919 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,919 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,920 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,921 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,921 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,922 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,922 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,923 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,924 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,924 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,925 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,925 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,926 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,927 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,927 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,928 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,928 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,929 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,930 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,930 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,931 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,934 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,935 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,935 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,936 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,937 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,937 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,938 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,939 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,939 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,940 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,940 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,941 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,944 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,945 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,946 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,949 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,950 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,951 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,951 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,952 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,952 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,953 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,953 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,954 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,954 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,955 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,955 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,956 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,956 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,957 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,958 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,961 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,961 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,962 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,966 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,967 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,967 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,968 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,968 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,969 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,969 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,973 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,974 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,974 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,975 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,975 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,976 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,977 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,977 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,978 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,978 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,979 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,979 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,980 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,981 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,981 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,982 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,982 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,983 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,983 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,984 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,988 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,994 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,995 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,995 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,996 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,996 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,997 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,998 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,999 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:49,999 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,000 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,000 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,001 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,001 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,002 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,002 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,003 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,003 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,004 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,005 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,005 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,006 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,006 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,007 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,007 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,008 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,008 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,009 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,010 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,010 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,014 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,015 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,015 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,016 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,016 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,017 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,018 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,018 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,019 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,019 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,020 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,020 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,021 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,022 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,022 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,023 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,023 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,024 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,024 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,025 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,026 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,026 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,027 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,027 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,028 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,029 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,029 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,030 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,034 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,034 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,035 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,035 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,036 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,037 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,037 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,038 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,038 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,039 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,039 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,040 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,040 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,044 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,045 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,045 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,046 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,049 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,050 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,050 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,060 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,061 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,061 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,062 [ERROR] app:751: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:06:50,105 [INFO] app:807: Search completed: openai -> 332 mentions
2025-05-29 16:06:50,242 [ERROR] app:1352: Visualization generation failed: 'EnhancedRedditMentionTracker' object has no attribute '_generate_sentiment_analysis'
2025-05-29 16:06:55,164 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:08:01,285 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:09:07,391 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:13:08,124 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:13:08,147 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:13:08,148 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 16:13:08,148 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 16:13:08,148 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:13:08,150 [INFO] __main__:378: Database initialized successfully
2025-05-29 16:13:08,152 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:13:08,153 [INFO] __main__:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:13:08,761 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:13:08,856 [INFO] httpx:1025: HTTP Request: GET http://127.0.0.1:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 16:13:08,871 [INFO] httpx:1025: HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-05-29 16:13:09,164 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 16:13:09,641 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:13:10,171 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:14:00,730 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:14:00,753 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:14:00,753 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 16:14:00,753 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 16:14:00,754 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:14:00,755 [INFO] app:378: Database initialized successfully
2025-05-29 16:14:00,758 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:14:00,759 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:14:01,346 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:14:01,769 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 16:14:02,149 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:14:02,776 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:14:03,345 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 16:14:05,371 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 16:14:19,168 [INFO] app:591: Cache hit for search term: openai
2025-05-29 16:14:19,301 [ERROR] app:674: Error generating metrics from cached data: Invalid property specified for object of type plotly.graph_objs.Layout: 'axis'

Did you mean "xaxis"?

    Valid properties:
        activeselection
            :class:`plotly.graph_objects.layout.Activeselection`
            instance or dict with compatible properties
        activeshape
            :class:`plotly.graph_objects.layout.Activeshape`
            instance or dict with compatible properties
        annotations
            A tuple of
            :class:`plotly.graph_objects.layout.Annotation`
            instances or dicts with compatible properties
        annotationdefaults
            When used in a template (as
            layout.template.layout.annotationdefaults), sets the
            default property values to use for elements of
            layout.annotations
        autosize
            Determines whether or not a layout width or height that
            has been left undefined by the user is initialized on
            each relayout. Note that, regardless of this attribute,
            an undefined layout width or height is always
            initialized on the first call to plot.
        autotypenumbers
            Using "strict" a numeric string in trace data is not
            converted to a number. Using *convert types* a numeric
            string in trace data may be treated as a number during
            automatic axis `type` detection. This is the default
            value; however it could be overridden for individual
            axes.
        barcornerradius
            Sets the rounding of bar corners. May be an integer
            number of pixels, or a percentage of bar width (as a
            string ending in %).
        bargap
            Sets the gap (in plot fraction) between bars of
            adjacent location coordinates.
        bargroupgap
            Sets the gap (in plot fraction) between bars of the
            same location coordinate.
        barmode
            Determines how bars at the same location coordinate are
            displayed on the graph. With "stack", the bars are
            stacked on top of one another With "relative", the bars
            are stacked on top of one another, with negative values
            below the axis, positive values above With "group", the
            bars are plotted next to one another centered around
            the shared location. With "overlay", the bars are
            plotted over one another, you might need to reduce
            "opacity" to see multiple bars.
        barnorm
            Sets the normalization for bar traces on the graph.
            With "fraction", the value of each bar is divided by
            the sum of all values at that location coordinate.
            "percent" is the same but multiplied by 100 to show
            percentages.
        boxgap
            Sets the gap (in plot fraction) between boxes of
            adjacent location coordinates. Has no effect on traces
            that have "width" set.
        boxgroupgap
            Sets the gap (in plot fraction) between boxes of the
            same location coordinate. Has no effect on traces that
            have "width" set.
        boxmode
            Determines how boxes at the same location coordinate
            are displayed on the graph. If "group", the boxes are
            plotted next to one another centered around the shared
            location. If "overlay", the boxes are plotted over one
            another, you might need to set "opacity" to see them
            multiple boxes. Has no effect on traces that have
            "width" set.
        calendar
            Sets the default calendar system to use for
            interpreting and displaying dates throughout the plot.
        clickmode
            Determines the mode of single click interactions.
            "event" is the default value and emits the
            `plotly_click` event. In addition this mode emits the
            `plotly_selected` event in drag modes "lasso" and
            "select", but with no event data attached (kept for
            compatibility reasons). The "select" flag enables
            selecting single data points via click. This mode also
            supports persistent selections, meaning that pressing
            Shift while clicking, adds to / subtracts from an
            existing selection. "select" with `hovermode`: "x" can
            be confusing, consider explicitly setting `hovermode`:
            "closest" when using this feature. Selection events are
            sent accordingly as long as "event" flag is set as
            well. When the "event" flag is missing, `plotly_click`
            and `plotly_selected` events are not fired.
        coloraxis
            :class:`plotly.graph_objects.layout.Coloraxis` instance
            or dict with compatible properties
        colorscale
            :class:`plotly.graph_objects.layout.Colorscale`
            instance or dict with compatible properties
        colorway
            Sets the default trace colors.
        computed
            Placeholder for exporting automargin-impacting values
            namely `margin.t`, `margin.b`, `margin.l` and
            `margin.r` in "full-json" mode.
        datarevision
            If provided, a changed value tells `Plotly.react` that
            one or more data arrays has changed. This way you can
            modify arrays in-place rather than making a complete
            new copy for an incremental change. If NOT provided,
            `Plotly.react` assumes that data arrays are being
            treated as immutable, thus any data array with a
            different identity from its predecessor contains new
            data.
        dragmode
            Determines the mode of drag interactions. "select" and
            "lasso" apply only to scatter traces with markers or
            text. "orbit" and "turntable" apply only to 3D scenes.
        editrevision
            Controls persistence of user-driven changes in
            `editable: true` configuration, other than trace names
            and axis titles. Defaults to `layout.uirevision`.
        extendfunnelareacolors
            If `true`, the funnelarea slice colors (whether given
            by `funnelareacolorway` or inherited from `colorway`)
            will be extended to three times its original length by
            first repeating every color 20% lighter then each color
            20% darker. This is intended to reduce the likelihood
            of reusing the same color when you have many slices,
            but you can set `false` to disable. Colors provided in
            the trace, using `marker.colors`, are never extended.
        extendiciclecolors
            If `true`, the icicle slice colors (whether given by
            `iciclecolorway` or inherited from `colorway`) will be
            extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        extendpiecolors
            If `true`, the pie slice colors (whether given by
            `piecolorway` or inherited from `colorway`) will be
            extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        extendsunburstcolors
            If `true`, the sunburst slice colors (whether given by
            `sunburstcolorway` or inherited from `colorway`) will
            be extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        extendtreemapcolors
            If `true`, the treemap slice colors (whether given by
            `treemapcolorway` or inherited from `colorway`) will be
            extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        font
            Sets the global font. Note that fonts used in traces
            and other layout components inherit from the global
            font.
        funnelareacolorway
            Sets the default funnelarea slice colors. Defaults to
            the main `colorway` used for trace colors. If you
            specify a new list here it can still be extended with
            lighter and darker colors, see
            `extendfunnelareacolors`.
        funnelgap
            Sets the gap (in plot fraction) between bars of
            adjacent location coordinates.
        funnelgroupgap
            Sets the gap (in plot fraction) between bars of the
            same location coordinate.
        funnelmode
            Determines how bars at the same location coordinate are
            displayed on the graph. With "stack", the bars are
            stacked on top of one another With "group", the bars
            are plotted next to one another centered around the
            shared location. With "overlay", the bars are plotted
            over one another, you might need to reduce "opacity" to
            see multiple bars.
        geo
            :class:`plotly.graph_objects.layout.Geo` instance or
            dict with compatible properties
        grid
            :class:`plotly.graph_objects.layout.Grid` instance or
            dict with compatible properties
        height
            Sets the plot's height (in px).
        hiddenlabels
            hiddenlabels is the funnelarea & pie chart analog of
            visible:'legendonly' but it can contain many labels,
            and can simultaneously hide slices from several
            pies/funnelarea charts
        hiddenlabelssrc
            Sets the source reference on Chart Studio Cloud for
            `hiddenlabels`.
        hidesources
            Determines whether or not a text link citing the data
            source is placed at the bottom-right cored of the
            figure. Has only an effect only on graphs that have
            been generated via forked graphs from the Chart Studio
            Cloud (at https://chart-studio.plotly.com or on-
            premise).
        hoverdistance
            Sets the default distance (in pixels) to look for data
            to add hover labels (-1 means no cutoff, 0 means no
            looking for data). This is only a real distance for
            hovering on point-like objects, like scatter points.
            For area-like objects (bars, scatter fills, etc)
            hovering is on inside the area and off outside, but
            these objects will not supersede hover on point-like
            objects in case of conflict.
        hoverlabel
            :class:`plotly.graph_objects.layout.Hoverlabel`
            instance or dict with compatible properties
        hovermode
            Determines the mode of hover interactions. If
            "closest", a single hoverlabel will appear for the
            "closest" point within the `hoverdistance`. If "x" (or
            "y"), multiple hoverlabels will appear for multiple
            points at the "closest" x- (or y-) coordinate within
            the `hoverdistance`, with the caveat that no more than
            one hoverlabel will appear per trace. If *x unified*
            (or *y unified*), a single hoverlabel will appear
            multiple points at the closest x- (or y-) coordinate
            within the `hoverdistance` with the caveat that no more
            than one hoverlabel will appear per trace. In this
            mode, spikelines are enabled by default perpendicular
            to the specified axis. If false, hover interactions are
            disabled.
        hoversubplots
            Determines expansion of hover effects to other subplots
            If "single" just the axis pair of the primary point is
            included without overlaying subplots. If "overlaying"
            all subplots using the main axis and occupying the same
            space are included. If "axis", also include stacked
            subplots using the same axis when `hovermode` is set to
            "x", *x unified*, "y" or *y unified*.
        iciclecolorway
            Sets the default icicle slice colors. Defaults to the
            main `colorway` used for trace colors. If you specify a
            new list here it can still be extended with lighter and
            darker colors, see `extendiciclecolors`.
        images
            A tuple of :class:`plotly.graph_objects.layout.Image`
            instances or dicts with compatible properties
        imagedefaults
            When used in a template (as
            layout.template.layout.imagedefaults), sets the default
            property values to use for elements of layout.images
        legend
            :class:`plotly.graph_objects.layout.Legend` instance or
            dict with compatible properties
        map
            :class:`plotly.graph_objects.layout.Map` instance or
            dict with compatible properties
        mapbox
            :class:`plotly.graph_objects.layout.Mapbox` instance or
            dict with compatible properties
        margin
            :class:`plotly.graph_objects.layout.Margin` instance or
            dict with compatible properties
        meta
            Assigns extra meta information that can be used in
            various `text` attributes. Attributes such as the
            graph, axis and colorbar `title.text`, annotation
            `text` `trace.name` in legend items, `rangeselector`,
            `updatemenus` and `sliders` `label` text all support
            `meta`. One can access `meta` fields using template
            strings: `%{meta[i]}` where `i` is the index of the
            `meta` item in question. `meta` can also be an object
            for example `{key: value}` which can be accessed
            %{meta[key]}.
        metasrc
            Sets the source reference on Chart Studio Cloud for
            `meta`.
        minreducedheight
            Minimum height of the plot with margin.automargin
            applied (in px)
        minreducedwidth
            Minimum width of the plot with margin.automargin
            applied (in px)
        modebar
            :class:`plotly.graph_objects.layout.Modebar` instance
            or dict with compatible properties
        newselection
            :class:`plotly.graph_objects.layout.Newselection`
            instance or dict with compatible properties
        newshape
            :class:`plotly.graph_objects.layout.Newshape` instance
            or dict with compatible properties
        paper_bgcolor
            Sets the background color of the paper where the graph
            is drawn.
        piecolorway
            Sets the default pie slice colors. Defaults to the main
            `colorway` used for trace colors. If you specify a new
            list here it can still be extended with lighter and
            darker colors, see `extendpiecolors`.
        plot_bgcolor
            Sets the background color of the plotting area in-
            between x and y axes.
        polar
            :class:`plotly.graph_objects.layout.Polar` instance or
            dict with compatible properties
        scattergap
            Sets the gap (in plot fraction) between scatter points
            of adjacent location coordinates. Defaults to `bargap`.
        scattermode
            Determines how scatter points at the same location
            coordinate are displayed on the graph. With "group",
            the scatter points are plotted next to one another
            centered around the shared location. With "overlay",
            the scatter points are plotted over one another, you
            might need to reduce "opacity" to see multiple scatter
            points.
        scene
            :class:`plotly.graph_objects.layout.Scene` instance or
            dict with compatible properties
        selectdirection
            When `dragmode` is set to "select", this limits the
            selection of the drag to horizontal, vertical or
            diagonal. "h" only allows horizontal selection, "v"
            only vertical, "d" only diagonal and "any" sets no
            limit.
        selectionrevision
            Controls persistence of user-driven changes in selected
            points from all traces.
        selections
            A tuple of
            :class:`plotly.graph_objects.layout.Selection`
            instances or dicts with compatible properties
        selectiondefaults
            When used in a template (as
            layout.template.layout.selectiondefaults), sets the
            default property values to use for elements of
            layout.selections
        separators
            Sets the decimal and thousand separators. For example,
            *. * puts a '.' before decimals and a space between
            thousands. In English locales, dflt is ".," but other
            locales may alter this default.
        shapes
            A tuple of :class:`plotly.graph_objects.layout.Shape`
            instances or dicts with compatible properties
        shapedefaults
            When used in a template (as
            layout.template.layout.shapedefaults), sets the default
            property values to use for elements of layout.shapes
        showlegend
            Determines whether or not a legend is drawn. Default is
            `true` if there is a trace to show and any of these: a)
            Two or more traces would by default be shown in the
            legend. b) One pie trace is shown in the legend. c) One
            trace is explicitly given with `showlegend: true`.
        sliders
            A tuple of :class:`plotly.graph_objects.layout.Slider`
            instances or dicts with compatible properties
        sliderdefaults
            When used in a template (as
            layout.template.layout.sliderdefaults), sets the
            default property values to use for elements of
            layout.sliders
        smith
            :class:`plotly.graph_objects.layout.Smith` instance or
            dict with compatible properties
        spikedistance
            Sets the default distance (in pixels) to look for data
            to draw spikelines to (-1 means no cutoff, 0 means no
            looking for data). As with hoverdistance, distance does
            not apply to area-like objects. In addition, some
            objects can be hovered on but will not generate
            spikelines, such as scatter fills.
        sunburstcolorway
            Sets the default sunburst slice colors. Defaults to the
            main `colorway` used for trace colors. If you specify a
            new list here it can still be extended with lighter and
            darker colors, see `extendsunburstcolors`.
        template
            Default attributes to be applied to the plot. This
            should be a dict with format: `{'layout':
            layoutTemplate, 'data': {trace_type: [traceTemplate,
            ...], ...}}` where `layoutTemplate` is a dict matching
            the structure of `figure.layout` and `traceTemplate` is
            a dict matching the structure of the trace with type
            `trace_type` (e.g. 'scatter'). Alternatively, this may
            be specified as an instance of
            plotly.graph_objs.layout.Template.  Trace templates are
            applied cyclically to traces of each type. Container
            arrays (eg `annotations`) have special handling: An
            object ending in `defaults` (eg `annotationdefaults`)
            is applied to each array item. But if an item has a
            `templateitemname` key we look in the template array
            for an item with matching `name` and apply that
            instead. If no matching `name` is found we mark the
            item invisible. Any named template item not referenced
            is appended to the end of the array, so this can be
            used to add a watermark annotation or a logo image, for
            example. To omit one of these items on the plot, make
            an item with matching `templateitemname` and `visible:
            false`.
        ternary
            :class:`plotly.graph_objects.layout.Ternary` instance
            or dict with compatible properties
        title
            :class:`plotly.graph_objects.layout.Title` instance or
            dict with compatible properties
        transition
            Sets transition options used during Plotly.react
            updates.
        treemapcolorway
            Sets the default treemap slice colors. Defaults to the
            main `colorway` used for trace colors. If you specify a
            new list here it can still be extended with lighter and
            darker colors, see `extendtreemapcolors`.
        uirevision
            Used to allow user interactions with the plot to
            persist after `Plotly.react` calls that are unaware of
            these interactions. If `uirevision` is omitted, or if
            it is given and it changed from the previous
            `Plotly.react` call, the exact new figure is used. If
            `uirevision` is truthy and did NOT change, any
            attribute that has been affected by user interactions
            and did not receive a different value in the new figure
            will keep the interaction value. `layout.uirevision`
            attribute serves as the default for `uirevision`
            attributes in various sub-containers. For finer control
            you can set these sub-attributes directly. For example,
            if your app separately controls the data on the x and y
            axes you might set `xaxis.uirevision=*time*` and
            `yaxis.uirevision=*cost*`. Then if only the y data is
            changed, you can update `yaxis.uirevision=*quantity*`
            and the y axis range will reset but the x axis range
            will retain any user-driven zoom.
        uniformtext
            :class:`plotly.graph_objects.layout.Uniformtext`
            instance or dict with compatible properties
        updatemenus
            A tuple of
            :class:`plotly.graph_objects.layout.Updatemenu`
            instances or dicts with compatible properties
        updatemenudefaults
            When used in a template (as
            layout.template.layout.updatemenudefaults), sets the
            default property values to use for elements of
            layout.updatemenus
        violingap
            Sets the gap (in plot fraction) between violins of
            adjacent location coordinates. Has no effect on traces
            that have "width" set.
        violingroupgap
            Sets the gap (in plot fraction) between violins of the
            same location coordinate. Has no effect on traces that
            have "width" set.
        violinmode
            Determines how violins at the same location coordinate
            are displayed on the graph. If "group", the violins are
            plotted next to one another centered around the shared
            location. If "overlay", the violins are plotted over
            one another, you might need to set "opacity" to see
            them multiple violins. Has no effect on traces that
            have "width" set.
        waterfallgap
            Sets the gap (in plot fraction) between bars of
            adjacent location coordinates.
        waterfallgroupgap
            Sets the gap (in plot fraction) between bars of the
            same location coordinate.
        waterfallmode
            Determines how bars at the same location coordinate are
            displayed on the graph. With "group", the bars are
            plotted next to one another centered around the shared
            location. With "overlay", the bars are plotted over one
            another, you might need to reduce "opacity" to see
            multiple bars.
        width
            Sets the plot's width (in px).
        xaxis
            :class:`plotly.graph_objects.layout.XAxis` instance or
            dict with compatible properties
        yaxis
            :class:`plotly.graph_objects.layout.YAxis` instance or
            dict with compatible properties
        
Did you mean "xaxis"?

Bad property path:
axis_title_size
^^^^
2025-05-29 16:14:19,333 [ERROR] app:676: Traceback (most recent call last):
  File "C:\Users\Devansh\Downloads\intern_application_round1\app.py", line 638, in search_mentions
    temporal_time_fig = self.visualizer.create_temporal_time_distribution(cached_metrics)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Devansh\Downloads\intern_application_round1\ui\visualization.py", line 915, in create_temporal_time_distribution
    fig.update_layout(
  File "C:\Users\Devansh\AppData\Roaming\Python\Python312\site-packages\plotly\graph_objs\_figure.py", line 787, in update_layout
    return super(Figure, self).update_layout(dict1, overwrite, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Devansh\AppData\Roaming\Python\Python312\site-packages\plotly\basedatatypes.py", line 1392, in update_layout
    self.layout.update(dict1, overwrite=overwrite, **kwargs)
  File "C:\Users\Devansh\AppData\Roaming\Python\Python312\site-packages\plotly\basedatatypes.py", line 5123, in update
    BaseFigure._perform_update(self, kwargs, overwrite=overwrite)
  File "C:\Users\Devansh\AppData\Roaming\Python\Python312\site-packages\plotly\basedatatypes.py", line 3882, in _perform_update
    raise err
ValueError: Invalid property specified for object of type plotly.graph_objs.Layout: 'axis'

Did you mean "xaxis"?

    Valid properties:
        activeselection
            :class:`plotly.graph_objects.layout.Activeselection`
            instance or dict with compatible properties
        activeshape
            :class:`plotly.graph_objects.layout.Activeshape`
            instance or dict with compatible properties
        annotations
            A tuple of
            :class:`plotly.graph_objects.layout.Annotation`
            instances or dicts with compatible properties
        annotationdefaults
            When used in a template (as
            layout.template.layout.annotationdefaults), sets the
            default property values to use for elements of
            layout.annotations
        autosize
            Determines whether or not a layout width or height that
            has been left undefined by the user is initialized on
            each relayout. Note that, regardless of this attribute,
            an undefined layout width or height is always
            initialized on the first call to plot.
        autotypenumbers
            Using "strict" a numeric string in trace data is not
            converted to a number. Using *convert types* a numeric
            string in trace data may be treated as a number during
            automatic axis `type` detection. This is the default
            value; however it could be overridden for individual
            axes.
        barcornerradius
            Sets the rounding of bar corners. May be an integer
            number of pixels, or a percentage of bar width (as a
            string ending in %).
        bargap
            Sets the gap (in plot fraction) between bars of
            adjacent location coordinates.
        bargroupgap
            Sets the gap (in plot fraction) between bars of the
            same location coordinate.
        barmode
            Determines how bars at the same location coordinate are
            displayed on the graph. With "stack", the bars are
            stacked on top of one another With "relative", the bars
            are stacked on top of one another, with negative values
            below the axis, positive values above With "group", the
            bars are plotted next to one another centered around
            the shared location. With "overlay", the bars are
            plotted over one another, you might need to reduce
            "opacity" to see multiple bars.
        barnorm
            Sets the normalization for bar traces on the graph.
            With "fraction", the value of each bar is divided by
            the sum of all values at that location coordinate.
            "percent" is the same but multiplied by 100 to show
            percentages.
        boxgap
            Sets the gap (in plot fraction) between boxes of
            adjacent location coordinates. Has no effect on traces
            that have "width" set.
        boxgroupgap
            Sets the gap (in plot fraction) between boxes of the
            same location coordinate. Has no effect on traces that
            have "width" set.
        boxmode
            Determines how boxes at the same location coordinate
            are displayed on the graph. If "group", the boxes are
            plotted next to one another centered around the shared
            location. If "overlay", the boxes are plotted over one
            another, you might need to set "opacity" to see them
            multiple boxes. Has no effect on traces that have
            "width" set.
        calendar
            Sets the default calendar system to use for
            interpreting and displaying dates throughout the plot.
        clickmode
            Determines the mode of single click interactions.
            "event" is the default value and emits the
            `plotly_click` event. In addition this mode emits the
            `plotly_selected` event in drag modes "lasso" and
            "select", but with no event data attached (kept for
            compatibility reasons). The "select" flag enables
            selecting single data points via click. This mode also
            supports persistent selections, meaning that pressing
            Shift while clicking, adds to / subtracts from an
            existing selection. "select" with `hovermode`: "x" can
            be confusing, consider explicitly setting `hovermode`:
            "closest" when using this feature. Selection events are
            sent accordingly as long as "event" flag is set as
            well. When the "event" flag is missing, `plotly_click`
            and `plotly_selected` events are not fired.
        coloraxis
            :class:`plotly.graph_objects.layout.Coloraxis` instance
            or dict with compatible properties
        colorscale
            :class:`plotly.graph_objects.layout.Colorscale`
            instance or dict with compatible properties
        colorway
            Sets the default trace colors.
        computed
            Placeholder for exporting automargin-impacting values
            namely `margin.t`, `margin.b`, `margin.l` and
            `margin.r` in "full-json" mode.
        datarevision
            If provided, a changed value tells `Plotly.react` that
            one or more data arrays has changed. This way you can
            modify arrays in-place rather than making a complete
            new copy for an incremental change. If NOT provided,
            `Plotly.react` assumes that data arrays are being
            treated as immutable, thus any data array with a
            different identity from its predecessor contains new
            data.
        dragmode
            Determines the mode of drag interactions. "select" and
            "lasso" apply only to scatter traces with markers or
            text. "orbit" and "turntable" apply only to 3D scenes.
        editrevision
            Controls persistence of user-driven changes in
            `editable: true` configuration, other than trace names
            and axis titles. Defaults to `layout.uirevision`.
        extendfunnelareacolors
            If `true`, the funnelarea slice colors (whether given
            by `funnelareacolorway` or inherited from `colorway`)
            will be extended to three times its original length by
            first repeating every color 20% lighter then each color
            20% darker. This is intended to reduce the likelihood
            of reusing the same color when you have many slices,
            but you can set `false` to disable. Colors provided in
            the trace, using `marker.colors`, are never extended.
        extendiciclecolors
            If `true`, the icicle slice colors (whether given by
            `iciclecolorway` or inherited from `colorway`) will be
            extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        extendpiecolors
            If `true`, the pie slice colors (whether given by
            `piecolorway` or inherited from `colorway`) will be
            extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        extendsunburstcolors
            If `true`, the sunburst slice colors (whether given by
            `sunburstcolorway` or inherited from `colorway`) will
            be extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        extendtreemapcolors
            If `true`, the treemap slice colors (whether given by
            `treemapcolorway` or inherited from `colorway`) will be
            extended to three times its original length by first
            repeating every color 20% lighter then each color 20%
            darker. This is intended to reduce the likelihood of
            reusing the same color when you have many slices, but
            you can set `false` to disable. Colors provided in the
            trace, using `marker.colors`, are never extended.
        font
            Sets the global font. Note that fonts used in traces
            and other layout components inherit from the global
            font.
        funnelareacolorway
            Sets the default funnelarea slice colors. Defaults to
            the main `colorway` used for trace colors. If you
            specify a new list here it can still be extended with
            lighter and darker colors, see
            `extendfunnelareacolors`.
        funnelgap
            Sets the gap (in plot fraction) between bars of
            adjacent location coordinates.
        funnelgroupgap
            Sets the gap (in plot fraction) between bars of the
            same location coordinate.
        funnelmode
            Determines how bars at the same location coordinate are
            displayed on the graph. With "stack", the bars are
            stacked on top of one another With "group", the bars
            are plotted next to one another centered around the
            shared location. With "overlay", the bars are plotted
            over one another, you might need to reduce "opacity" to
            see multiple bars.
        geo
            :class:`plotly.graph_objects.layout.Geo` instance or
            dict with compatible properties
        grid
            :class:`plotly.graph_objects.layout.Grid` instance or
            dict with compatible properties
        height
            Sets the plot's height (in px).
        hiddenlabels
            hiddenlabels is the funnelarea & pie chart analog of
            visible:'legendonly' but it can contain many labels,
            and can simultaneously hide slices from several
            pies/funnelarea charts
        hiddenlabelssrc
            Sets the source reference on Chart Studio Cloud for
            `hiddenlabels`.
        hidesources
            Determines whether or not a text link citing the data
            source is placed at the bottom-right cored of the
            figure. Has only an effect only on graphs that have
            been generated via forked graphs from the Chart Studio
            Cloud (at https://chart-studio.plotly.com or on-
            premise).
        hoverdistance
            Sets the default distance (in pixels) to look for data
            to add hover labels (-1 means no cutoff, 0 means no
            looking for data). This is only a real distance for
            hovering on point-like objects, like scatter points.
            For area-like objects (bars, scatter fills, etc)
            hovering is on inside the area and off outside, but
            these objects will not supersede hover on point-like
            objects in case of conflict.
        hoverlabel
            :class:`plotly.graph_objects.layout.Hoverlabel`
            instance or dict with compatible properties
        hovermode
            Determines the mode of hover interactions. If
            "closest", a single hoverlabel will appear for the
            "closest" point within the `hoverdistance`. If "x" (or
            "y"), multiple hoverlabels will appear for multiple
            points at the "closest" x- (or y-) coordinate within
            the `hoverdistance`, with the caveat that no more than
            one hoverlabel will appear per trace. If *x unified*
            (or *y unified*), a single hoverlabel will appear
            multiple points at the closest x- (or y-) coordinate
            within the `hoverdistance` with the caveat that no more
            than one hoverlabel will appear per trace. In this
            mode, spikelines are enabled by default perpendicular
            to the specified axis. If false, hover interactions are
            disabled.
        hoversubplots
            Determines expansion of hover effects to other subplots
            If "single" just the axis pair of the primary point is
            included without overlaying subplots. If "overlaying"
            all subplots using the main axis and occupying the same
            space are included. If "axis", also include stacked
            subplots using the same axis when `hovermode` is set to
            "x", *x unified*, "y" or *y unified*.
        iciclecolorway
            Sets the default icicle slice colors. Defaults to the
            main `colorway` used for trace colors. If you specify a
            new list here it can still be extended with lighter and
            darker colors, see `extendiciclecolors`.
        images
            A tuple of :class:`plotly.graph_objects.layout.Image`
            instances or dicts with compatible properties
        imagedefaults
            When used in a template (as
            layout.template.layout.imagedefaults), sets the default
            property values to use for elements of layout.images
        legend
            :class:`plotly.graph_objects.layout.Legend` instance or
            dict with compatible properties
        map
            :class:`plotly.graph_objects.layout.Map` instance or
            dict with compatible properties
        mapbox
            :class:`plotly.graph_objects.layout.Mapbox` instance or
            dict with compatible properties
        margin
            :class:`plotly.graph_objects.layout.Margin` instance or
            dict with compatible properties
        meta
            Assigns extra meta information that can be used in
            various `text` attributes. Attributes such as the
            graph, axis and colorbar `title.text`, annotation
            `text` `trace.name` in legend items, `rangeselector`,
            `updatemenus` and `sliders` `label` text all support
            `meta`. One can access `meta` fields using template
            strings: `%{meta[i]}` where `i` is the index of the
            `meta` item in question. `meta` can also be an object
            for example `{key: value}` which can be accessed
            %{meta[key]}.
        metasrc
            Sets the source reference on Chart Studio Cloud for
            `meta`.
        minreducedheight
            Minimum height of the plot with margin.automargin
            applied (in px)
        minreducedwidth
            Minimum width of the plot with margin.automargin
            applied (in px)
        modebar
            :class:`plotly.graph_objects.layout.Modebar` instance
            or dict with compatible properties
        newselection
            :class:`plotly.graph_objects.layout.Newselection`
            instance or dict with compatible properties
        newshape
            :class:`plotly.graph_objects.layout.Newshape` instance
            or dict with compatible properties
        paper_bgcolor
            Sets the background color of the paper where the graph
            is drawn.
        piecolorway
            Sets the default pie slice colors. Defaults to the main
            `colorway` used for trace colors. If you specify a new
            list here it can still be extended with lighter and
            darker colors, see `extendpiecolors`.
        plot_bgcolor
            Sets the background color of the plotting area in-
            between x and y axes.
        polar
            :class:`plotly.graph_objects.layout.Polar` instance or
            dict with compatible properties
        scattergap
            Sets the gap (in plot fraction) between scatter points
            of adjacent location coordinates. Defaults to `bargap`.
        scattermode
            Determines how scatter points at the same location
            coordinate are displayed on the graph. With "group",
            the scatter points are plotted next to one another
            centered around the shared location. With "overlay",
            the scatter points are plotted over one another, you
            might need to reduce "opacity" to see multiple scatter
            points.
        scene
            :class:`plotly.graph_objects.layout.Scene` instance or
            dict with compatible properties
        selectdirection
            When `dragmode` is set to "select", this limits the
            selection of the drag to horizontal, vertical or
            diagonal. "h" only allows horizontal selection, "v"
            only vertical, "d" only diagonal and "any" sets no
            limit.
        selectionrevision
            Controls persistence of user-driven changes in selected
            points from all traces.
        selections
            A tuple of
            :class:`plotly.graph_objects.layout.Selection`
            instances or dicts with compatible properties
        selectiondefaults
            When used in a template (as
            layout.template.layout.selectiondefaults), sets the
            default property values to use for elements of
            layout.selections
        separators
            Sets the decimal and thousand separators. For example,
            *. * puts a '.' before decimals and a space between
            thousands. In English locales, dflt is ".," but other
            locales may alter this default.
        shapes
            A tuple of :class:`plotly.graph_objects.layout.Shape`
            instances or dicts with compatible properties
        shapedefaults
            When used in a template (as
            layout.template.layout.shapedefaults), sets the default
            property values to use for elements of layout.shapes
        showlegend
            Determines whether or not a legend is drawn. Default is
            `true` if there is a trace to show and any of these: a)
            Two or more traces would by default be shown in the
            legend. b) One pie trace is shown in the legend. c) One
            trace is explicitly given with `showlegend: true`.
        sliders
            A tuple of :class:`plotly.graph_objects.layout.Slider`
            instances or dicts with compatible properties
        sliderdefaults
            When used in a template (as
            layout.template.layout.sliderdefaults), sets the
            default property values to use for elements of
            layout.sliders
        smith
            :class:`plotly.graph_objects.layout.Smith` instance or
            dict with compatible properties
        spikedistance
            Sets the default distance (in pixels) to look for data
            to draw spikelines to (-1 means no cutoff, 0 means no
            looking for data). As with hoverdistance, distance does
            not apply to area-like objects. In addition, some
            objects can be hovered on but will not generate
            spikelines, such as scatter fills.
        sunburstcolorway
            Sets the default sunburst slice colors. Defaults to the
            main `colorway` used for trace colors. If you specify a
            new list here it can still be extended with lighter and
            darker colors, see `extendsunburstcolors`.
        template
            Default attributes to be applied to the plot. This
            should be a dict with format: `{'layout':
            layoutTemplate, 'data': {trace_type: [traceTemplate,
            ...], ...}}` where `layoutTemplate` is a dict matching
            the structure of `figure.layout` and `traceTemplate` is
            a dict matching the structure of the trace with type
            `trace_type` (e.g. 'scatter'). Alternatively, this may
            be specified as an instance of
            plotly.graph_objs.layout.Template.  Trace templates are
            applied cyclically to traces of each type. Container
            arrays (eg `annotations`) have special handling: An
            object ending in `defaults` (eg `annotationdefaults`)
            is applied to each array item. But if an item has a
            `templateitemname` key we look in the template array
            for an item with matching `name` and apply that
            instead. If no matching `name` is found we mark the
            item invisible. Any named template item not referenced
            is appended to the end of the array, so this can be
            used to add a watermark annotation or a logo image, for
            example. To omit one of these items on the plot, make
            an item with matching `templateitemname` and `visible:
            false`.
        ternary
            :class:`plotly.graph_objects.layout.Ternary` instance
            or dict with compatible properties
        title
            :class:`plotly.graph_objects.layout.Title` instance or
            dict with compatible properties
        transition
            Sets transition options used during Plotly.react
            updates.
        treemapcolorway
            Sets the default treemap slice colors. Defaults to the
            main `colorway` used for trace colors. If you specify a
            new list here it can still be extended with lighter and
            darker colors, see `extendtreemapcolors`.
        uirevision
            Used to allow user interactions with the plot to
            persist after `Plotly.react` calls that are unaware of
            these interactions. If `uirevision` is omitted, or if
            it is given and it changed from the previous
            `Plotly.react` call, the exact new figure is used. If
            `uirevision` is truthy and did NOT change, any
            attribute that has been affected by user interactions
            and did not receive a different value in the new figure
            will keep the interaction value. `layout.uirevision`
            attribute serves as the default for `uirevision`
            attributes in various sub-containers. For finer control
            you can set these sub-attributes directly. For example,
            if your app separately controls the data on the x and y
            axes you might set `xaxis.uirevision=*time*` and
            `yaxis.uirevision=*cost*`. Then if only the y data is
            changed, you can update `yaxis.uirevision=*quantity*`
            and the y axis range will reset but the x axis range
            will retain any user-driven zoom.
        uniformtext
            :class:`plotly.graph_objects.layout.Uniformtext`
            instance or dict with compatible properties
        updatemenus
            A tuple of
            :class:`plotly.graph_objects.layout.Updatemenu`
            instances or dicts with compatible properties
        updatemenudefaults
            When used in a template (as
            layout.template.layout.updatemenudefaults), sets the
            default property values to use for elements of
            layout.updatemenus
        violingap
            Sets the gap (in plot fraction) between violins of
            adjacent location coordinates. Has no effect on traces
            that have "width" set.
        violingroupgap
            Sets the gap (in plot fraction) between violins of the
            same location coordinate. Has no effect on traces that
            have "width" set.
        violinmode
            Determines how violins at the same location coordinate
            are displayed on the graph. If "group", the violins are
            plotted next to one another centered around the shared
            location. If "overlay", the violins are plotted over
            one another, you might need to set "opacity" to see
            them multiple violins. Has no effect on traces that
            have "width" set.
        waterfallgap
            Sets the gap (in plot fraction) between bars of
            adjacent location coordinates.
        waterfallgroupgap
            Sets the gap (in plot fraction) between bars of the
            same location coordinate.
        waterfallmode
            Determines how bars at the same location coordinate are
            displayed on the graph. With "group", the bars are
            plotted next to one another centered around the shared
            location. With "overlay", the bars are plotted over one
            another, you might need to reduce "opacity" to see
            multiple bars.
        width
            Sets the plot's width (in px).
        xaxis
            :class:`plotly.graph_objects.layout.XAxis` instance or
            dict with compatible properties
        yaxis
            :class:`plotly.graph_objects.layout.YAxis` instance or
            dict with compatible properties
        
Did you mean "xaxis"?

Bad property path:
axis_title_size
^^^^

2025-05-29 16:14:19,372 [INFO] scraper.reddit_scraper:629: Found 332 cached mentions for 'openai'
2025-05-29 16:14:19,372 [INFO] analytics.data_validator:236: Validating dataset of 332 mentions
2025-05-29 16:14:23,140 [INFO] analytics.data_validator:259: Validation complete: 332/332 mentions passed
2025-05-29 16:14:23,140 [INFO] app:742: Data validation: 332/332 mentions passed
2025-05-29 16:14:23,348 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,349 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,350 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,350 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,351 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,352 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,352 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,353 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,353 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,354 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,354 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,355 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,355 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,356 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,356 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,357 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,358 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,358 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,359 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,359 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,360 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,360 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,361 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,362 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,362 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,363 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,363 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,364 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,364 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,364 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,365 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,365 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,366 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,367 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,367 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,368 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,368 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,369 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,369 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,370 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,371 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,371 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,371 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,372 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,372 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,373 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,373 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,374 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,374 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,375 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,375 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,376 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,376 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,377 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,377 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,378 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,379 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,379 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,380 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,380 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,381 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,381 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,382 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,382 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,383 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,383 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,384 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,385 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,385 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,385 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,386 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,386 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,387 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,387 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,388 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,389 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,389 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,394 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,399 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,399 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,400 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,401 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,401 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,402 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,402 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,403 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,403 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,404 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,404 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,405 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,405 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,406 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,406 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,407 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,407 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,408 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,408 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,408 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,409 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,409 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,410 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,410 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,411 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,411 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,412 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,413 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,413 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,414 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,414 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,415 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,415 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,416 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,416 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,417 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,417 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,418 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,418 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,419 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,419 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,420 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,420 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,421 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,421 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,422 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,422 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,423 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,423 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,424 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,424 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,425 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,425 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,426 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,426 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,426 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,427 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,427 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,428 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,428 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,429 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,430 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,430 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,431 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,433 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,434 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,434 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,435 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,435 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,436 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,436 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,437 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,437 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,438 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,438 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,439 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,440 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,440 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,440 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,441 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,442 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,442 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,442 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,443 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,443 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,444 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,444 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,445 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,446 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,446 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,447 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,450 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,450 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,451 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,451 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,452 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,452 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,453 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,453 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,454 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,454 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,455 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,455 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,458 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,458 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,459 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,462 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,463 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,463 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,464 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,464 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,465 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,465 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,466 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,466 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,467 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,467 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,468 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,468 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,469 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,469 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,470 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,473 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,473 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,474 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,477 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,477 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,478 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,479 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,479 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,480 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,480 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,483 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,484 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,484 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,485 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,485 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,486 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,486 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,487 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,488 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,488 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,489 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,489 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,490 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,490 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,491 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,491 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,492 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,492 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,493 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,493 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,496 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,502 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,502 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,503 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,503 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,504 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,504 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,505 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,505 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,506 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,506 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,507 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,507 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,508 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,508 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,509 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,509 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,510 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,511 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,511 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,511 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,512 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,513 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,513 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,514 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,514 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,515 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,515 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,516 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,516 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,519 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,519 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,521 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,521 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,522 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,522 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,523 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,523 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,524 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,524 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,525 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,525 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,526 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,526 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,527 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,527 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,528 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,528 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,529 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,529 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,530 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,530 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,531 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,531 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,532 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,533 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,533 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,534 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,536 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,537 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,537 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,538 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,538 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,539 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,539 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,540 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,540 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,541 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,541 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,541 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,542 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,545 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,546 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,546 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,546 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,550 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,550 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,551 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,559 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,559 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,560 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,560 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:14:23,592 [INFO] app:846: Search completed: openai -> 332 mentions
2025-05-29 16:14:23,650 [ERROR] app:2863: Error getting top mentions: 'datetime.datetime' object cannot be interpreted as an integer
2025-05-29 16:15:08,894 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:16:15,009 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:16:58,076 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.64 (avg: 93.64, std: 0.00)
2025-05-29 16:16:58,076 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.22 (avg: 121.23, std: 0.00)
2025-05-29 16:17:21,124 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:19:19,245 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:19:19,267 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:19:19,267 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 16:19:19,267 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 16:19:19,268 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:19:19,269 [INFO] app:378: Database initialized successfully
2025-05-29 16:19:19,272 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:19:19,273 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:19:19,827 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:19:20,281 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 16:19:20,709 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:19:21,289 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:19:21,823 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 16:19:23,874 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 16:20:04,807 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:20:04,836 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:20:04,836 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 16:20:04,836 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 16:20:04,837 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:20:04,838 [INFO] app:378: Database initialized successfully
2025-05-29 16:20:04,840 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:20:04,841 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:20:05,050 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/gradio-messaging/en "HTTP/1.1 200 OK"
2025-05-29 16:20:05,364 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:20:05,852 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 16:20:06,220 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:20:06,860 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:20:07,422 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 16:20:09,488 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 16:20:16,872 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 16:20:17,760 [INFO] app:591: Cache hit for search term: openai
2025-05-29 16:20:17,877 [ERROR] app:674: Error generating metrics from cached data: plotly.graph_objs._figure.Figure.update_layout() got multiple values for keyword argument 'title'
2025-05-29 16:20:17,877 [ERROR] app:676: Traceback (most recent call last):
  File "C:\Users\Devansh\Downloads\intern_application_round1\app.py", line 638, in search_mentions
    temporal_time_fig = self.visualizer.create_temporal_time_distribution(cached_metrics)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Devansh\Downloads\intern_application_round1\ui\visualization.py", line 928, in create_temporal_time_distribution
    fig.update_layout(
TypeError: plotly.graph_objs._figure.Figure.update_layout() got multiple values for keyword argument 'title'

2025-05-29 16:20:17,890 [INFO] scraper.reddit_scraper:629: Found 332 cached mentions for 'openai'
2025-05-29 16:20:17,890 [INFO] analytics.data_validator:236: Validating dataset of 332 mentions
2025-05-29 16:20:21,623 [INFO] analytics.data_validator:259: Validation complete: 332/332 mentions passed
2025-05-29 16:20:21,624 [INFO] app:742: Data validation: 332/332 mentions passed
2025-05-29 16:20:21,837 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,837 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,838 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,839 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,839 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,840 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,840 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,841 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,841 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,842 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,843 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,843 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,844 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,844 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,845 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,845 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,846 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,847 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,847 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,848 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,848 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,849 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,850 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,850 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,851 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,851 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,852 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,852 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,853 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,853 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,853 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,854 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,854 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,855 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,855 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,856 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,856 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,857 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,857 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,858 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,858 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,859 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,859 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,860 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,861 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,861 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,862 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,862 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,863 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,863 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,864 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,864 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,865 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,865 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,866 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,866 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,867 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,867 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,868 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,868 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,869 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,869 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,870 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,870 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,871 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,871 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,872 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,872 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,873 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,874 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,874 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,875 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,875 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,876 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,876 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,877 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,877 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,882 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,887 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,887 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,888 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,888 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,889 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,889 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,890 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,891 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,891 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,891 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,892 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,892 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,893 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,894 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,894 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,895 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,895 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,896 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,896 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,897 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,897 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,898 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,898 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,899 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,899 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,900 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,900 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,901 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,901 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,902 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,902 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,903 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,904 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,904 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,905 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,905 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,906 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,906 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,907 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,907 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,908 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,908 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,909 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,909 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,910 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,911 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,911 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,912 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,912 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,913 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,913 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,914 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,914 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,915 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,915 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,916 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,916 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,917 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,917 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,918 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,918 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,919 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,920 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,920 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,923 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,924 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,924 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,925 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,925 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,926 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,926 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,927 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,928 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,928 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,929 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,929 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,930 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,930 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,931 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,931 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,932 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,932 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,933 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,933 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,934 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,934 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,935 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,935 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,936 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,936 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,937 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,940 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,940 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,941 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,941 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,942 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,942 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,943 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,944 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,944 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,945 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,945 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,946 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,949 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,949 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,950 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,953 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,953 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,954 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,954 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,955 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,955 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,956 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,956 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,957 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,957 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,958 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,958 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,959 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,960 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,960 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,961 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,964 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,964 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,965 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,972 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,973 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,974 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,974 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,975 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,975 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,976 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,979 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,980 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,980 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,981 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,982 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,982 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,982 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,983 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,983 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,984 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,984 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,985 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,985 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,986 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,986 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,987 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,987 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,988 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,988 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,989 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,992 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,997 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,998 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,998 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,999 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:21,999 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,000 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,000 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,001 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,001 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,002 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,002 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,003 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,003 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,004 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,004 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,005 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,005 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,006 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,006 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,007 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,007 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,008 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,008 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,009 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,010 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,010 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,011 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,011 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,012 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,015 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,016 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,016 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,017 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,017 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,018 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,018 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,019 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,019 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,020 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,020 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,021 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,021 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,022 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,022 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,023 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,023 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,024 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,024 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,024 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,025 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,025 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,026 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,027 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,027 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,028 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,028 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,029 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,032 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,033 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,033 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,034 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,034 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,035 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,035 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,035 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,036 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,037 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,037 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,038 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,038 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,041 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,042 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,042 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,043 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,046 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,047 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,047 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,055 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,056 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,057 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,057 [ERROR] app:790: Failed to save mention: 'quality_score' is an invalid keyword argument for RedditMention
2025-05-29 16:20:22,090 [INFO] app:846: Search completed: openai -> 332 mentions
2025-05-29 16:21:12,986 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:24:38,034 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:24:38,056 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:24:38,056 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 16:24:38,056 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 16:24:38,057 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:24:38,058 [INFO] app:378: Database initialized successfully
2025-05-29 16:24:38,061 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:24:38,062 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:24:38,544 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:24:39,073 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 16:24:39,538 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:24:40,080 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:24:40,636 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 16:24:42,705 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 16:26:07,073 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:26:07,103 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:26:07,103 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 16:26:07,103 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 16:26:07,104 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:26:07,105 [INFO] app:378: Database initialized successfully
2025-05-29 16:26:07,108 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:26:07,110 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:26:07,738 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:26:08,125 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 16:26:08,593 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:26:09,134 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:26:09,832 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 16:26:11,876 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 16:26:23,158 [INFO] scraper.reddit_scraper:645: Attempting Reddit API search for: chatgpt
2025-05-29 16:26:23,158 [INFO] scraper.reddit_scraper.APIClient:172: [SEARCH] Starting COMPREHENSIVE search for 'chatgpt' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 16:26:23,159 [INFO] scraper.reddit_scraper.APIClient:187: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 16:26:23,159 [INFO] scraper.reddit_scraper.APIClient:191:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 16:26:25,583 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 95 posts with sort=relevance, time=week. New unique: 95. Total unique: 95
2025-05-29 16:26:25,583 [INFO] scraper.reddit_scraper.APIClient:191:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 16:26:27,810 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 98 posts with sort=top, time=week. New unique: 67. Total unique: 162
2025-05-29 16:26:27,810 [INFO] scraper.reddit_scraper.APIClient:191:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 16:26:29,999 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 100 posts with sort=hot, time=week. New unique: 92. Total unique: 254
2025-05-29 16:26:29,999 [INFO] scraper.reddit_scraper.APIClient:191:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 16:26:32,297 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 98. Total unique: 352
2025-05-29 16:26:32,297 [INFO] scraper.reddit_scraper.APIClient:191:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 16:26:34,653 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 95 posts with sort=relevance, time=week. New unique: 0. Total unique: 352
2025-05-29 16:26:34,653 [INFO] scraper.reddit_scraper.APIClient:191:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 16:26:38,167 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 352
2025-05-29 16:26:38,167 [INFO] scraper.reddit_scraper.APIClient:218: [TARGET] Need 648 more results, searching popular subreddits...
2025-05-29 16:26:38,623 [INFO] scraper.reddit_scraper.APIClient:383: Found 8 posts in r/technology. Total additional: 7
2025-05-29 16:26:39,810 [INFO] scraper.reddit_scraper.APIClient:383: Found 3 posts in r/programming. Total additional: 10
2025-05-29 16:26:41,155 [INFO] scraper.reddit_scraper.APIClient:383: Found 5 posts in r/MachineLearning. Total additional: 15
2025-05-29 16:26:44,307 [INFO] scraper.reddit_scraper.APIClient:383: Found 17 posts in r/artificial. Total additional: 32
2025-05-29 16:26:44,750 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/gadgets. Total additional: 32
2025-05-29 16:26:45,828 [INFO] scraper.reddit_scraper.APIClient:383: Found 2 posts in r/science. Total additional: 33
2025-05-29 16:26:47,100 [INFO] scraper.reddit_scraper.APIClient:383: Found 25 posts in r/askreddit. Total additional: 58
2025-05-29 16:26:47,994 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/explainlikeimfive. Total additional: 59
2025-05-29 16:26:49,097 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/news. Total additional: 59
2025-05-29 16:26:50,171 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/worldnews. Total additional: 59
2025-05-29 16:26:51,300 [INFO] scraper.reddit_scraper.APIClient:383: Found 3 posts in r/business. Total additional: 62
2025-05-29 16:26:52,485 [INFO] scraper.reddit_scraper.APIClient:383: Found 10 posts in r/entrepreneur. Total additional: 72
2025-05-29 16:26:53,518 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/startup. Total additional: 73
2025-05-29 16:26:54,654 [INFO] scraper.reddit_scraper.APIClient:383: Found 6 posts in r/investing. Total additional: 79
2025-05-29 16:26:55,719 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/datascience. Total additional: 80
2025-05-29 16:26:55,720 [INFO] scraper.reddit_scraper.APIClient:222:    [SUBREDDIT] Added 80 from subreddit search. Total: 432
2025-05-29 16:26:55,720 [INFO] scraper.reddit_scraper.APIClient:227: [VARIANTS] Need 568 more results, trying query variations...
2025-05-29 16:26:55,720 [INFO] scraper.reddit_scraper.APIClient:238:    [1/4] Trying variant: "chatgpt"
2025-05-29 16:26:57,967 [INFO] scraper.reddit_scraper.APIClient:251:    [VARIANT] Variant '"chatgpt"' added 0 new posts. Total: 432
2025-05-29 16:26:57,967 [INFO] scraper.reddit_scraper.APIClient:238:    [2/4] Trying variant: chatgpt OR chatgpt
2025-05-29 16:26:59,105 [INFO] scraper.reddit_scraper.APIClient:251:    [VARIANT] Variant 'chatgpt OR chatgpt' added 0 new posts. Total: 432
2025-05-29 16:26:59,105 [INFO] scraper.reddit_scraper.APIClient:238:    [3/4] Trying variant: title:chatgpt
2025-05-29 16:27:00,455 [INFO] scraper.reddit_scraper.APIClient:251:    [VARIANT] Variant 'title:chatgpt' added 0 new posts. Total: 432
2025-05-29 16:27:00,456 [INFO] scraper.reddit_scraper.APIClient:238:    [4/4] Trying variant: selftext:chatgpt
2025-05-29 16:27:02,036 [INFO] scraper.reddit_scraper.APIClient:251:    [VARIANT] Variant 'selftext:chatgpt' added 14 new posts. Total: 446
2025-05-29 16:27:02,036 [INFO] scraper.reddit_scraper.APIClient:263: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'chatgpt':
[STATS] Total unique posts found: 446
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 232
[RESULT] SUCCESS: Found 446 posts that will be processed and saved

2025-05-29 16:27:02,036 [INFO] scraper.reddit_scraper:654: Reddit comprehensive API returned 446 mentions
2025-05-29 16:27:02,037 [INFO] scraper.reddit_scraper:1000: Post-processing 446 mentions...
2025-05-29 16:27:02,037 [INFO] scraper.reddit_scraper:1446: Deduplication: 446 -> 446 mentions (removed 0 exact duplicates)
2025-05-29 16:27:02,037 [INFO] scraper.reddit_scraper:1004: After deduplication: 446 mentions
2025-05-29 16:27:02,045 [INFO] scraper.reddit_scraper:1008: After validation: 446 mentions
2025-05-29 16:27:02,462 [INFO] scraper.reddit_scraper:1025: After database mapping: 446 mentions (ALL KEPT - no quality filtering)
2025-05-29 16:27:02,463 [INFO] scraper.reddit_scraper:1045: Final result: 446 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 16:27:02,465 [INFO] analytics.data_validator:236: Validating dataset of 446 mentions
2025-05-29 16:27:07,265 [INFO] analytics.data_validator:259: Validation complete: 446/446 mentions passed
2025-05-29 16:27:07,265 [INFO] app:742: Data validation: 446/446 mentions passed
2025-05-29 16:27:09,009 [INFO] app:846: Search completed: chatgpt -> 446 mentions
2025-05-29 16:27:15,290 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:27:58,357 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in process_count: 388.00 (avg: 388.91, std: 0.30)
2025-05-29 16:28:09,385 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in process_count: 386.00 (avg: 388.67, std: 0.89)
2025-05-29 16:28:21,422 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:28:42,452 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 16:28:42,453 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.64 (avg: 93.65, std: 0.00)
2025-05-29 16:28:42,453 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.22 (avg: 121.18, std: 0.01)
2025-05-29 16:28:42,453 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in process_count: 380.00 (avg: 387.73, std: 2.46)
2025-05-29 16:29:27,526 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:32:13,700 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:32:13,723 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:32:13,724 [INFO] scraper.reddit_scraper:489: Cache manager enabled
2025-05-29 16:32:13,724 [INFO] scraper.reddit_scraper:499: Real-time monitoring enabled
2025-05-29 16:32:13,724 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:32:13,726 [INFO] app:378: Database initialized successfully
2025-05-29 16:32:13,728 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:32:13,729 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:32:14,349 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:32:14,743 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.64 (threshold: 85)
2025-05-29 16:32:15,196 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:32:15,750 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:32:16,331 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 16:32:18,389 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 16:32:36,795 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 16:32:39,902 [INFO] app:591: Cache hit for search term: openai
2025-05-29 16:32:40,066 [ERROR] app:674: Error generating metrics from cached data: plotly.graph_objs._figure.Figure.update_layout() got multiple values for keyword argument 'xaxis'
2025-05-29 16:32:40,067 [ERROR] app:676: Traceback (most recent call last):
  File "C:\Users\Devansh\Downloads\intern_application_round1\app.py", line 640, in search_mentions
    temporal_hourly_fig = self.visualizer.create_temporal_hourly_distribution(cached_metrics)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Devansh\Downloads\intern_application_round1\ui\visualization.py", line 1002, in create_temporal_hourly_distribution
    fig.update_layout(
TypeError: plotly.graph_objs._figure.Figure.update_layout() got multiple values for keyword argument 'xaxis'

2025-05-29 16:32:40,075 [INFO] scraper.reddit_scraper:629: Found 332 cached mentions for 'openai'
2025-05-29 16:32:40,076 [INFO] scraper.reddit_scraper:1651: Cleaned 332 cached mentions -> 332 valid mentions
2025-05-29 16:32:40,076 [INFO] analytics.data_validator:236: Validating dataset of 332 mentions
2025-05-29 16:32:43,837 [INFO] analytics.data_validator:259: Validation complete: 332/332 mentions passed
2025-05-29 16:32:43,837 [INFO] app:742: Data validation: 332/332 mentions passed
2025-05-29 16:32:44,933 [INFO] app:846: Search completed: openai -> 332 mentions
2025-05-29 16:32:44,966 [ERROR] app:1413: Visualization generation failed: plotly.graph_objs._figure.Figure.update_layout() got multiple values for keyword argument 'xaxis'
2025-05-29 16:33:21,872 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:34:27,984 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:34:55,994 [INFO] scraper.reddit_scraper:645: Attempting Reddit API search for: openai
2025-05-29 16:34:55,994 [INFO] scraper.reddit_scraper.APIClient:172: [SEARCH] Starting COMPREHENSIVE search for 'openai' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 16:34:55,994 [INFO] scraper.reddit_scraper.APIClient:187: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 16:34:55,995 [INFO] scraper.reddit_scraper.APIClient:191:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 16:34:58,344 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 93 posts with sort=relevance, time=week. New unique: 93. Total unique: 93
2025-05-29 16:34:58,344 [INFO] scraper.reddit_scraper.APIClient:191:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 16:35:00,738 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 91 posts with sort=top, time=week. New unique: 60. Total unique: 153
2025-05-29 16:35:00,738 [INFO] scraper.reddit_scraper.APIClient:191:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 16:35:03,282 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 100 posts with sort=hot, time=week. New unique: 56. Total unique: 209
2025-05-29 16:35:03,282 [INFO] scraper.reddit_scraper.APIClient:191:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 16:35:07,355 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 89. Total unique: 298
2025-05-29 16:35:07,356 [INFO] scraper.reddit_scraper.APIClient:191:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 16:35:09,398 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 93 posts with sort=relevance, time=week. New unique: 0. Total unique: 298
2025-05-29 16:35:09,398 [INFO] scraper.reddit_scraper.APIClient:191:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 16:35:11,067 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_recv: 2845334102.00 (avg: 2844254413.82, std: 356852.80)
2025-05-29 16:35:11,153 [INFO] scraper.reddit_scraper.APIClient:205:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 298
2025-05-29 16:35:11,153 [INFO] scraper.reddit_scraper.APIClient:218: [TARGET] Need 702 more results, searching popular subreddits...
2025-05-29 16:35:11,565 [INFO] scraper.reddit_scraper.APIClient:383: Found 6 posts in r/technology. Total additional: 2
2025-05-29 16:35:12,665 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/programming. Total additional: 3
2025-05-29 16:35:13,769 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/MachineLearning. Total additional: 4
2025-05-29 16:35:14,905 [INFO] scraper.reddit_scraper.APIClient:383: Found 7 posts in r/artificial. Total additional: 9
2025-05-29 16:35:15,923 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/gadgets. Total additional: 9
2025-05-29 16:35:17,025 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/science. Total additional: 9
2025-05-29 16:35:18,146 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/askreddit. Total additional: 10
2025-05-29 16:35:19,239 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/explainlikeimfive. Total additional: 10
2025-05-29 16:35:20,358 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/news. Total additional: 10
2025-05-29 16:35:21,445 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/worldnews. Total additional: 10
2025-05-29 16:35:22,526 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/business. Total additional: 10
2025-05-29 16:35:23,667 [INFO] scraper.reddit_scraper.APIClient:383: Found 2 posts in r/entrepreneur. Total additional: 12
2025-05-29 16:35:24,738 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/startup. Total additional: 12
2025-05-29 16:35:25,864 [INFO] scraper.reddit_scraper.APIClient:383: Found 1 posts in r/investing. Total additional: 13
2025-05-29 16:35:26,937 [INFO] scraper.reddit_scraper.APIClient:383: Found 0 posts in r/datascience. Total additional: 13
2025-05-29 16:35:26,938 [INFO] scraper.reddit_scraper.APIClient:222:    [SUBREDDIT] Added 13 from subreddit search. Total: 311
2025-05-29 16:35:26,938 [INFO] scraper.reddit_scraper.APIClient:227: [VARIANTS] Need 689 more results, trying query variations...
2025-05-29 16:35:26,938 [INFO] scraper.reddit_scraper.APIClient:238:    [1/4] Trying variant: "openai"
2025-05-29 16:35:29,118 [INFO] scraper.reddit_scraper.APIClient:251:    [VARIANT] Variant '"openai"' added 0 new posts. Total: 311
2025-05-29 16:35:29,118 [INFO] scraper.reddit_scraper.APIClient:238:    [2/4] Trying variant: openai OR openai
2025-05-29 16:35:30,651 [INFO] scraper.reddit_scraper.APIClient:251:    [VARIANT] Variant 'openai OR openai' added 0 new posts. Total: 311
2025-05-29 16:35:30,651 [INFO] scraper.reddit_scraper.APIClient:238:    [3/4] Trying variant: title:openai
2025-05-29 16:35:31,608 [INFO] scraper.reddit_scraper.APIClient:251:    [VARIANT] Variant 'title:openai' added 0 new posts. Total: 311
2025-05-29 16:35:31,608 [INFO] scraper.reddit_scraper.APIClient:238:    [4/4] Trying variant: selftext:openai
2025-05-29 16:35:33,618 [INFO] scraper.reddit_scraper.APIClient:251:    [VARIANT] Variant 'selftext:openai' added 23 new posts. Total: 334
2025-05-29 16:35:33,618 [INFO] scraper.reddit_scraper.APIClient:263: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'openai':
[STATS] Total unique posts found: 334
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 164
[RESULT] SUCCESS: Found 334 posts that will be processed and saved

2025-05-29 16:35:33,619 [INFO] scraper.reddit_scraper:654: Reddit comprehensive API returned 334 mentions
2025-05-29 16:35:33,619 [INFO] scraper.reddit_scraper:1000: Post-processing 334 mentions...
2025-05-29 16:35:33,619 [INFO] scraper.reddit_scraper:1446: Deduplication: 334 -> 334 mentions (removed 0 exact duplicates)
2025-05-29 16:35:33,619 [INFO] scraper.reddit_scraper:1004: After deduplication: 334 mentions
2025-05-29 16:35:33,626 [INFO] scraper.reddit_scraper:1008: After validation: 334 mentions
2025-05-29 16:35:33,833 [INFO] scraper.reddit_scraper:1025: After database mapping: 334 mentions (ALL KEPT - no quality filtering)
2025-05-29 16:35:33,834 [INFO] scraper.reddit_scraper:1045: Final result: 334 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 16:35:33,835 [INFO] analytics.data_validator:236: Validating dataset of 334 mentions
2025-05-29 16:35:34,517 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:35:34,567 [INFO] analytics.data_validator:259: Validation complete: 334/334 mentions passed
2025-05-29 16:35:34,567 [INFO] app:742: Data validation: 334/334 mentions passed
2025-05-29 16:35:35,632 [INFO] app:846: Search completed: openai -> 334 mentions
2025-05-29 16:35:35,658 [ERROR] app:1413: Visualization generation failed: plotly.graph_objs._figure.Figure.update_layout() got multiple values for keyword argument 'xaxis'
2025-05-29 16:36:40,236 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:36:49,005 [INFO] app:1521: Search history cleared: 1362 mentions, 20 sessions
2025-05-29 16:37:46,359 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:47:21,344 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:47:21,345 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:47:21,345 [INFO] scraper.reddit_scraper:507: HTTP session initialized successfully
2025-05-29 16:47:21,345 [INFO] scraper.reddit_scraper:485: Reddit scraper initialized with enhanced error handling
2025-05-29 16:47:21,346 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:47:21,347 [INFO] app:392: Database initialized successfully
2025-05-29 16:47:21,350 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:47:21,351 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:47:21,456 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 16:47:21,460 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 16:47:21,461 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 16:47:21,462 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 16:47:21,497 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 16:47:21,498 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 16:47:21,608 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 16:47:21,609 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 16:47:21,710 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000011F3F492780>
2025-05-29 16:47:21,711 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000011F3D89CED0> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 16:47:21,803 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000011F3F492690>
2025-05-29 16:47:21,803 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 16:47:21,803 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:47:21,803 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 16:47:21,804 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:47:21,804 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 16:47:21,891 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 11:17:21 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 16:47:21,892 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:47:21,892 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 16:47:21,892 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:47:21,892 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:47:21,893 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:47:21,893 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:47:21,893 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:47:21,900 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 16:47:21,976 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000011F3F4691C0>
2025-05-29 16:47:21,976 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000011F3D803C50> server_hostname='api.gradio.app' timeout=3
2025-05-29 16:47:22,362 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 16:47:22,594 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000011F3F491F40>
2025-05-29 16:47:22,594 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 16:47:22,595 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:47:22,595 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 16:47:22,595 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:47:22,595 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 16:47:22,907 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 11:17:22 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 16:47:22,908 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:47:22,908 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 16:47:22,908 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:47:22,908 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:47:22,908 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:47:22,909 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:47:22,909 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:47:23,373 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:47:23,935 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000011F3F6BB980>
2025-05-29 16:47:23,935 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 16:47:23,935 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:47:23,936 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 16:47:23,936 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:47:23,936 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 16:47:23,937 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 11:17:23 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 16:47:23,937 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 16:47:23,937 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 16:47:23,937 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:47:23,937 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:47:23,938 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:47:23,938 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:47:23,938 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:47:23,943 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 16:47:26,027 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000011F3F848D70>
2025-05-29 16:47:26,027 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 16:47:26,027 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:47:26,028 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 16:47:26,028 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:47:26,028 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 16:47:26,036 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 11:17:24 GMT'), (b'server', b'uvicorn'), (b'content-length', b'68650'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 16:47:26,036 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 16:47:26,036 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 16:47:26,037 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:47:26,037 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:47:26,037 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:47:26,037 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:47:26,037 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:48:08,226 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:48:08,227 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:48:08,227 [INFO] scraper.reddit_scraper:507: HTTP session initialized successfully
2025-05-29 16:48:08,227 [INFO] scraper.reddit_scraper:485: Reddit scraper initialized with enhanced error handling
2025-05-29 16:48:08,228 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:48:08,228 [INFO] app:392: Database initialized successfully
2025-05-29 16:48:08,231 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:48:08,232 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:48:08,320 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 16:48:08,324 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 16:48:08,325 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 16:48:08,325 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 16:48:08,360 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 16:48:08,361 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 16:48:08,474 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 16:48:08,474 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 16:48:08,577 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D65BD624B0>
2025-05-29 16:48:08,577 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001D65A0D8CD0> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 16:48:08,669 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D65BD623F0>
2025-05-29 16:48:08,669 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 16:48:08,670 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:48:08,670 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 16:48:08,670 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:48:08,670 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 16:48:08,764 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 11:18:08 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 16:48:08,764 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:48:08,765 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 16:48:08,765 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:48:08,765 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:48:08,765 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:48:08,765 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:48:08,766 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:48:08,799 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D65BD308F0>
2025-05-29 16:48:08,799 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001D65A0D3A50> server_hostname='api.gradio.app' timeout=3
2025-05-29 16:48:09,244 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 16:48:09,388 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D65BD62B70>
2025-05-29 16:48:09,388 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 16:48:09,388 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:48:09,389 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 16:48:09,389 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:48:09,389 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 16:48:09,684 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 11:18:09 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 16:48:09,684 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:48:09,684 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 16:48:09,685 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:48:09,685 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:48:09,685 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:48:09,685 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:48:09,685 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:48:10,251 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:48:29,472 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:48:41,917 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:48:41,917 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:48:41,919 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:48:46,555 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:48:46,557 [INFO] app:605: Cache hit for search term: openai
2025-05-29 16:48:46,912 [ERROR] app:1474: Search failed: too many values to unpack (expected 2)
2025-05-29 16:48:46,921 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:48:46,977 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:48:46,979 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:49:34,582 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_recv: 2852386302.00 (avg: 2849052785.15, std: 1014119.28)
2025-05-29 16:49:35,587 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:50:41,710 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:51:33,116 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:51:33,117 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:51:33,117 [INFO] scraper.reddit_scraper:507: HTTP session initialized successfully
2025-05-29 16:51:33,117 [INFO] scraper.reddit_scraper:485: Reddit scraper initialized with enhanced error handling
2025-05-29 16:51:33,117 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:51:33,118 [INFO] app:392: Database initialized successfully
2025-05-29 16:51:33,120 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:51:33,121 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:51:33,212 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 16:51:33,216 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 16:51:33,217 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 16:51:33,217 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 16:51:33,252 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 16:51:33,254 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 16:51:33,362 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 16:51:33,373 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 16:51:33,457 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002253249AF30>
2025-05-29 16:51:33,458 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022530854CD0> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 16:51:33,536 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022532499D00>
2025-05-29 16:51:33,536 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 16:51:33,537 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:51:33,537 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 16:51:33,537 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:51:33,537 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 16:51:33,615 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 11:21:33 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 16:51:33,616 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:51:33,616 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 16:51:33,616 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:51:33,616 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:51:33,616 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:51:33,617 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:51:33,617 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:51:33,654 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002253221EE40>
2025-05-29 16:51:33,654 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x00000225307BBA50> server_hostname='api.gradio.app' timeout=3
2025-05-29 16:51:33,661 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 16:51:34,135 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 16:51:34,212 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002253244EB70>
2025-05-29 16:51:34,212 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 16:51:34,212 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:51:34,212 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 16:51:34,213 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:51:34,213 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 16:51:34,494 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 11:21:34 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 16:51:34,495 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:51:34,495 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 16:51:34,495 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:51:34,495 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:51:34,495 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:51:34,496 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:51:34,496 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:51:35,145 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:51:35,682 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022532673920>
2025-05-29 16:51:35,682 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 16:51:35,682 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:51:35,683 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 16:51:35,683 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:51:35,683 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 16:51:35,683 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 11:21:35 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 16:51:35,684 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 16:51:35,684 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 16:51:35,684 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:51:35,684 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:51:35,684 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:51:35,685 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:51:35,685 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:51:35,693 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 16:51:37,728 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022532054BF0>
2025-05-29 16:51:37,728 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 16:51:37,729 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:51:37,729 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 16:51:37,729 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:51:37,730 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 16:51:37,738 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 11:21:36 GMT'), (b'server', b'uvicorn'), (b'content-length', b'68651'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 16:51:37,738 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 16:51:37,738 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 16:51:37,738 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:51:37,739 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:51:37,739 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:51:37,739 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:51:37,739 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:51:51,510 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:51:51,511 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:51:51,512 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:51:56,636 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:51:56,640 [INFO] app:1531: Search history cleared: 0 mentions, 0 sessions
2025-05-29 16:51:56,641 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:51:58,568 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:51:58,569 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:52:04,317 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:52:04,322 [ERROR] app:857: Search failed for 'openai': 'RedditScraper' object has no attribute 'circuit_breaker'
2025-05-29 16:52:04,322 [ERROR] app:1461: Search failed: cannot access local variable 'traceback' where it is not associated with a value
2025-05-29 16:52:04,351 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:52:04,427 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:52:04,431 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:52:41,272 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:53:47,363 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:55:30,857 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:55:30,859 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:55:30,859 [INFO] scraper.reddit_scraper:519: HTTP session initialized successfully
2025-05-29 16:55:30,859 [INFO] scraper.reddit_scraper:497: Reddit scraper initialized with enhanced error handling
2025-05-29 16:55:30,859 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 16:55:30,860 [INFO] app:392: Database initialized successfully
2025-05-29 16:55:30,863 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 16:55:30,864 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 16:55:30,956 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 16:55:30,959 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 16:55:30,960 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 16:55:30,960 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 16:55:30,995 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 16:55:30,996 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 16:55:31,107 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 16:55:31,107 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 16:55:31,233 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B8C1130CE0>
2025-05-29 16:55:31,233 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000002B8BF4418D0> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 16:55:31,324 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B8C1130B30>
2025-05-29 16:55:31,325 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 16:55:31,325 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:55:31,325 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 16:55:31,325 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:55:31,325 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 16:55:31,394 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 16:55:31,413 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 11:25:31 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 16:55:31,414 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 16:55:31,414 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 16:55:31,414 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:55:31,415 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:55:31,415 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:55:31,415 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:55:31,415 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:55:31,437 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B8C1131340>
2025-05-29 16:55:31,437 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000002B8BF4402D0> server_hostname='api.gradio.app' timeout=3
2025-05-29 16:55:31,874 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 16:55:32,036 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B8C1131250>
2025-05-29 16:55:32,036 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 16:55:32,037 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:55:32,037 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 16:55:32,037 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:55:32,037 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 16:55:32,329 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 11:25:32 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 16:55:32,329 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 16:55:32,330 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 16:55:32,330 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:55:32,330 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:55:32,330 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:55:32,330 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:55:32,330 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:55:32,882 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:55:33,424 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B8C132E210>
2025-05-29 16:55:33,424 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 16:55:33,425 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:55:33,425 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 16:55:33,425 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:55:33,425 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 16:55:33,426 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 11:25:33 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 16:55:33,426 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 16:55:33,426 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 16:55:33,426 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:55:33,426 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:55:33,427 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:55:33,427 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:55:33,427 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:55:33,433 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 16:55:35,468 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002B8C132DEE0>
2025-05-29 16:55:35,468 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 16:55:35,468 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 16:55:35,469 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 16:55:35,469 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 16:55:35,469 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 16:55:35,477 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 11:25:34 GMT'), (b'server', b'uvicorn'), (b'content-length', b'68695'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 16:55:35,478 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 16:55:35,478 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 16:55:35,478 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 16:55:35,479 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 16:55:35,479 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 16:55:35,479 [DEBUG] httpcore.connection:47: close.started
2025-05-29 16:55:35,479 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 16:55:42,067 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:55:42,067 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:55:42,069 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:55:45,590 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:55:45,593 [INFO] app:1530: Search history cleared: 0 mentions, 1 sessions
2025-05-29 16:55:45,595 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:55:48,669 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:55:48,670 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:55:53,520 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:55:53,526 [INFO] scraper.reddit_scraper:654: Attempting Reddit API search for: openAI
2025-05-29 16:55:53,526 [INFO] scraper.reddit_scraper.APIClient:173: [SEARCH] Starting COMPREHENSIVE search for 'openAI' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 16:55:53,526 [INFO] scraper.reddit_scraper.APIClient:188: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 16:55:53,526 [INFO] scraper.reddit_scraper.APIClient:192:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 16:55:55,924 [INFO] scraper.reddit_scraper.APIClient:206:    [FOUND] Found 93 posts with sort=relevance, time=week. New unique: 93. Total unique: 93
2025-05-29 16:55:55,925 [INFO] scraper.reddit_scraper.APIClient:192:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 16:55:58,981 [INFO] scraper.reddit_scraper.APIClient:206:    [FOUND] Found 93 posts with sort=top, time=week. New unique: 61. Total unique: 154
2025-05-29 16:55:58,981 [INFO] scraper.reddit_scraper.APIClient:192:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 16:56:01,156 [INFO] scraper.reddit_scraper.APIClient:206:    [FOUND] Found 97 posts with sort=hot, time=week. New unique: 70. Total unique: 224
2025-05-29 16:56:01,157 [INFO] scraper.reddit_scraper.APIClient:192:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 16:56:03,550 [INFO] scraper.reddit_scraper.APIClient:206:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 99. Total unique: 323
2025-05-29 16:56:03,551 [INFO] scraper.reddit_scraper.APIClient:192:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 16:56:06,323 [INFO] scraper.reddit_scraper.APIClient:206:    [FOUND] Found 93 posts with sort=relevance, time=week. New unique: 0. Total unique: 323
2025-05-29 16:56:06,323 [INFO] scraper.reddit_scraper.APIClient:192:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 16:56:11,847 [INFO] scraper.reddit_scraper.APIClient:206:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 323
2025-05-29 16:56:11,847 [INFO] scraper.reddit_scraper.APIClient:219: [TARGET] Need 677 more results, searching popular subreddits...
2025-05-29 16:56:12,333 [INFO] scraper.reddit_scraper.APIClient:384: Found 9 posts in r/technology. Total additional: 6
2025-05-29 16:56:13,368 [INFO] scraper.reddit_scraper.APIClient:384: Found 4 posts in r/programming. Total additional: 10
2025-05-29 16:56:14,910 [INFO] scraper.reddit_scraper.APIClient:384: Found 10 posts in r/MachineLearning. Total additional: 20
2025-05-29 16:56:16,393 [INFO] scraper.reddit_scraper.APIClient:384: Found 19 posts in r/artificial. Total additional: 37
2025-05-29 16:56:16,876 [INFO] scraper.reddit_scraper.APIClient:384: Found 0 posts in r/gadgets. Total additional: 37
2025-05-29 16:56:17,877 [INFO] scraper.reddit_scraper.APIClient:384: Found 0 posts in r/science. Total additional: 37
2025-05-29 16:56:19,011 [INFO] scraper.reddit_scraper.APIClient:384: Found 3 posts in r/askreddit. Total additional: 40
2025-05-29 16:56:21,465 [INFO] scraper.reddit_scraper.APIClient:384: Found 0 posts in r/explainlikeimfive. Total additional: 40
2025-05-29 16:56:21,859 [INFO] scraper.reddit_scraper.APIClient:384: Found 1 posts in r/news. Total additional: 40
2025-05-29 16:56:22,976 [INFO] scraper.reddit_scraper.APIClient:384: Found 0 posts in r/worldnews. Total additional: 40
2025-05-29 16:56:24,081 [INFO] scraper.reddit_scraper.APIClient:384: Found 1 posts in r/business. Total additional: 41
2025-05-29 16:56:25,204 [INFO] scraper.reddit_scraper.APIClient:384: Found 7 posts in r/entrepreneur. Total additional: 48
2025-05-29 16:56:26,252 [INFO] scraper.reddit_scraper.APIClient:384: Found 0 posts in r/startup. Total additional: 48
2025-05-29 16:56:27,416 [INFO] scraper.reddit_scraper.APIClient:384: Found 2 posts in r/investing. Total additional: 50
2025-05-29 16:56:28,484 [INFO] scraper.reddit_scraper.APIClient:384: Found 1 posts in r/datascience. Total additional: 51
2025-05-29 16:56:28,484 [INFO] scraper.reddit_scraper.APIClient:223:    [SUBREDDIT] Added 51 from subreddit search. Total: 374
2025-05-29 16:56:28,484 [INFO] scraper.reddit_scraper.APIClient:228: [VARIANTS] Need 626 more results, trying query variations...
2025-05-29 16:56:28,484 [INFO] scraper.reddit_scraper.APIClient:239:    [1/4] Trying variant: "openAI"
2025-05-29 16:56:30,660 [INFO] scraper.reddit_scraper.APIClient:252:    [VARIANT] Variant '"openAI"' added 0 new posts. Total: 374
2025-05-29 16:56:30,660 [INFO] scraper.reddit_scraper.APIClient:239:    [2/4] Trying variant: openAI OR openAI
2025-05-29 16:56:32,403 [INFO] scraper.reddit_scraper.APIClient:252:    [VARIANT] Variant 'openAI OR openAI' added 0 new posts. Total: 374
2025-05-29 16:56:32,403 [INFO] scraper.reddit_scraper.APIClient:239:    [3/4] Trying variant: title:openAI
2025-05-29 16:56:33,646 [INFO] scraper.reddit_scraper.APIClient:252:    [VARIANT] Variant 'title:openAI' added 0 new posts. Total: 374
2025-05-29 16:56:33,647 [INFO] scraper.reddit_scraper.APIClient:239:    [4/4] Trying variant: selftext:openAI
2025-05-29 16:56:35,664 [INFO] scraper.reddit_scraper.APIClient:252:    [VARIANT] Variant 'selftext:openAI' added 22 new posts. Total: 396
2025-05-29 16:56:35,664 [INFO] scraper.reddit_scraper.APIClient:264: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'openAI':
[STATS] Total unique posts found: 396
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 234
[RESULT] SUCCESS: Found 396 posts that will be processed and saved

2025-05-29 16:56:35,665 [INFO] scraper.reddit_scraper:663: Reddit comprehensive API returned 396 mentions
2025-05-29 16:56:35,665 [INFO] scraper.reddit_scraper:1009: Post-processing 396 mentions...
2025-05-29 16:56:35,665 [INFO] scraper.reddit_scraper:1455: Deduplication: 396 -> 396 mentions (removed 0 exact duplicates)
2025-05-29 16:56:35,665 [INFO] scraper.reddit_scraper:1013: After deduplication: 396 mentions
2025-05-29 16:56:35,666 [INFO] scraper.reddit_scraper:1519: Validation: 396 raw mentions  396 kept (EXTREMELY LENIENT - keeping everything possible)
2025-05-29 16:56:35,666 [INFO] scraper.reddit_scraper:1017: After validation: 396 mentions
2025-05-29 16:56:36,213 [INFO] scraper.reddit_scraper:1034: After database mapping: 396 mentions (ALL KEPT - no quality filtering)
2025-05-29 16:56:36,213 [WARNING] scraper.reddit_scraper:695: Reddit comprehensive API failed: 'RedditScraper' object has no attribute 'advanced_sentiment', falling back to web scraping
2025-05-29 16:56:36,213 [INFO] scraper.reddit_scraper:703: Falling back to enhanced web scraping approach
2025-05-29 16:56:36,603 [WARNING] scraper.reddit_scraper:737: Playwright scraping failed: 'RedditScraper' object has no attribute 'proxy_list'
2025-05-29 16:56:36,674 [INFO] scraper.reddit_scraper:1673: Starting Selenium fallback scraping
2025-05-29 16:56:36,675 [DEBUG] selenium.webdriver.common.selenium_manager:100: Selenium Manager binary found at: C:\Users\Devansh\AppData\Roaming\Python\Python312\site-packages\selenium\webdriver\common\windows\selenium-manager.exe
2025-05-29 16:56:36,675 [DEBUG] selenium.webdriver.common.selenium_manager:113: Executing process: C:\Users\Devansh\AppData\Roaming\Python\Python312\site-packages\selenium\webdriver\common\windows\selenium-manager.exe --browser chrome --debug --language-binding python --output json
2025-05-29 16:56:38,993 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:57:45,081 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:58:13,184 [DEBUG] selenium.webdriver.common.selenium_manager:139: Sending stats to Plausible: Props { browser: "chrome", browser_version: "", os: "windows", arch: "amd64", lang: "python", selenium_version: "4.33" }
2025-05-29 16:58:13,185 [DEBUG] selenium.webdriver.common.selenium_manager:139: chromedriver not found in PATH
2025-05-29 16:58:13,185 [DEBUG] selenium.webdriver.common.selenium_manager:139: chrome detected at C:\Program Files\Google\Chrome\Application\chrome.exe
2025-05-29 16:58:13,185 [DEBUG] selenium.webdriver.common.selenium_manager:139: Detected browser: chrome 136.0.7103.114
2025-05-29 16:58:13,185 [DEBUG] selenium.webdriver.common.selenium_manager:139: Discovering versions from https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json
2025-05-29 16:58:13,185 [DEBUG] selenium.webdriver.common.selenium_manager:139: Required driver: chromedriver 136.0.7103.113
2025-05-29 16:58:13,185 [DEBUG] selenium.webdriver.common.selenium_manager:139: Acquiring lock: C:\Users\Devansh\.cache\selenium\chromedriver\win64\136.0.7103.113\sm.lock
2025-05-29 16:58:13,185 [DEBUG] selenium.webdriver.common.selenium_manager:139: Downloading chromedriver 136.0.7103.113 from https://storage.googleapis.com/chrome-for-testing-public/136.0.7103.113/win64/chromedriver-win64.zip
2025-05-29 16:58:13,185 [DEBUG] selenium.webdriver.common.selenium_manager:139: Driver path: C:\Users\Devansh\.cache\selenium\chromedriver\win64\136.0.7103.113\chromedriver.exe
2025-05-29 16:58:13,186 [DEBUG] selenium.webdriver.common.selenium_manager:139: Browser path: C:\Program Files\Google\Chrome\Application\chrome.exe
2025-05-29 16:58:13,330 [DEBUG] selenium.webdriver.common.service:224: Started executable: `C:\Users\Devansh\.cache\selenium\chromedriver\win64\136.0.7103.113\chromedriver.exe` in a child process with pid: 32044 using 0 to output -3
2025-05-29 16:58:13,834 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session {'capabilities': {'firstMatch': [{}], 'alwaysMatch': {'browserName': 'chrome', 'pageLoadStrategy': <PageLoadStrategy.normal: 'normal'>, 'browserVersion': None, 'goog:chromeOptions': {'extensions': [], 'binary': 'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe', 'args': ['--headless', '--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu', '--window-size=1920,1080', '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36']}}}}
2025-05-29 16:58:14,278 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"136.0.7103.114","chrome":{"chromedriverVersion":"136.0.7103.113 (76fa3c1782406c63308c70b54f228fd39c7aaa71-refs/branch-heads/7103_108@{#3})","userDataDir":"C:\\Users\\Devansh\\AppData\\Local\\Temp\\scoped_dir32044_1306611314"},"fedcm:accounts":true,"goog:chromeOptions":{"debuggerAddress":"localhost:57890"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:extension:credBlob":true,"webauthn:extension:largeBlob":true,"webauthn:extension:minPinLength":true,"webauthn:extension:prf":true,"webauthn:virtualAuthenticators":true},"sessionId":"b001fd8853604c391e05703ef86db04a"}} | headers=HTTPHeaderDict({'Content-Length': '887', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:14,279 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:14,279 [INFO] scraper.reddit_scraper:1701: Selenium scraping URL 1/3: https://www.reddit.com/search/?q=openAI&sort=relevance&t=week
2025-05-29 16:58:14,279 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/url {'url': 'https://www.reddit.com/search/?q=openAI&sort=relevance&t=week'}
2025-05-29 16:58:17,142 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.65 (avg: 93.65, std: 0.00)
2025-05-29 16:58:17,142 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.10 (avg: 121.13, std: 0.01)
2025-05-29 16:58:17,142 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_recv: 2872551414.00 (avg: 2858436557.56, std: 4100866.99)
2025-05-29 16:58:19,187 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:19,187 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:19,187 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:19,192 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:19,193 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:19,694 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:19,713 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:19,714 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:20,215 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:20,219 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:20,219 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:20,720 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:20,723 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:20,724 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:21,225 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:21,229 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:21,229 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:21,731 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:21,734 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:21,734 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:22,234 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:22,237 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:22,238 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:22,739 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:22,742 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:22,742 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:23,244 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:23,247 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:23,247 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:23,747 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:23,750 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:23,751 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:24,252 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:24,255 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:24,256 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:24,757 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:24,761 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:24,761 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:25,262 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:25,266 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:25,266 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:25,767 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:25,770 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:25,771 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:26,272 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:26,275 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:26,276 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:26,777 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:26,780 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:26,781 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:27,282 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:27,285 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:27,286 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:27,787 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:27,790 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:27,791 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:28,170 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.65 (avg: 93.65, std: 0.00)
2025-05-29 16:58:28,170 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.09 (avg: 121.13, std: 0.01)
2025-05-29 16:58:28,170 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_recv: 2880421291.00 (avg: 2859729777.18, std: 6648092.15)
2025-05-29 16:58:28,170 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in process_count: 395.00 (avg: 387.88, std: 2.23)
2025-05-29 16:58:28,292 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:28,295 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:28,296 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:28,797 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:28,800 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:28,801 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:29,302 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:29,305 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:29,306 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:29,306 [WARNING] scraper.reddit_scraper:1728: Timeout loading https://www.reddit.com/search/?q=openAI&sort=relevance&t=week
2025-05-29 16:58:29,306 [INFO] scraper.reddit_scraper:1701: Selenium scraping URL 2/3: https://www.reddit.com/search/?q=openAI&sort=top&t=week
2025-05-29 16:58:29,307 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/url {'url': 'https://www.reddit.com/search/?q=openAI&sort=top&t=week'}
2025-05-29 16:58:31,481 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:31,481 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:31,481 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:31,486 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:31,487 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:31,988 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:31,991 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:31,992 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:32,492 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:32,495 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:32,496 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:32,997 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:33,000 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:33,001 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:33,502 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:33,505 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:33,506 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:34,007 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:34,011 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:34,011 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:34,512 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:34,515 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:34,516 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:35,017 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:35,020 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:35,021 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:35,521 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:35,524 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:35,525 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:36,026 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:36,029 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:36,029 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:36,531 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:36,534 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:36,535 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:37,035 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:37,038 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:37,039 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:37,541 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:37,544 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:37,545 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:38,046 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:38,049 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:38,050 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:38,551 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:38,554 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:38,554 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:39,056 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:39,059 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:39,059 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:39,560 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:39,563 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:39,564 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:40,066 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:40,069 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:40,070 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:40,571 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:40,574 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:40,575 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:41,076 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:41,079 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:41,080 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:41,580 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:41,583 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:41,584 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:41,584 [WARNING] scraper.reddit_scraper:1728: Timeout loading https://www.reddit.com/search/?q=openAI&sort=top&t=week
2025-05-29 16:58:41,584 [INFO] scraper.reddit_scraper:1701: Selenium scraping URL 3/3: https://www.reddit.com/search/?q=openAI&sort=new&t=week
2025-05-29 16:58:41,584 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/url {'url': 'https://www.reddit.com/search/?q=openAI&sort=new&t=week'}
2025-05-29 16:58:43,753 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:43,753 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:43,753 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:43,763 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:43,763 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:44,265 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:44,269 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:44,269 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:44,770 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:44,773 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:44,774 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:45,275 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:45,278 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:45,279 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:45,780 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:45,783 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:45,784 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:46,285 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:46,288 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:46,288 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:46,789 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:46,792 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:46,793 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:47,295 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:47,298 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:47,298 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:47,799 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:47,802 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:47,803 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:48,305 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:48,308 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:48,308 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:48,810 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:48,813 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:48,813 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:49,315 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:49,318 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:49,319 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:49,819 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:49,823 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:49,824 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:50,324 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:50,327 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:50,328 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:50,829 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:50,832 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:50,833 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:51,220 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 16:58:51,334 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:51,338 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:51,338 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:51,839 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:51,842 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:51,842 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:52,343 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:52,346 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:52,347 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:52,849 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:52,852 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:52,853 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:53,354 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:53,357 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:53,357 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:53,859 [DEBUG] selenium.webdriver.remote.remote_connection:403: POST http://localhost:57863/session/b001fd8853604c391e05703ef86db04a/element {'using': 'css selector', 'value': '[data-testid="post-container"]'}
2025-05-29 16:58:53,862 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=404 | data={"value":{"error":"no such element","message":"no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-testid=\"post-container\"]\"}\n  (Session info: chrome=136.0.7103.114)","stacktrace":"\tGetHandleVerifier [0x00007FF60AE5CF45+75717]\n\tGetHandleVerifier [0x00007FF60AE5CFA0+75808]\n\t(No symbol) [0x00007FF60AC28F9A]\n\t(No symbol) [0x00007FF60AC7F4C6]\n\t(No symbol) [0x00007FF60AC7F77C]\n\t(No symbol) [0x00007FF60ACD2577]\n\t(No symbol) [0x00007FF60ACA73BF]\n\t(No symbol) [0x00007FF60ACCF39C]\n\t(No symbol) [0x00007FF60ACA7153]\n\t(No symbol) [0x00007FF60AC70421]\n\t(No symbol) [0x00007FF60AC711B3]\n\tGetHandleVerifier [0x00007FF60B15D71D+3223453]\n\tGetHandleVerifier [0x00007FF60B157CC2+3200322]\n\tGetHandleVerifier [0x00007FF60B175AF3+3322739]\n\tGetHandleVerifier [0x00007FF60AE76A1A+180890]\n\tGetHandleVerifier [0x00007FF60AE7E11F+211359]\n\tGetHandleVerifier [0x00007FF60AE65294+109332]\n\tGetHandleVerifier [0x00007FF60AE65442+109762]\n\tGetHandleVerifier [0x00007FF60AE4BA59+4825]\n\tBaseThreadInitThunk [0x00007FFDC86A7374+20]\n\tRtlUserThreadStart [0x00007FFDCA2DCC91+33]\n"}} | headers=HTTPHeaderDict({'Content-Length': '1135', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:53,863 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:53,863 [WARNING] scraper.reddit_scraper:1728: Timeout loading https://www.reddit.com/search/?q=openAI&sort=new&t=week
2025-05-29 16:58:53,863 [INFO] scraper.reddit_scraper:1734: Selenium fallback completed with 0 mentions
2025-05-29 16:58:53,864 [DEBUG] selenium.webdriver.remote.remote_connection:403: DELETE http://localhost:57863/session/b001fd8853604c391e05703ef86db04a {}
2025-05-29 16:58:53,881 [DEBUG] selenium.webdriver.remote.remote_connection:436: Remote response: status=200 | data={"value":null} | headers=HTTPHeaderDict({'Content-Length': '14', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2025-05-29 16:58:53,881 [DEBUG] selenium.webdriver.remote.remote_connection:465: Finished Request
2025-05-29 16:58:55,920 [INFO] scraper.reddit_scraper:1009: Post-processing 396 mentions...
2025-05-29 16:58:55,920 [INFO] scraper.reddit_scraper:1455: Deduplication: 396 -> 396 mentions (removed 0 exact duplicates)
2025-05-29 16:58:55,920 [INFO] scraper.reddit_scraper:1013: After deduplication: 396 mentions
2025-05-29 16:58:55,921 [INFO] scraper.reddit_scraper:1519: Validation: 396 raw mentions  396 kept (EXTREMELY LENIENT - keeping everything possible)
2025-05-29 16:58:55,921 [INFO] scraper.reddit_scraper:1017: After validation: 396 mentions
2025-05-29 16:58:56,294 [INFO] scraper.reddit_scraper:1034: After database mapping: 396 mentions (ALL KEPT - no quality filtering)
2025-05-29 16:58:56,295 [ERROR] scraper.reddit_scraper:786: Both API and web scraping failed for 'openAI': 'RedditScraper' object has no attribute 'advanced_sentiment'
2025-05-29 16:58:56,295 [ERROR] app:856: Search failed for 'openAI': 'SystemMonitor' object has no attribute 'emit_search_failed'
2025-05-29 16:58:56,299 [ERROR] app:857: Traceback (most recent call last):
  File "C:\Users\Devansh\Downloads\intern_application_round1\scraper\reddit_scraper.py", line 759, in scrape_mentions
    enhanced_mentions = await self._post_process_mentions(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Devansh\Downloads\intern_application_round1\scraper\reddit_scraper.py", line 1037, in _post_process_mentions
    if processed_mentions and self.advanced_sentiment and self.advanced_sentiment.is_available():
                              ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RedditScraper' object has no attribute 'advanced_sentiment'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Devansh\Downloads\intern_application_round1\app.py", line 709, in search_mentions
    raw_mentions = await self.scraper.scrape_mentions(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Devansh\Downloads\intern_application_round1\scraper\reddit_scraper.py", line 789, in scrape_mentions
    self.monitor.emit_search_failed(session_id, error_msg)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'SystemMonitor' object has no attribute 'emit_search_failed'

2025-05-29 16:58:56,324 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:59:01,244 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 889092714.00 (avg: 887616119.20, std: 464188.22)
2025-05-29 16:59:07,521 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:59:07,525 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:59:12,125 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 16:59:12,316 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 16:59:57,328 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:01:03,475 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:05:40,140 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:05:40,140 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:05:40,140 [INFO] scraper.reddit_scraper:294: HTTP session initialized successfully
2025-05-29 17:05:40,140 [INFO] scraper.reddit_scraper:277: Reddit scraper initialized with API-only configuration
2025-05-29 17:05:40,141 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 17:05:40,141 [INFO] app:392: Database initialized successfully
2025-05-29 17:05:40,143 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 17:05:40,144 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 17:05:40,237 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 17:05:40,241 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:05:40,242 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 17:05:40,242 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 17:05:40,278 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:05:40,279 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 17:05:40,387 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:05:40,400 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:05:40,486 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C29AAFFC80>
2025-05-29 17:05:40,487 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001C298E5FCD0> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 17:05:40,566 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C29AAFFB90>
2025-05-29 17:05:40,566 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:05:40,566 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:05:40,567 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:05:40,567 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:05:40,567 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:05:40,648 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 11:35:40 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 17:05:40,649 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 17:05:40,649 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:05:40,650 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:05:40,650 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:05:40,650 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:05:40,650 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:05:40,650 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:05:40,681 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 17:05:40,735 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C29AAB57F0>
2025-05-29 17:05:40,735 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001C298E5E4D0> server_hostname='api.gradio.app' timeout=3
2025-05-29 17:05:41,154 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 17:05:41,307 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C29AAB4C80>
2025-05-29 17:05:41,307 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:05:41,307 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:05:41,308 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:05:41,308 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:05:41,308 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:05:41,596 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 11:35:41 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 17:05:41,596 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 17:05:41,597 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:05:41,597 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:05:41,597 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:05:41,597 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:05:41,597 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:05:41,597 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:05:42,162 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:05:42,705 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C29ACE6810>
2025-05-29 17:05:42,705 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:05:42,706 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:05:42,706 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:05:42,706 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:05:42,706 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:05:42,707 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 11:35:42 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 17:05:42,707 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 17:05:42,707 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:05:42,707 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:05:42,707 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:05:42,708 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:05:42,708 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:05:42,708 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:05:42,715 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 17:05:44,748 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C29ABB58E0>
2025-05-29 17:05:44,748 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 17:05:44,749 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:05:44,749 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 17:05:44,749 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:05:44,749 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 17:05:44,758 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 11:35:43 GMT'), (b'server', b'uvicorn'), (b'content-length', b'68694'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 17:05:44,758 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 17:05:44,759 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 17:05:44,759 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:05:44,759 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:05:44,759 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:05:44,759 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:05:44,759 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:06:23,829 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:06:23,830 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:06:23,830 [INFO] scraper.reddit_scraper:294: HTTP session initialized successfully
2025-05-29 17:06:23,830 [INFO] scraper.reddit_scraper:277: Reddit scraper initialized with API-only configuration
2025-05-29 17:06:23,830 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 17:06:23,831 [INFO] app:392: Database initialized successfully
2025-05-29 17:06:23,834 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 17:06:23,835 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 17:06:23,937 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 17:06:23,941 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:06:23,942 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 17:06:23,942 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 17:06:23,980 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:06:23,981 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 17:06:24,077 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:06:24,082 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:06:24,385 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 17:06:24,846 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 17:06:25,443 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D2ABFD8D40>
2025-05-29 17:06:25,443 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D2ABF79970>
2025-05-29 17:06:25,443 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001D2AA3C7D50> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 17:06:25,443 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001D2AA3C6550> server_hostname='api.gradio.app' timeout=3
2025-05-29 17:06:25,853 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:06:26,417 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D2AC23EEA0>
2025-05-29 17:06:26,417 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:06:26,418 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:06:26,418 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:06:26,418 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:06:26,419 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:06:26,419 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 11:36:26 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 17:06:26,419 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 17:06:26,419 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:06:26,420 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:06:26,420 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:06:26,420 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:06:26,420 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:06:26,420 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:06:26,427 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 17:06:28,450 [DEBUG] httpcore.connection:47: start_tls.failed exception=ConnectTimeout(TimeoutError('_ssl.c:983: The handshake operation timed out'))
2025-05-29 17:06:28,450 [DEBUG] httpcore.connection:47: start_tls.failed exception=ConnectTimeout(TimeoutError('_ssl.c:983: The handshake operation timed out'))
2025-05-29 17:06:28,481 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D2AC30E9F0>
2025-05-29 17:06:28,481 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 17:06:28,481 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:06:28,482 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 17:06:28,482 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:06:28,482 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 17:06:28,491 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 11:36:27 GMT'), (b'server', b'uvicorn'), (b'content-length', b'68695'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 17:06:28,491 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 17:06:28,492 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 17:06:28,492 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:06:28,492 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:06:28,492 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:06:28,492 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:06:28,492 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:06:28,684 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:06:28,778 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D2AC447B60>
2025-05-29 17:06:28,778 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001D2AC434250> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 17:06:28,858 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001D2AC401B20>
2025-05-29 17:06:28,858 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:06:28,859 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:06:28,859 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:06:28,859 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:06:28,859 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:06:28,940 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 11:36:28 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 17:06:28,940 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 17:06:28,940 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:06:28,940 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:06:28,942 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:06:28,942 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:06:28,942 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:06:28,942 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:07:02,229 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:07:02,229 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:07:02,230 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:07:05,929 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:07:06,091 [INFO] scraper.reddit_scraper:403: Starting Reddit API search for: amazon
2025-05-29 17:07:06,091 [INFO] scraper.reddit_scraper.APIClient:97: [SEARCH] Starting COMPREHENSIVE search for 'amazon' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 17:07:06,092 [INFO] scraper.reddit_scraper.APIClient:112: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 17:07:06,092 [INFO] scraper.reddit_scraper.APIClient:116:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 17:07:09,237 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 96 posts with sort=relevance, time=week. New unique: 96. Total unique: 96
2025-05-29 17:07:09,237 [INFO] scraper.reddit_scraper.APIClient:116:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 17:07:11,694 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 95 posts with sort=top, time=week. New unique: 60. Total unique: 156
2025-05-29 17:07:11,694 [INFO] scraper.reddit_scraper.APIClient:116:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 17:07:13,864 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=hot, time=week. New unique: 94. Total unique: 250
2025-05-29 17:07:13,864 [INFO] scraper.reddit_scraper.APIClient:116:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 17:07:16,468 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 100. Total unique: 350
2025-05-29 17:07:16,468 [INFO] scraper.reddit_scraper.APIClient:116:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 17:07:19,190 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 96 posts with sort=relevance, time=week. New unique: 0. Total unique: 350
2025-05-29 17:07:19,190 [INFO] scraper.reddit_scraper.APIClient:116:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 17:07:21,615 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 350
2025-05-29 17:07:21,616 [INFO] scraper.reddit_scraper.APIClient:143: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'amazon':
[STATS] Total unique posts found: 350
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 268
[RESULT] SUCCESS: Found 350 posts that will be processed and saved

2025-05-29 17:07:21,616 [INFO] scraper.reddit_scraper:415: Reddit API returned 350 mentions
2025-05-29 17:07:21,616 [INFO] scraper.reddit_scraper:487: Post-processing 350 mentions...
2025-05-29 17:07:21,616 [INFO] scraper.reddit_scraper:589: Deduplication: 350 -> 350 mentions (removed 0 exact duplicates)
2025-05-29 17:07:21,617 [INFO] scraper.reddit_scraper:491: After deduplication: 350 mentions
2025-05-29 17:07:21,617 [INFO] scraper.reddit_scraper:653: Validation: 350 raw mentions  350 kept (EXTREMELY LENIENT - keeping everything possible)
2025-05-29 17:07:21,617 [INFO] scraper.reddit_scraper:495: After validation: 350 mentions
2025-05-29 17:07:21,811 [INFO] scraper.reddit_scraper:512: After database mapping: 350 mentions (ALL KEPT - no quality filtering)
2025-05-29 17:07:21,811 [INFO] scraper.reddit_scraper:524: Enhanced 350 mentions with sentiment analysis
2025-05-29 17:07:21,812 [INFO] scraper.reddit_scraper:538: Final result: 350 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 17:07:21,813 [INFO] scraper.reddit_scraper:544: 
        Scraping completed for 'amazon':
        - Duration: 15.72s
        - Pages scraped: 0
        - Mentions found: 350
        - Success rate: 0.00%
        - Errors: 0
        - Retries: 0
        - Rate limit hits: 0
        - Cache hits: 0
        
2025-05-29 17:07:21,814 [INFO] analytics.data_validator:236: Validating dataset of 350 mentions
2025-05-29 17:07:25,159 [INFO] analytics.data_validator:259: Validation complete: 350/350 mentions passed
2025-05-29 17:07:25,160 [INFO] app:742: Data validation: 350/350 mentions passed
2025-05-29 17:07:26,260 [INFO] app:846: Search completed: amazon -> 350 mentions
2025-05-29 17:07:26,437 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:07:31,969 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:07:33,251 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:07:33,259 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:08:01,368 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:08:01,529 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:08:02,290 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:08:02,437 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:08:06,394 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:08:06,396 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:08:38,078 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:09:44,181 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:10:50,279 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:11:24,715 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:11:24,717 [INFO] app:605: Cache hit for search term: amazon
2025-05-29 17:11:24,817 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:11:24,872 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:11:24,878 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:11:32,296 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:11:32,438 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:11:56,392 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:12:32,599 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:12:32,747 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:13:02,503 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:13:32,307 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:13:32,459 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:14:08,615 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:15:02,729 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 905038650.00 (avg: 901663834.88, std: 1020441.43)
2025-05-29 17:18:16,662 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:18:16,663 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:18:16,663 [INFO] scraper.reddit_scraper:294: HTTP session initialized successfully
2025-05-29 17:18:16,663 [INFO] scraper.reddit_scraper:277: Reddit scraper initialized with API-only configuration
2025-05-29 17:18:16,663 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 17:18:16,664 [INFO] app:392: Database initialized successfully
2025-05-29 17:18:16,667 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 17:18:16,668 [INFO] app:258: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 17:18:16,767 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 17:18:16,771 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:18:16,772 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 17:18:16,772 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 17:18:16,807 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:18:16,809 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 17:18:16,903 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:18:16,905 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:18:17,001 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E011D008F0>
2025-05-29 17:18:17,002 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E010138450> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 17:18:17,084 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E011D00860>
2025-05-29 17:18:17,085 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:18:17,085 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:18:17,085 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:18:17,085 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:18:17,085 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:18:17,160 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 11:48:16 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 17:18:17,160 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 17:18:17,161 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:18:17,161 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:18:17,161 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:18:17,161 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:18:17,161 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:18:17,162 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:18:17,202 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E011D30E30>
2025-05-29 17:18:17,202 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E010072F50> server_hostname='api.gradio.app' timeout=3
2025-05-29 17:18:17,213 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 17:18:17,679 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 17:18:17,757 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E011D30D40>
2025-05-29 17:18:17,758 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:18:17,758 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:18:17,758 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:18:17,758 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:18:17,759 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:18:18,037 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 11:48:17 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 17:18:18,037 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 17:18:18,038 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:18:18,038 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:18:18,038 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:18:18,038 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:18:18,038 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:18:18,038 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:18:18,686 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:18:19,239 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E011F73260>
2025-05-29 17:18:19,239 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:18:19,240 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:18:19,240 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:18:19,240 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:18:19,240 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:18:19,241 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 11:48:19 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 17:18:19,241 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 17:18:19,241 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:18:19,241 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:18:19,241 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:18:19,242 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:18:19,242 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:18:19,242 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:18:19,249 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 17:18:21,298 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E01193AD80>
2025-05-29 17:18:21,298 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 17:18:21,298 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:18:21,299 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 17:18:21,299 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:18:21,299 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 17:18:21,307 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 11:48:20 GMT'), (b'server', b'uvicorn'), (b'content-length', b'68743'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 17:18:21,307 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 17:18:21,308 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 17:18:21,308 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:18:21,308 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:18:21,308 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:18:21,308 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:18:21,309 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:18:27,307 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:18:27,307 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:18:27,308 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:18:35,638 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:18:35,644 [INFO] scraper.reddit_scraper:403: Starting Reddit API search for: openai
2025-05-29 17:18:35,644 [INFO] scraper.reddit_scraper.APIClient:97: [SEARCH] Starting COMPREHENSIVE search for 'openai' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 17:18:35,644 [INFO] scraper.reddit_scraper.APIClient:112: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 17:18:35,644 [INFO] scraper.reddit_scraper.APIClient:116:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 17:18:37,835 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 93 posts with sort=relevance, time=week. New unique: 93. Total unique: 93
2025-05-29 17:18:37,836 [INFO] scraper.reddit_scraper.APIClient:116:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 17:18:40,150 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 90 posts with sort=top, time=week. New unique: 59. Total unique: 152
2025-05-29 17:18:40,150 [INFO] scraper.reddit_scraper.APIClient:116:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 17:18:42,183 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=hot, time=week. New unique: 57. Total unique: 209
2025-05-29 17:18:42,184 [INFO] scraper.reddit_scraper.APIClient:116:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 17:18:44,361 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 89. Total unique: 298
2025-05-29 17:18:44,362 [INFO] scraper.reddit_scraper.APIClient:116:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 17:18:45,989 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 93 posts with sort=relevance, time=week. New unique: 0. Total unique: 298
2025-05-29 17:18:45,989 [INFO] scraper.reddit_scraper.APIClient:116:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 17:18:47,688 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 298
2025-05-29 17:18:47,688 [INFO] scraper.reddit_scraper.APIClient:143: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'openai':
[STATS] Total unique posts found: 298
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 147
[RESULT] SUCCESS: Found 298 posts that will be processed and saved

2025-05-29 17:18:47,689 [INFO] scraper.reddit_scraper:415: Reddit API returned 298 mentions
2025-05-29 17:18:47,689 [INFO] scraper.reddit_scraper:487: Post-processing 298 mentions...
2025-05-29 17:18:47,690 [INFO] scraper.reddit_scraper:589: Deduplication: 298 -> 298 mentions (removed 0 exact duplicates)
2025-05-29 17:18:47,690 [INFO] scraper.reddit_scraper:491: After deduplication: 298 mentions
2025-05-29 17:18:47,690 [INFO] scraper.reddit_scraper:653: Validation: 298 raw mentions  298 kept (EXTREMELY LENIENT - keeping everything possible)
2025-05-29 17:18:47,690 [INFO] scraper.reddit_scraper:495: After validation: 298 mentions
2025-05-29 17:18:47,875 [INFO] scraper.reddit_scraper:512: After database mapping: 298 mentions (ALL KEPT - no quality filtering)
2025-05-29 17:18:47,875 [INFO] scraper.reddit_scraper:524: Enhanced 298 mentions with sentiment analysis
2025-05-29 17:18:47,875 [INFO] scraper.reddit_scraper:538: Final result: 298 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 17:18:47,877 [INFO] scraper.reddit_scraper:544: 
        Scraping completed for 'openai':
        - Duration: 12.23s
        - Pages scraped: 0
        - Mentions found: 298
        - Success rate: 0.00%
        - Errors: 0
        - Retries: 0
        - Rate limit hits: 0
        - Cache hits: 0
        
2025-05-29 17:18:47,878 [INFO] analytics.data_validator:236: Validating dataset of 298 mentions
2025-05-29 17:18:51,018 [INFO] analytics.data_validator:259: Validation complete: 298/298 mentions passed
2025-05-29 17:18:51,018 [INFO] app:763: Data validation: 298/298 mentions passed
2025-05-29 17:18:51,990 [INFO] app:867: Search completed: openai -> 298 mentions
2025-05-29 17:18:52,166 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:19:04,586 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:19:04,594 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:19:24,047 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:19:24,206 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:19:24,845 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:19:27,392 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:19:27,540 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:19:30,514 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:19:30,516 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:19:45,871 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:19:45,873 [INFO] app:605: Cache hit for search term: openai
2025-05-29 17:19:46,037 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:19:46,088 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:19:46,095 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:20:27,697 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:20:27,848 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:20:29,181 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:20:29,185 [INFO] scraper.reddit_scraper:403: Starting Reddit API search for: hello
2025-05-29 17:20:29,185 [INFO] scraper.reddit_scraper.APIClient:97: [SEARCH] Starting COMPREHENSIVE search for 'hello' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 17:20:29,185 [INFO] scraper.reddit_scraper.APIClient:112: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 17:20:29,185 [INFO] scraper.reddit_scraper.APIClient:116:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 17:20:31,011 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:20:32,010 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 95 posts with sort=relevance, time=week. New unique: 95. Total unique: 95
2025-05-29 17:20:32,010 [INFO] scraper.reddit_scraper.APIClient:116:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 17:20:34,906 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 96 posts with sort=top, time=week. New unique: 68. Total unique: 163
2025-05-29 17:20:34,906 [INFO] scraper.reddit_scraper.APIClient:116:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 17:20:37,169 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=hot, time=week. New unique: 94. Total unique: 257
2025-05-29 17:20:37,169 [INFO] scraper.reddit_scraper.APIClient:116:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 17:20:39,076 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 100. Total unique: 357
2025-05-29 17:20:39,077 [INFO] scraper.reddit_scraper.APIClient:116:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 17:20:41,657 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 95 posts with sort=relevance, time=week. New unique: 0. Total unique: 357
2025-05-29 17:20:41,657 [INFO] scraper.reddit_scraper.APIClient:116:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 17:20:43,380 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 357
2025-05-29 17:20:43,380 [INFO] scraper.reddit_scraper.APIClient:143: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'hello':
[STATS] Total unique posts found: 357
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 327
[RESULT] SUCCESS: Found 357 posts that will be processed and saved

2025-05-29 17:20:43,381 [INFO] scraper.reddit_scraper:415: Reddit API returned 357 mentions
2025-05-29 17:20:43,381 [INFO] scraper.reddit_scraper:487: Post-processing 357 mentions...
2025-05-29 17:20:43,381 [INFO] scraper.reddit_scraper:589: Deduplication: 357 -> 357 mentions (removed 0 exact duplicates)
2025-05-29 17:20:43,381 [INFO] scraper.reddit_scraper:491: After deduplication: 357 mentions
2025-05-29 17:20:43,382 [INFO] scraper.reddit_scraper:653: Validation: 357 raw mentions  357 kept (EXTREMELY LENIENT - keeping everything possible)
2025-05-29 17:20:43,382 [INFO] scraper.reddit_scraper:495: After validation: 357 mentions
2025-05-29 17:20:43,539 [INFO] scraper.reddit_scraper:512: After database mapping: 357 mentions (ALL KEPT - no quality filtering)
2025-05-29 17:20:43,539 [INFO] scraper.reddit_scraper:524: Enhanced 357 mentions with sentiment analysis
2025-05-29 17:20:43,539 [INFO] scraper.reddit_scraper:538: Final result: 357 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 17:20:43,541 [INFO] scraper.reddit_scraper:544: 
        Scraping completed for 'hello':
        - Duration: 14.36s
        - Pages scraped: 0
        - Mentions found: 357
        - Success rate: 0.00%
        - Errors: 0
        - Retries: 0
        - Rate limit hits: 0
        - Cache hits: 0
        
2025-05-29 17:20:43,542 [INFO] analytics.data_validator:236: Validating dataset of 357 mentions
2025-05-29 17:20:46,972 [INFO] analytics.data_validator:259: Validation complete: 357/357 mentions passed
2025-05-29 17:20:46,972 [INFO] app:763: Data validation: 357/357 mentions passed
2025-05-29 17:20:48,302 [INFO] app:867: Search completed: hello -> 357 mentions
2025-05-29 17:20:48,358 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:20:49,398 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:20:49,405 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:21:27,377 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:21:27,530 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:21:36,087 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.65 (avg: 93.65, std: 0.00)
2025-05-29 17:21:36,088 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.01 (avg: 121.03, std: 0.01)
2025-05-29 17:21:37,096 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:22:41,694 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:22:41,862 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:22:43,235 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:22:57,402 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:22:57,561 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:23:44,257 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:23:44,260 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:23:49,375 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:24:21,421 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 913051947.00 (avg: 910814643.35, std: 644090.00)
2025-05-29 17:24:55,480 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:26:01,591 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:31:48,317 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:31:48,318 [INFO] scraper.reddit_scraper:294: HTTP session initialized successfully
2025-05-29 17:31:48,318 [INFO] scraper.reddit_scraper:277: Reddit scraper initialized with API-only configuration
2025-05-29 17:31:48,318 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 17:31:48,319 [INFO] app:388: Database initialized successfully
2025-05-29 17:31:48,321 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 17:31:48,322 [INFO] app:254: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 17:31:48,424 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 17:31:48,427 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:31:48,428 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 17:31:48,428 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 17:31:48,467 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:31:48,469 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 17:31:48,588 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:31:48,589 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:31:48,873 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 17:31:48,929 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000012F29A69BB0>
2025-05-29 17:31:48,929 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000012F27F1DC50> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 17:31:48,971 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000012F29A87590>
2025-05-29 17:31:48,971 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000012F27F1C4D0> server_hostname='api.gradio.app' timeout=3
2025-05-29 17:31:49,336 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.66 (threshold: 85)
2025-05-29 17:31:50,341 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000012F29B765A0>
2025-05-29 17:31:50,341 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:31:50,342 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000012F29AD16D0>
2025-05-29 17:31:50,342 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:31:50,342 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:31:50,342 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:31:50,343 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:31:50,343 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:31:50,343 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:31:50,343 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:31:50,343 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:31:50,343 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:31:50,343 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:31:50,574 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 12:01:50 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 17:31:50,575 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 17:31:50,575 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:31:50,575 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:31:50,576 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:31:50,576 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:31:50,576 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:31:50,576 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:31:50,619 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 12:01:50 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 17:31:50,619 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 17:31:50,619 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:31:50,620 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:31:50,620 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:31:50,620 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:31:50,620 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:31:50,620 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:31:50,904 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000012F29982090>
2025-05-29 17:31:50,904 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:31:50,904 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:31:50,905 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:31:50,905 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:31:50,905 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:31:50,906 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:01:50 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 17:31:50,906 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 17:31:50,906 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:31:50,906 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:31:50,906 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:31:50,906 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:31:50,907 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:31:50,907 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:31:50,913 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 17:31:52,942 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000012F1583F140>
2025-05-29 17:31:52,942 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 17:31:52,943 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:31:52,943 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 17:31:52,943 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:31:52,943 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 17:31:52,953 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:01:51 GMT'), (b'server', b'uvicorn'), (b'content-length', b'65609'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 17:31:52,953 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 17:31:52,954 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 17:31:52,954 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:31:52,954 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:31:52,954 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:31:52,954 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:31:52,954 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:31:59,932 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:31:59,932 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:31:59,933 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:32:03,103 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:32:03,107 [INFO] app:1571: Search history cleared: 1001 mentions, 4 sessions
2025-05-29 17:32:03,109 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:32:08,816 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:32:08,823 [INFO] scraper.reddit_scraper:384: Found 298 cached mentions for 'openai'
2025-05-29 17:32:08,823 [INFO] scraper.reddit_scraper:782: Cleaned 298 cached mentions -> 298 valid mentions
2025-05-29 17:32:08,824 [INFO] analytics.data_validator:236: Validating dataset of 298 mentions
2025-05-29 17:32:12,067 [INFO] analytics.data_validator:259: Validation complete: 298/298 mentions passed
2025-05-29 17:32:12,068 [INFO] app:661: Data validation: 298/298 mentions passed
2025-05-29 17:32:13,039 [INFO] app:791: Search completed: openai -> 298 mentions
2025-05-29 17:32:13,199 [ERROR] app:1413: Visualization generation failed: 'EnhancedRedditMentionTracker' object has no attribute '_get_top_mentions_df'
2025-05-29 17:32:13,200 [ERROR] app:1451: Search failed: 'EnhancedRedditMentionTracker' object has no attribute '_generate_temporal_analysis'
2025-05-29 17:32:13,209 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:32:18,321 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:32:18,328 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:32:56,517 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:33:00,007 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:33:00,174 [DEBUG] app:1547: Monitor refresh completed successfully
2025-05-29 17:33:00,177 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:34:02,654 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:35:08,775 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:36:11,597 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:36:11,597 [INFO] scraper.reddit_scraper:294: HTTP session initialized successfully
2025-05-29 17:36:11,598 [INFO] scraper.reddit_scraper:277: Reddit scraper initialized with API-only configuration
2025-05-29 17:36:11,598 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 17:36:11,599 [INFO] app:388: Database initialized successfully
2025-05-29 17:36:11,601 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 17:36:11,602 [INFO] app:254: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 17:36:11,695 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 17:36:11,700 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:36:11,701 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 17:36:11,701 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 17:36:11,736 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:36:11,738 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 17:36:11,844 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:36:11,844 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:36:11,941 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028BABA8CBC0>
2025-05-29 17:36:11,942 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000028BA9E3F350> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 17:36:12,027 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028BABA8CAA0>
2025-05-29 17:36:12,027 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:36:12,028 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:36:12,028 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:36:12,028 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:36:12,028 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:36:12,112 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 12:06:11 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 17:36:12,112 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 17:36:12,112 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:36:12,113 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:36:12,113 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:36:12,113 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:36:12,113 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:36:12,113 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:36:12,131 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028BABA4ABD0>
2025-05-29 17:36:12,131 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000028BA9E3DBD0> server_hostname='api.gradio.app' timeout=3
2025-05-29 17:36:12,142 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 17:36:12,612 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 17:36:12,690 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028BABA6FB30>
2025-05-29 17:36:12,690 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:36:12,690 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:36:12,690 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:36:12,691 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:36:12,691 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:36:12,965 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 12:06:12 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 17:36:12,965 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 17:36:12,965 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:36:12,966 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:36:12,966 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:36:12,966 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:36:12,967 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:36:12,967 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:36:13,620 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:36:14,171 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028BABCC9F10>
2025-05-29 17:36:14,171 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:36:14,172 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:36:14,172 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:36:14,173 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:36:14,173 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:36:14,173 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:06:14 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 17:36:14,173 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 17:36:14,173 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:36:14,174 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:36:14,174 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:36:14,174 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:36:14,174 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:36:14,174 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:36:14,180 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 17:36:16,215 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028BABD210D0>
2025-05-29 17:36:16,215 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 17:36:16,216 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:36:16,216 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 17:36:16,216 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:36:16,217 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 17:36:16,225 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:06:15 GMT'), (b'server', b'uvicorn'), (b'content-length', b'65465'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 17:36:16,226 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 17:36:16,226 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 17:36:16,226 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:36:16,226 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:36:16,226 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:36:16,226 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:36:16,227 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:36:19,486 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:36:19,487 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:36:19,488 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:36:21,533 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:36:21,537 [INFO] app:1571: Search history cleared: 298 mentions, 1 sessions
2025-05-29 17:36:21,539 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:36:25,991 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:36:25,998 [INFO] scraper.reddit_scraper:384: Found 298 cached mentions for 'openai'
2025-05-29 17:36:25,999 [INFO] scraper.reddit_scraper:782: Cleaned 298 cached mentions -> 298 valid mentions
2025-05-29 17:36:25,999 [INFO] analytics.data_validator:236: Validating dataset of 298 mentions
2025-05-29 17:36:29,165 [INFO] analytics.data_validator:259: Validation complete: 298/298 mentions passed
2025-05-29 17:36:29,165 [INFO] app:661: Data validation: 298/298 mentions passed
2025-05-29 17:36:30,148 [INFO] app:791: Search completed: openai -> 298 mentions
2025-05-29 17:36:30,321 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:36:34,055 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:36:34,063 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:36:49,590 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:36:49,763 [DEBUG] app:1547: Monitor refresh completed successfully
2025-05-29 17:36:49,767 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:36:54,003 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:36:54,009 [INFO] scraper.reddit_scraper:384: Found 350 cached mentions for 'amazon'
2025-05-29 17:36:54,009 [INFO] scraper.reddit_scraper:782: Cleaned 350 cached mentions -> 350 valid mentions
2025-05-29 17:36:54,009 [INFO] analytics.data_validator:236: Validating dataset of 350 mentions
2025-05-29 17:36:57,386 [INFO] analytics.data_validator:259: Validation complete: 350/350 mentions passed
2025-05-29 17:36:57,386 [INFO] app:661: Data validation: 350/350 mentions passed
2025-05-29 17:36:58,590 [INFO] app:791: Search completed: amazon -> 350 mentions
2025-05-29 17:36:58,655 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:36:58,718 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:36:58,724 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:37:16,654 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:37:16,658 [INFO] scraper.reddit_scraper:403: Starting Reddit API search for: navya
2025-05-29 17:37:16,658 [INFO] scraper.reddit_scraper.APIClient:97: [SEARCH] Starting COMPREHENSIVE search for 'navya' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 17:37:16,658 [INFO] scraper.reddit_scraper.APIClient:112: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 17:37:16,658 [INFO] scraper.reddit_scraper.APIClient:116:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 17:37:17,547 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 12 posts with sort=relevance, time=week. New unique: 12. Total unique: 12
2025-05-29 17:37:17,547 [INFO] scraper.reddit_scraper.APIClient:116:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 17:37:18,499 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 8 posts with sort=top, time=week. New unique: 0. Total unique: 12
2025-05-29 17:37:18,499 [INFO] scraper.reddit_scraper.APIClient:116:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 17:37:19,316 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 5 posts with sort=hot, time=week. New unique: 0. Total unique: 12
2025-05-29 17:37:19,317 [INFO] scraper.reddit_scraper.APIClient:116:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 17:37:19,764 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:37:20,434 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 8 posts with sort=new, time=week. New unique: 0. Total unique: 12
2025-05-29 17:37:20,435 [INFO] scraper.reddit_scraper.APIClient:116:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 17:37:21,829 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 12 posts with sort=relevance, time=week. New unique: 0. Total unique: 12
2025-05-29 17:37:21,829 [INFO] scraper.reddit_scraper.APIClient:116:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 17:37:22,572 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 8 posts with sort=new, time=week. New unique: 0. Total unique: 12
2025-05-29 17:37:22,573 [INFO] scraper.reddit_scraper.APIClient:143: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'navya':
[STATS] Total unique posts found: 12
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 12
[RESULT] SUCCESS: Found 12 posts that will be processed and saved

2025-05-29 17:37:22,573 [INFO] scraper.reddit_scraper:415: Reddit API returned 12 mentions
2025-05-29 17:37:22,573 [INFO] scraper.reddit_scraper:487: Post-processing 12 mentions...
2025-05-29 17:37:22,573 [INFO] scraper.reddit_scraper:589: Deduplication: 12 -> 12 mentions (removed 0 exact duplicates)
2025-05-29 17:37:22,573 [INFO] scraper.reddit_scraper:491: After deduplication: 12 mentions
2025-05-29 17:37:22,574 [INFO] scraper.reddit_scraper:653: Validation: 12 raw mentions  12 kept (EXTREMELY LENIENT - keeping everything possible)
2025-05-29 17:37:22,574 [INFO] scraper.reddit_scraper:495: After validation: 12 mentions
2025-05-29 17:37:22,579 [INFO] scraper.reddit_scraper:512: After database mapping: 12 mentions (ALL KEPT - no quality filtering)
2025-05-29 17:37:22,579 [INFO] scraper.reddit_scraper:524: Enhanced 12 mentions with sentiment analysis
2025-05-29 17:37:22,579 [INFO] scraper.reddit_scraper:538: Final result: 12 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 17:37:22,580 [INFO] scraper.reddit_scraper:544: 
        Scraping completed for 'navya':
        - Duration: 5.92s
        - Pages scraped: 0
        - Mentions found: 12
        - Success rate: 0.00%
        - Errors: 0
        - Retries: 0
        - Rate limit hits: 0
        - Cache hits: 0
        
2025-05-29 17:37:22,580 [INFO] analytics.data_validator:236: Validating dataset of 12 mentions
2025-05-29 17:37:22,671 [INFO] analytics.data_validator:259: Validation complete: 12/12 mentions passed
2025-05-29 17:37:22,671 [INFO] app:661: Data validation: 12/12 mentions passed
2025-05-29 17:37:22,735 [INFO] app:791: Search completed: navya -> 12 mentions
2025-05-29 17:37:22,790 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:37:22,841 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:37:22,994 [DEBUG] app:1547: Monitor refresh completed successfully
2025-05-29 17:37:22,996 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:37:49,858 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:37:50,007 [DEBUG] app:1547: Monitor refresh completed successfully
2025-05-29 17:37:50,009 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:37:53,579 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:37:53,596 [INFO] app:967: Data exported successfully to C:\Users\Devansh\Downloads\intern_application_round1\exports\reddit_mentions_1_20250529_120753.csv (size: 431942 bytes)
2025-05-29 17:37:53,612 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:38:25,884 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:38:49,575 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:38:49,728 [DEBUG] app:1547: Monitor refresh completed successfully
2025-05-29 17:38:49,730 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:38:57,929 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 932641458.00 (avg: 931767520.69, std: 240888.70)
2025-05-29 17:39:19,862 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:39:20,015 [DEBUG] app:1547: Monitor refresh completed successfully
2025-05-29 17:39:20,017 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:39:31,990 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:39:49,546 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:39:49,695 [DEBUG] app:1547: Monitor refresh completed successfully
2025-05-29 17:39:49,698 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:40:38,094 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:41:44,186 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:44:14,546 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:44:14,547 [INFO] scraper.reddit_scraper:294: HTTP session initialized successfully
2025-05-29 17:44:14,547 [INFO] scraper.reddit_scraper:277: Reddit scraper initialized with API-only configuration
2025-05-29 17:44:14,547 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 17:44:14,548 [INFO] app:388: Database initialized successfully
2025-05-29 17:44:14,551 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 17:44:14,551 [INFO] app:254: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 17:44:14,645 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 17:44:14,649 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:44:14,650 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 17:44:14,650 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 17:44:14,685 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:44:14,686 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 17:44:14,789 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:44:14,800 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:44:14,886 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015A6EBB91F0>
2025-05-29 17:44:14,886 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015A6CF1B850> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 17:44:14,964 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015A6EBB9100>
2025-05-29 17:44:14,965 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:44:14,965 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:44:14,965 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:44:14,966 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:44:14,966 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:44:15,039 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 12:14:14 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 17:44:15,040 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 17:44:15,040 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:44:15,040 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:44:15,040 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:44:15,040 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:44:15,041 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:44:15,041 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:44:15,085 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 17:44:15,090 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015A6EB2B1A0>
2025-05-29 17:44:15,090 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000015A6CF1A0D0> server_hostname='api.gradio.app' timeout=3
2025-05-29 17:44:15,561 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 17:44:15,665 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015A6ED3DAC0>
2025-05-29 17:44:15,665 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:44:15,666 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:44:15,666 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:44:15,666 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:44:15,666 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:44:15,954 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 12:14:15 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 17:44:15,954 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 17:44:15,954 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:44:15,954 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:44:15,954 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:44:15,954 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:44:15,954 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:44:15,954 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:44:16,570 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:44:17,115 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015A6EDA9D90>
2025-05-29 17:44:17,115 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:44:17,116 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:44:17,116 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:44:17,116 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:44:17,116 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:44:17,117 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:14:17 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 17:44:17,117 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 17:44:17,117 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:44:17,117 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:44:17,117 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:44:17,118 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:44:17,118 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:44:17,118 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:44:17,124 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 17:44:19,150 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000015A6EEE0F20>
2025-05-29 17:44:19,150 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 17:44:19,151 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:44:19,151 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 17:44:19,151 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:44:19,151 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 17:44:19,159 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:14:18 GMT'), (b'server', b'uvicorn'), (b'content-length', b'65561'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 17:44:19,159 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 17:44:19,160 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 17:44:19,160 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:44:19,160 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:44:19,160 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:44:19,160 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:44:19,160 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:44:49,054 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:44:49,054 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:44:49,056 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:44:53,444 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:44:53,450 [INFO] scraper.reddit_scraper:384: Found 298 cached mentions for 'openai'
2025-05-29 17:44:53,451 [INFO] scraper.reddit_scraper:782: Cleaned 298 cached mentions -> 298 valid mentions
2025-05-29 17:44:53,451 [INFO] analytics.data_validator:236: Validating dataset of 298 mentions
2025-05-29 17:44:56,684 [INFO] analytics.data_validator:259: Validation complete: 298/298 mentions passed
2025-05-29 17:44:56,684 [INFO] app:661: Data validation: 298/298 mentions passed
2025-05-29 17:44:57,719 [INFO] app:791: Search completed: openai -> 298 mentions
2025-05-29 17:44:57,908 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:44:57,959 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:44:57,965 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:45:06,834 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:45:06,838 [INFO] scraper.reddit_scraper:403: Starting Reddit API search for: mazda
2025-05-29 17:45:06,838 [INFO] scraper.reddit_scraper.APIClient:97: [SEARCH] Starting COMPREHENSIVE search for 'mazda' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 17:45:06,838 [INFO] scraper.reddit_scraper.APIClient:112: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 17:45:06,838 [INFO] scraper.reddit_scraper.APIClient:116:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 17:45:10,468 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 94 posts with sort=relevance, time=week. New unique: 94. Total unique: 94
2025-05-29 17:45:10,468 [INFO] scraper.reddit_scraper.APIClient:116:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 17:45:13,189 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 95 posts with sort=top, time=week. New unique: 62. Total unique: 156
2025-05-29 17:45:13,190 [INFO] scraper.reddit_scraper.APIClient:116:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 17:45:16,008 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=hot, time=week. New unique: 63. Total unique: 219
2025-05-29 17:45:16,009 [INFO] scraper.reddit_scraper.APIClient:116:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 17:45:18,460 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 92. Total unique: 311
2025-05-29 17:45:18,461 [INFO] scraper.reddit_scraper.APIClient:116:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 17:45:21,078 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 94 posts with sort=relevance, time=week. New unique: 0. Total unique: 311
2025-05-29 17:45:21,079 [INFO] scraper.reddit_scraper.APIClient:116:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 17:45:22,669 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:45:23,436 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 311
2025-05-29 17:45:23,437 [INFO] scraper.reddit_scraper.APIClient:143: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'mazda':
[STATS] Total unique posts found: 311
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 118
[RESULT] SUCCESS: Found 311 posts that will be processed and saved

2025-05-29 17:45:23,437 [INFO] scraper.reddit_scraper:415: Reddit API returned 311 mentions
2025-05-29 17:45:23,437 [INFO] scraper.reddit_scraper:487: Post-processing 311 mentions...
2025-05-29 17:45:23,437 [INFO] scraper.reddit_scraper:589: Deduplication: 311 -> 311 mentions (removed 0 exact duplicates)
2025-05-29 17:45:23,438 [INFO] scraper.reddit_scraper:491: After deduplication: 311 mentions
2025-05-29 17:45:23,438 [INFO] scraper.reddit_scraper:653: Validation: 311 raw mentions  311 kept (EXTREMELY LENIENT - keeping everything possible)
2025-05-29 17:45:23,438 [INFO] scraper.reddit_scraper:495: After validation: 311 mentions
2025-05-29 17:45:23,544 [INFO] scraper.reddit_scraper:512: After database mapping: 311 mentions (ALL KEPT - no quality filtering)
2025-05-29 17:45:23,545 [INFO] scraper.reddit_scraper:524: Enhanced 311 mentions with sentiment analysis
2025-05-29 17:45:23,545 [INFO] scraper.reddit_scraper:538: Final result: 311 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 17:45:23,546 [INFO] scraper.reddit_scraper:544: 
        Scraping completed for 'mazda':
        - Duration: 16.71s
        - Pages scraped: 0
        - Mentions found: 311
        - Success rate: 0.00%
        - Errors: 0
        - Retries: 0
        - Rate limit hits: 0
        - Cache hits: 0
        
2025-05-29 17:45:23,547 [INFO] analytics.data_validator:236: Validating dataset of 311 mentions
2025-05-29 17:45:25,492 [INFO] analytics.data_validator:259: Validation complete: 311/311 mentions passed
2025-05-29 17:45:25,492 [INFO] app:661: Data validation: 311/311 mentions passed
2025-05-29 17:45:26,414 [INFO] app:791: Search completed: mazda -> 311 mentions
2025-05-29 17:45:26,474 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:45:26,837 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:45:26,846 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:45:48,974 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:45:49,134 [DEBUG] app:1553: Monitor refresh completed successfully
2025-05-29 17:45:49,136 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:45:49,239 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:45:49,393 [DEBUG] app:1553: Monitor refresh completed successfully
2025-05-29 17:45:49,395 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:45:51,302 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:45:51,303 [WARNING] app:893: No mentions found for session 1
2025-05-29 17:45:51,304 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:46:28,755 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:46:32,829 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:46:32,832 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:47:00,807 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 939533031.00 (avg: 939192561.94, std: 112088.37)
2025-05-29 17:47:34,869 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:47:44,870 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 940569032.00 (avg: 939323102.00, std: 344262.75)
2025-05-29 17:48:06,908 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 941557739.00 (avg: 939481823.18, std: 627611.28)
2025-05-29 17:51:24,650 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:51:24,650 [INFO] scraper.reddit_scraper:294: HTTP session initialized successfully
2025-05-29 17:51:24,651 [INFO] scraper.reddit_scraper:277: Reddit scraper initialized with API-only configuration
2025-05-29 17:51:24,651 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 17:51:24,651 [INFO] app:388: Database initialized successfully
2025-05-29 17:51:24,654 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 17:51:24,655 [INFO] app:254: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 17:51:24,749 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 17:51:24,752 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:51:24,753 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 17:51:24,753 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 17:51:24,788 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:51:24,789 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 17:51:24,907 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:51:24,907 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:51:25,019 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C0EB7DD310>
2025-05-29 17:51:25,019 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C0EB728050> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 17:51:25,099 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C0ED372CC0>
2025-05-29 17:51:25,099 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:51:25,099 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:51:25,099 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:51:25,100 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:51:25,100 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:51:25,176 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 12:21:24 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 17:51:25,176 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 17:51:25,177 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:51:25,177 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:51:25,177 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:51:25,177 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:51:25,177 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:51:25,178 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:51:25,188 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 17:51:25,189 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C0ED32FAD0>
2025-05-29 17:51:25,189 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000002C0EB66E850> server_hostname='api.gradio.app' timeout=3
2025-05-29 17:51:25,665 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 17:51:25,735 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C0ED372660>
2025-05-29 17:51:25,735 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:51:25,735 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:51:25,736 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:51:25,736 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:51:25,736 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:51:26,010 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 12:21:25 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 17:51:26,010 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 17:51:26,011 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:51:26,011 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:51:26,011 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:51:26,011 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:51:26,011 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:51:26,011 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:51:26,672 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:51:27,225 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C0ED506030>
2025-05-29 17:51:27,225 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:51:27,226 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:51:27,226 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:51:27,227 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:51:27,227 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:51:27,227 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:21:27 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 17:51:27,227 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 17:51:27,228 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:51:27,228 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:51:27,228 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:51:27,228 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:51:27,228 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:51:27,228 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:51:27,238 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 17:51:29,278 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002C0ED526300>
2025-05-29 17:51:29,278 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 17:51:29,278 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:51:29,279 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 17:51:29,279 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:51:29,279 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 17:51:29,287 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:21:28 GMT'), (b'server', b'uvicorn'), (b'content-length', b'65656'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 17:51:29,288 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 17:51:29,288 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 17:51:29,288 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:51:29,288 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:51:29,289 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:51:29,289 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:51:29,289 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:51:35,494 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:51:35,495 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:51:35,496 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:51:42,748 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:51:42,754 [INFO] scraper.reddit_scraper:403: Starting Reddit API search for: python
2025-05-29 17:51:42,754 [INFO] scraper.reddit_scraper.APIClient:97: [SEARCH] Starting COMPREHENSIVE search for 'python' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 17:51:42,754 [INFO] scraper.reddit_scraper.APIClient:112: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 17:51:42,755 [INFO] scraper.reddit_scraper.APIClient:116:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 17:51:45,160 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 97 posts with sort=relevance, time=week. New unique: 97. Total unique: 97
2025-05-29 17:51:45,161 [INFO] scraper.reddit_scraper.APIClient:116:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 17:51:47,556 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 91 posts with sort=top, time=week. New unique: 69. Total unique: 166
2025-05-29 17:51:47,556 [INFO] scraper.reddit_scraper.APIClient:116:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 17:51:49,195 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=hot, time=week. New unique: 78. Total unique: 244
2025-05-29 17:51:49,195 [INFO] scraper.reddit_scraper.APIClient:116:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 17:51:51,141 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 94. Total unique: 338
2025-05-29 17:51:51,141 [INFO] scraper.reddit_scraper.APIClient:116:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 17:51:52,810 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 97 posts with sort=relevance, time=week. New unique: 0. Total unique: 338
2025-05-29 17:51:52,811 [INFO] scraper.reddit_scraper.APIClient:116:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 17:51:54,469 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 338
2025-05-29 17:51:54,470 [INFO] scraper.reddit_scraper.APIClient:143: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'python':
[STATS] Total unique posts found: 338
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 200
[RESULT] SUCCESS: Found 338 posts that will be processed and saved

2025-05-29 17:51:54,470 [INFO] scraper.reddit_scraper:415: Reddit API returned 338 mentions
2025-05-29 17:51:54,471 [INFO] scraper.reddit_scraper:487: Post-processing 338 mentions...
2025-05-29 17:51:54,471 [INFO] scraper.reddit_scraper:589: Deduplication: 338 -> 338 mentions (removed 0 exact duplicates)
2025-05-29 17:51:54,471 [INFO] scraper.reddit_scraper:491: After deduplication: 338 mentions
2025-05-29 17:51:54,472 [INFO] scraper.reddit_scraper:653: Validation: 338 raw mentions  338 kept (EXTREMELY LENIENT - keeping everything possible)
2025-05-29 17:51:54,472 [INFO] scraper.reddit_scraper:495: After validation: 338 mentions
2025-05-29 17:51:54,657 [INFO] scraper.reddit_scraper:512: After database mapping: 338 mentions (ALL KEPT - no quality filtering)
2025-05-29 17:51:54,657 [INFO] scraper.reddit_scraper:524: Enhanced 338 mentions with sentiment analysis
2025-05-29 17:51:54,657 [INFO] scraper.reddit_scraper:538: Final result: 338 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 17:51:54,659 [INFO] scraper.reddit_scraper:544: 
        Scraping completed for 'python':
        - Duration: 11.90s
        - Pages scraped: 0
        - Mentions found: 338
        - Success rate: 0.00%
        - Errors: 0
        - Retries: 0
        - Rate limit hits: 0
        - Cache hits: 0
        
2025-05-29 17:51:54,659 [INFO] analytics.data_validator:236: Validating dataset of 338 mentions
2025-05-29 17:51:58,099 [INFO] analytics.data_validator:259: Validation complete: 338/338 mentions passed
2025-05-29 17:51:58,099 [INFO] app:661: Data validation: 338/338 mentions passed
2025-05-29 17:51:59,490 [INFO] app:721: Initial metrics from analyzer: ['overview', 'engagement', 'temporal', 'subreddit_analysis', 'sentiment', 'content_analysis', 'trending', 'author_analysis', 'performance_analysis', 'quality_analysis', 'competition_analysis']
2025-05-29 17:51:59,490 [INFO] app:752: Final metrics structure: ['overview', 'engagement', 'temporal', 'subreddit_analysis', 'sentiment', 'content_analysis', 'trending', 'author_analysis', 'performance_analysis', 'quality_analysis', 'competition_analysis']
2025-05-29 17:51:59,490 [INFO] app:756: Temporal metrics keys: ['daily_timeline', 'hourly_distribution', 'day_of_week_distribution', 'trend', 'peak_day', 'peak_hour']
2025-05-29 17:51:59,490 [INFO] app:758: Author analysis keys: ['total_unique_authors', 'single_post_authors', 'multi_post_authors', 'author_diversity_ratio', 'top_authors_by_posts', 'top_authors_by_engagement', 'avg_posts_per_author', 'most_active_author', 'most_active_author_posts']
2025-05-29 17:51:59,490 [INFO] app:760: Quality analysis keys: ['avg_title_length', 'avg_content_length', 'avg_title_quality', 'avg_content_quality', 'avg_overall_quality', 'quality_distribution', 'posts_with_questions', 'posts_with_numbers', 'posts_with_links', 'quality_vs_engagement_correlation', 'best_quality_posts']
2025-05-29 17:51:59,491 [INFO] app:810: Search completed: python -> 338 mentions
2025-05-29 17:51:59,733 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:51:59,774 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:51:59,786 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:52:05,566 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:52:05,734 [DEBUG] app:1598: Monitor refresh completed successfully
2025-05-29 17:52:05,737 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:52:30,827 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:52:30,829 [INFO] app:917: Found 0 total mentions for session 1
2025-05-29 17:52:30,838 [INFO] app:922: Total mentions in database: 1297
2025-05-29 17:52:30,842 [INFO] app:929: Using 100 most recent mentions for export
2025-05-29 17:52:30,849 [INFO] app:1012: Data exported successfully to C:\Users\Devansh\Downloads\intern_application_round1\exports\reddit_mentions_1_20250529_122230.csv (size: 159140 bytes)
2025-05-29 17:52:30,850 [INFO] app:1547: Export successful: reddit_mentions_1_20250529_122230.csv (159140 bytes)
2025-05-29 17:52:30,861 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:52:32,795 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:53:05,542 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:53:05,690 [DEBUG] app:1598: Monitor refresh completed successfully
2025-05-29 17:53:05,691 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:53:38,931 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:53:59,955 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 950202058.00 (avg: 948590171.07, std: 466903.33)
2025-05-29 17:54:10,978 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_usage: 93.65 (avg: 93.65, std: 0.00)
2025-05-29 17:54:10,978 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in disk_free: 121.01 (avg: 121.02, std: 0.00)
2025-05-29 17:54:45,046 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:55:41,663 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:55:41,663 [INFO] scraper.reddit_scraper:294: HTTP session initialized successfully
2025-05-29 17:55:41,663 [INFO] scraper.reddit_scraper:277: Reddit scraper initialized with API-only configuration
2025-05-29 17:55:41,664 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 17:55:41,664 [INFO] app:388: Database initialized successfully
2025-05-29 17:55:41,667 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 17:55:41,668 [INFO] app:254: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 17:55:41,763 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 17:55:41,766 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:55:41,767 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 17:55:41,767 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 17:55:41,807 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:55:41,809 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 17:55:41,904 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:55:41,904 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:55:41,997 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B6F614F260>
2025-05-29 17:55:41,998 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B6F44EFD50> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 17:55:42,075 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B6F614F320>
2025-05-29 17:55:42,075 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:55:42,076 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:55:42,076 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:55:42,076 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:55:42,076 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:55:42,151 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 12:25:41 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 17:55:42,151 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 17:55:42,152 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:55:42,152 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:55:42,152 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:55:42,152 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:55:42,152 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:55:42,153 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:55:42,201 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 17:55:42,254 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B6F614ECC0>
2025-05-29 17:55:42,254 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B6F44EE950> server_hostname='api.gradio.app' timeout=3
2025-05-29 17:55:42,679 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.66 (threshold: 85)
2025-05-29 17:55:42,852 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B6F6122210>
2025-05-29 17:55:42,852 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:55:42,852 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:55:42,852 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:55:42,852 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:55:42,852 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:55:43,139 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 12:25:42 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 17:55:43,140 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 17:55:43,140 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:55:43,140 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:55:43,140 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:55:43,141 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:55:43,141 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:55:43,141 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:55:43,687 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:55:44,235 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B6F63F20C0>
2025-05-29 17:55:44,235 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:55:44,236 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:55:44,236 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:55:44,236 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:55:44,237 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:55:44,237 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:25:44 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 17:55:44,237 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 17:55:44,237 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:55:44,237 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:55:44,237 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:55:44,238 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:55:44,238 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:55:44,238 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:55:44,243 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 17:55:46,300 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B6F64453D0>
2025-05-29 17:55:46,300 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 17:55:46,300 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:55:46,301 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 17:55:46,301 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:55:46,301 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 17:55:46,309 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:25:45 GMT'), (b'server', b'uvicorn'), (b'content-length', b'65705'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 17:55:46,309 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 17:55:46,310 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 17:55:46,310 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:55:46,310 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:55:46,310 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:55:46,310 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:55:46,311 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:55:49,618 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:55:49,618 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:55:49,619 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:55:55,653 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:55:55,659 [INFO] scraper.reddit_scraper:403: Starting Reddit API search for: gorgeous
2025-05-29 17:55:55,659 [INFO] scraper.reddit_scraper.APIClient:97: [SEARCH] Starting COMPREHENSIVE search for 'gorgeous' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 17:55:55,659 [INFO] scraper.reddit_scraper.APIClient:112: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 17:55:55,659 [INFO] scraper.reddit_scraper.APIClient:116:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 17:55:59,328 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 98 posts with sort=relevance, time=week. New unique: 98. Total unique: 98
2025-05-29 17:55:59,329 [INFO] scraper.reddit_scraper.APIClient:116:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 17:56:02,381 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 91 posts with sort=top, time=week. New unique: 46. Total unique: 144
2025-05-29 17:56:02,381 [INFO] scraper.reddit_scraper.APIClient:116:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 17:56:05,384 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=hot, time=week. New unique: 93. Total unique: 237
2025-05-29 17:56:05,384 [INFO] scraper.reddit_scraper.APIClient:116:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 17:56:08,015 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 83. Total unique: 320
2025-05-29 17:56:08,015 [INFO] scraper.reddit_scraper.APIClient:116:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 17:56:11,512 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 98 posts with sort=relevance, time=week. New unique: 0. Total unique: 320
2025-05-29 17:56:11,512 [INFO] scraper.reddit_scraper.APIClient:116:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 17:56:14,371 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 320
2025-05-29 17:56:14,372 [INFO] scraper.reddit_scraper.APIClient:143: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'gorgeous':
[STATS] Total unique posts found: 320
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 273
[RESULT] SUCCESS: Found 320 posts that will be processed and saved

2025-05-29 17:56:14,372 [INFO] scraper.reddit_scraper:415: Reddit API returned 320 mentions
2025-05-29 17:56:14,372 [INFO] scraper.reddit_scraper:487: Post-processing 320 mentions...
2025-05-29 17:56:14,372 [INFO] scraper.reddit_scraper:589: Deduplication: 320 -> 320 mentions (removed 0 exact duplicates)
2025-05-29 17:56:14,373 [INFO] scraper.reddit_scraper:491: After deduplication: 320 mentions
2025-05-29 17:56:14,373 [INFO] scraper.reddit_scraper:653: Validation: 320 raw mentions  320 kept (EXTREMELY LENIENT - keeping everything possible)
2025-05-29 17:56:14,373 [INFO] scraper.reddit_scraper:495: After validation: 320 mentions
2025-05-29 17:56:14,528 [INFO] scraper.reddit_scraper:512: After database mapping: 320 mentions (ALL KEPT - no quality filtering)
2025-05-29 17:56:14,528 [INFO] scraper.reddit_scraper:524: Enhanced 320 mentions with sentiment analysis
2025-05-29 17:56:14,529 [INFO] scraper.reddit_scraper:538: Final result: 320 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 17:56:14,530 [INFO] scraper.reddit_scraper:544: 
        Scraping completed for 'gorgeous':
        - Duration: 18.87s
        - Pages scraped: 0
        - Mentions found: 320
        - Success rate: 0.00%
        - Errors: 0
        - Retries: 0
        - Rate limit hits: 0
        - Cache hits: 0
        
2025-05-29 17:56:14,531 [INFO] analytics.data_validator:236: Validating dataset of 320 mentions
2025-05-29 17:56:17,478 [INFO] analytics.data_validator:259: Validation complete: 320/320 mentions passed
2025-05-29 17:56:17,478 [INFO] app:661: Data validation: 320/320 mentions passed
2025-05-29 17:56:18,637 [INFO] app:721: Initial metrics from analyzer: ['overview', 'engagement', 'temporal', 'subreddit_analysis', 'sentiment', 'content_analysis', 'trending', 'author_analysis', 'performance_analysis', 'quality_analysis', 'competition_analysis']
2025-05-29 17:56:18,637 [INFO] app:725: Regenerating all metrics in visualizer-compatible format...
2025-05-29 17:56:18,637 [INFO] app:728: Regenerated overview metrics from mentions
2025-05-29 17:56:18,637 [INFO] app:2759: Generating temporal metrics from 320 mentions
2025-05-29 17:56:18,639 [INFO] app:2763: DataFrame columns: ['reddit_id', 'post_type', 'subreddit', 'url', 'title', 'content', 'author', 'score', 'num_comments', 'upvote_ratio', 'sentiment_score', 'relevance_score', 'created_utc', 'scraped_at']
2025-05-29 17:56:18,639 [INFO] app:2767: Processing created_utc timestamps
2025-05-29 17:56:18,641 [INFO] app:2771: After timestamp conversion: 320 valid records
2025-05-29 17:56:18,644 [INFO] app:2790: Generated temporal metrics: hourly_distribution has 24 entries
2025-05-29 17:56:18,644 [DEBUG] app:2791: Hourly distribution: {'0': 18, '1': 6, '2': 9, '3': 9, '4': 8, '5': 9, '6': 10, '7': 18, '8': 8, '9': 15, '10': 10, '11': 5, '12': 22, '13': 16, '14': 22, '15': 15, '16': 27, '17': 30, '18': 10, '19': 8, '20': 12, '21': 14, '22': 7, '23': 12}
2025-05-29 17:56:18,644 [INFO] app:731: Regenerated temporal metrics from mentions
2025-05-29 17:56:18,646 [INFO] app:734: Regenerated sentiment metrics from mentions
2025-05-29 17:56:18,651 [INFO] app:737: Regenerated engagement metrics from mentions
2025-05-29 17:56:18,651 [INFO] app:2885: Generating quality metrics from 320 mentions
2025-05-29 17:56:18,652 [INFO] app:2889: DataFrame columns: ['reddit_id', 'post_type', 'subreddit', 'url', 'title', 'content', 'author', 'score', 'num_comments', 'upvote_ratio', 'sentiment_score', 'relevance_score', 'created_utc', 'scraped_at']
2025-05-29 17:56:18,652 [INFO] app:2894: Using relevance_score for quality analysis
2025-05-29 17:56:18,653 [INFO] app:2916: Generated quality metrics: high=6, medium=191, low=123
2025-05-29 17:56:18,653 [INFO] app:740: Regenerated quality_analysis metrics from mentions
2025-05-29 17:56:18,653 [INFO] app:2931: Generating subreddit metrics from 320 mentions
2025-05-29 17:56:18,654 [INFO] app:2935: DataFrame columns: ['reddit_id', 'post_type', 'subreddit', 'url', 'title', 'content', 'author', 'score', 'num_comments', 'upvote_ratio', 'sentiment_score', 'relevance_score', 'created_utc', 'scraped_at']
2025-05-29 17:56:18,655 [INFO] app:2962: Generated subreddit metrics: top_subreddits_by_mentions has 10 entries
2025-05-29 17:56:18,656 [DEBUG] app:2963: Top subreddits: {'ArmpitCleavage': 13, 'SilkyHotHair': 5, 'LindaCardellini': 3, 'NativePlantGardening': 3, 'SupermodelCats': 3, 'andor': 2, 'Savannah_Clarke_': 2, 'steak': 2, 'snakes': 2, 'BMW': 2}
2025-05-29 17:56:18,656 [INFO] app:743: Regenerated subreddit_analysis metrics from mentions
2025-05-29 17:56:18,656 [INFO] app:2982: Generating author metrics from 320 mentions
2025-05-29 17:56:18,657 [INFO] app:2986: DataFrame columns: ['reddit_id', 'post_type', 'subreddit', 'url', 'title', 'content', 'author', 'score', 'num_comments', 'upvote_ratio', 'sentiment_score', 'relevance_score', 'created_utc', 'scraped_at']
2025-05-29 17:56:18,658 [INFO] app:3014: Generated author metrics: top_authors has 10 entries
2025-05-29 17:56:18,659 [DEBUG] app:3015: Top authors: {'Hefty-Collection8377': 13, 'Tiny_Health_7007': 8, 'Wobbly_Princess': 7, 'RovJos': 4, 'taylorlover2021': 3, 'Lemon_With_Honey': 3, 'CR_theprincess': 2, 'Cheekygreek84': 2, 'Fit_Addition1963': 2, 'SuddenAnybody7726': 2}
2025-05-29 17:56:18,659 [INFO] app:746: Regenerated author_analysis metrics from mentions
2025-05-29 17:56:18,659 [INFO] app:748: Final metrics structure: ['overview', 'engagement', 'temporal', 'subreddit_analysis', 'sentiment', 'content_analysis', 'trending', 'author_analysis', 'performance_analysis', 'quality_analysis', 'competition_analysis']
2025-05-29 17:56:18,659 [INFO] app:752: Temporal metrics keys: ['hourly_distribution', 'daily_timeline', 'total_timespan_days']
2025-05-29 17:56:18,659 [INFO] app:754: Hourly distribution has 24 entries
2025-05-29 17:56:18,659 [INFO] app:756: Author analysis keys: ['author_distribution', 'total_authors', 'top_authors', 'top_authors_by_engagement', 'author_diversity']
2025-05-29 17:56:18,660 [INFO] app:758: Top authors has 10 entries
2025-05-29 17:56:18,660 [INFO] app:760: Quality analysis keys: ['quality_distribution', 'average_quality', 'quality_trends', 'quality_by_subreddit']
2025-05-29 17:56:18,660 [INFO] app:810: Search completed: gorgeous -> 320 mentions
2025-05-29 17:56:18,766 [ERROR] app:1461: Visualization generation failed: 'str' object has no attribute 'get'
2025-05-29 17:56:18,893 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:56:18,957 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:56:18,967 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:56:19,647 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:56:19,816 [DEBUG] app:1598: Monitor refresh completed successfully
2025-05-29 17:56:19,819 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:56:26,803 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 17:56:49,850 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:57:58,055 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:57:58,055 [INFO] scraper.reddit_scraper:294: HTTP session initialized successfully
2025-05-29 17:57:58,055 [INFO] scraper.reddit_scraper:277: Reddit scraper initialized with API-only configuration
2025-05-29 17:57:58,056 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 17:57:58,056 [INFO] app:388: Database initialized successfully
2025-05-29 17:57:58,059 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 17:57:58,059 [INFO] app:254: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 17:57:58,153 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 17:57:58,157 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:57:58,158 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 17:57:58,158 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 17:57:58,192 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 17:57:58,194 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 17:57:58,301 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:57:58,310 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 17:57:58,420 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000111B9E4D880>
2025-05-29 17:57:58,420 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x00000111B81F8050> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 17:57:58,498 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000111B9E4D790>
2025-05-29 17:57:58,498 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:57:58,498 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:57:58,499 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:57:58,499 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:57:58,499 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:57:58,574 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 12:27:58 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 17:57:58,575 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 17:57:58,575 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:57:58,575 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:57:58,575 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:57:58,576 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:57:58,576 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:57:58,576 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:57:58,594 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 17:57:58,610 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000111B9DBEFF0>
2025-05-29 17:57:58,610 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x00000111B813A850> server_hostname='api.gradio.app' timeout=3
2025-05-29 17:57:59,066 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 17:57:59,158 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000111B9CD1DF0>
2025-05-29 17:57:59,159 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:57:59,159 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:57:59,159 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:57:59,160 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:57:59,160 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:57:59,433 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 12:27:59 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 17:57:59,433 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 17:57:59,434 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:57:59,434 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:57:59,435 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:57:59,435 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:57:59,435 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:57:59,435 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:58:00,073 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 17:58:00,640 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000111BA131160>
2025-05-29 17:58:00,640 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 17:58:00,641 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:58:00,641 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 17:58:00,641 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:58:00,641 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 17:58:00,641 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:28:00 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 17:58:00,642 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 17:58:00,642 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 17:58:00,642 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:58:00,642 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:58:00,642 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:58:00,642 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:58:00,643 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:58:00,649 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 17:58:02,672 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000111BA1D7140>
2025-05-29 17:58:02,672 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 17:58:02,672 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 17:58:02,674 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 17:58:02,674 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 17:58:02,674 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 17:58:02,682 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:28:01 GMT'), (b'server', b'uvicorn'), (b'content-length', b'65757'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 17:58:02,683 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 17:58:02,683 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 17:58:02,683 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 17:58:02,683 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 17:58:02,684 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 17:58:02,684 [DEBUG] httpcore.connection:47: close.started
2025-05-29 17:58:02,684 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 17:58:11,855 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:58:11,856 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:58:11,857 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:58:16,801 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:58:16,808 [INFO] scraper.reddit_scraper:403: Starting Reddit API search for: jaanesh
2025-05-29 17:58:16,808 [INFO] scraper.reddit_scraper.APIClient:97: [SEARCH] Starting COMPREHENSIVE search for 'jaanesh' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 17:58:16,808 [INFO] scraper.reddit_scraper.APIClient:112: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 17:58:16,808 [INFO] scraper.reddit_scraper.APIClient:116:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 17:58:17,241 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 0 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 17:58:17,241 [INFO] scraper.reddit_scraper.APIClient:116:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 17:58:18,268 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 0 posts with sort=top, time=week. New unique: 0. Total unique: 0
2025-05-29 17:58:18,269 [INFO] scraper.reddit_scraper.APIClient:116:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 17:58:19,410 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 0 posts with sort=hot, time=week. New unique: 0. Total unique: 0
2025-05-29 17:58:19,410 [INFO] scraper.reddit_scraper.APIClient:116:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 17:58:20,655 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 0 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 17:58:20,655 [INFO] scraper.reddit_scraper.APIClient:116:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 17:58:21,870 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 0 posts with sort=relevance, time=week. New unique: 0. Total unique: 0
2025-05-29 17:58:21,871 [INFO] scraper.reddit_scraper.APIClient:116:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 17:58:22,791 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 0 posts with sort=new, time=week. New unique: 0. Total unique: 0
2025-05-29 17:58:22,791 [INFO] scraper.reddit_scraper.APIClient:143: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'jaanesh':
[STATS] Total unique posts found: 0
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 0
[RESULT] SUCCESS: Found 0 posts that will be processed and saved

2025-05-29 17:58:22,791 [WARNING] scraper.reddit_scraper:412: No mentions found for 'jaanesh'
2025-05-29 17:58:22,793 [INFO] app:721: Initial metrics from analyzer: ['overview', 'engagement', 'temporal', 'subreddit_analysis', 'sentiment', 'content_analysis', 'trending', 'author_analysis', 'performance_analysis', 'quality_analysis', 'competition_analysis']
2025-05-29 17:58:22,793 [WARNING] app:2756: No mentions provided for temporal analysis
2025-05-29 17:58:22,793 [INFO] app:730: Generated temporal metrics from mentions
2025-05-29 17:58:22,793 [INFO] app:734: Generated sentiment metrics from mentions
2025-05-29 17:58:22,793 [INFO] app:738: Generated engagement metrics from mentions
2025-05-29 17:58:22,794 [WARNING] app:2882: No mentions provided for quality analysis
2025-05-29 17:58:22,794 [INFO] app:742: Generated quality_analysis metrics from mentions
2025-05-29 17:58:22,794 [WARNING] app:2928: No mentions provided for subreddit analysis
2025-05-29 17:58:22,794 [INFO] app:746: Generated subreddit_analysis metrics from mentions
2025-05-29 17:58:22,794 [WARNING] app:2979: No mentions provided for author analysis
2025-05-29 17:58:22,794 [INFO] app:750: Generated author_analysis metrics from mentions
2025-05-29 17:58:22,794 [INFO] app:752: Final metrics structure: ['overview', 'engagement', 'temporal', 'subreddit_analysis', 'sentiment', 'content_analysis', 'trending', 'author_analysis', 'performance_analysis', 'quality_analysis', 'competition_analysis']
2025-05-29 17:58:22,795 [INFO] app:810: Search completed: jaanesh -> 0 mentions
2025-05-29 17:58:22,825 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 17:58:24,770 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 17:58:24,777 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:02:15,578 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:02:15,578 [INFO] scraper.reddit_scraper:294: HTTP session initialized successfully
2025-05-29 18:02:15,578 [INFO] scraper.reddit_scraper:277: Reddit scraper initialized with API-only configuration
2025-05-29 18:02:15,579 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 18:02:15,579 [INFO] app:388: Database initialized successfully
2025-05-29 18:02:15,582 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 18:02:15,582 [INFO] app:254: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 18:02:15,682 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 18:02:15,685 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 18:02:15,686 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 18:02:15,686 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 18:02:15,721 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 18:02:15,722 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 18:02:15,824 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 18:02:15,824 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 18:02:15,918 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E6E0432BA0>
2025-05-29 18:02:15,919 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E6DE7D7F50> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 18:02:15,995 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E6E0432B10>
2025-05-29 18:02:15,996 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 18:02:15,996 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 18:02:15,996 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 18:02:15,997 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 18:02:15,997 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 18:02:16,074 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 12:32:15 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 18:02:16,075 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 18:02:16,075 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 18:02:16,076 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 18:02:16,076 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 18:02:16,076 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 18:02:16,076 [DEBUG] httpcore.connection:47: close.started
2025-05-29 18:02:16,076 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 18:02:16,112 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E6E0433200>
2025-05-29 18:02:16,113 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E6DE7D67D0> server_hostname='api.gradio.app' timeout=3
2025-05-29 18:02:16,124 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 18:02:16,593 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.66 (threshold: 85)
2025-05-29 18:02:16,674 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E6E033EA50>
2025-05-29 18:02:16,674 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 18:02:16,674 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 18:02:16,674 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 18:02:16,674 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 18:02:16,674 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 18:02:16,955 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 12:32:16 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 18:02:16,955 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 18:02:16,956 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 18:02:16,956 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 18:02:16,956 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 18:02:16,956 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 18:02:16,956 [DEBUG] httpcore.connection:47: close.started
2025-05-29 18:02:16,956 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 18:02:17,602 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:02:18,173 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E6E06D5D90>
2025-05-29 18:02:18,173 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 18:02:18,174 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 18:02:18,174 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 18:02:18,175 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 18:02:18,175 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 18:02:18,175 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:32:18 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 18:02:18,175 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 18:02:18,175 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 18:02:18,176 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 18:02:18,176 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 18:02:18,176 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 18:02:18,176 [DEBUG] httpcore.connection:47: close.started
2025-05-29 18:02:18,176 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 18:02:18,184 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 18:02:20,221 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E6E07C0920>
2025-05-29 18:02:20,221 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 18:02:20,221 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 18:02:20,222 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 18:02:20,222 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 18:02:20,222 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 18:02:20,230 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:32:19 GMT'), (b'server', b'uvicorn'), (b'content-length', b'65804'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 18:02:20,230 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 18:02:20,231 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 18:02:20,231 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 18:02:20,231 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 18:02:20,231 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 18:02:20,231 [DEBUG] httpcore.connection:47: close.started
2025-05-29 18:02:20,231 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 18:02:25,544 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:02:25,544 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:02:25,546 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:02:27,602 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.65 (threshold: 85)
2025-05-29 18:02:36,683 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:02:36,689 [INFO] scraper.reddit_scraper:403: Starting Reddit API search for: jhol
2025-05-29 18:02:36,689 [INFO] scraper.reddit_scraper.APIClient:97: [SEARCH] Starting COMPREHENSIVE search for 'jhol' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 18:02:36,689 [INFO] scraper.reddit_scraper.APIClient:112: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 18:02:36,689 [INFO] scraper.reddit_scraper.APIClient:116:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 18:02:37,630 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 19 posts with sort=relevance, time=week. New unique: 19. Total unique: 19
2025-05-29 18:02:37,630 [INFO] scraper.reddit_scraper.APIClient:116:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 18:02:38,615 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 14 posts with sort=top, time=week. New unique: 2. Total unique: 21
2025-05-29 18:02:38,615 [INFO] scraper.reddit_scraper.APIClient:116:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 18:02:39,459 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 7 posts with sort=hot, time=week. New unique: 0. Total unique: 21
2025-05-29 18:02:39,460 [INFO] scraper.reddit_scraper.APIClient:116:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 18:02:40,739 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 14 posts with sort=new, time=week. New unique: 0. Total unique: 21
2025-05-29 18:02:40,740 [INFO] scraper.reddit_scraper.APIClient:116:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 18:02:41,918 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 19 posts with sort=relevance, time=week. New unique: 0. Total unique: 21
2025-05-29 18:02:41,919 [INFO] scraper.reddit_scraper.APIClient:116:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 18:02:43,014 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 14 posts with sort=new, time=week. New unique: 0. Total unique: 21
2025-05-29 18:02:43,014 [INFO] scraper.reddit_scraper.APIClient:143: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'jhol':
[STATS] Total unique posts found: 21
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 20
[RESULT] SUCCESS: Found 21 posts that will be processed and saved

2025-05-29 18:02:43,014 [INFO] scraper.reddit_scraper:415: Reddit API returned 21 mentions
2025-05-29 18:02:43,015 [INFO] scraper.reddit_scraper:487: Post-processing 21 mentions...
2025-05-29 18:02:43,015 [INFO] scraper.reddit_scraper:589: Deduplication: 21 -> 21 mentions (removed 0 exact duplicates)
2025-05-29 18:02:43,015 [INFO] scraper.reddit_scraper:491: After deduplication: 21 mentions
2025-05-29 18:02:43,015 [INFO] scraper.reddit_scraper:653: Validation: 21 raw mentions  21 kept (EXTREMELY LENIENT - keeping everything possible)
2025-05-29 18:02:43,015 [INFO] scraper.reddit_scraper:495: After validation: 21 mentions
2025-05-29 18:02:43,036 [INFO] scraper.reddit_scraper:512: After database mapping: 21 mentions (ALL KEPT - no quality filtering)
2025-05-29 18:02:43,037 [INFO] scraper.reddit_scraper:524: Enhanced 21 mentions with sentiment analysis
2025-05-29 18:02:43,037 [INFO] scraper.reddit_scraper:538: Final result: 21 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 18:02:43,037 [INFO] scraper.reddit_scraper:544: 
        Scraping completed for 'jhol':
        - Duration: 6.35s
        - Pages scraped: 0
        - Mentions found: 21
        - Success rate: 0.00%
        - Errors: 0
        - Retries: 0
        - Rate limit hits: 0
        - Cache hits: 0
        
2025-05-29 18:02:43,038 [INFO] analytics.data_validator:236: Validating dataset of 21 mentions
2025-05-29 18:02:43,374 [INFO] analytics.data_validator:259: Validation complete: 21/21 mentions passed
2025-05-29 18:02:43,374 [INFO] app:661: Data validation: 21/21 mentions passed
2025-05-29 18:02:43,464 [INFO] app:721: Initial metrics from analyzer: ['overview', 'engagement', 'temporal', 'subreddit_analysis', 'sentiment', 'content_analysis', 'trending', 'author_analysis', 'performance_analysis', 'quality_analysis', 'competition_analysis']
2025-05-29 18:02:43,465 [INFO] app:752: Final metrics structure: ['overview', 'engagement', 'temporal', 'subreddit_analysis', 'sentiment', 'content_analysis', 'trending', 'author_analysis', 'performance_analysis', 'quality_analysis', 'competition_analysis']
2025-05-29 18:02:43,465 [INFO] app:756: Temporal metrics keys: ['daily_timeline', 'hourly_distribution', 'day_of_week_distribution', 'trend', 'peak_day', 'peak_hour']
2025-05-29 18:02:43,465 [INFO] app:758: Author analysis keys: ['total_unique_authors', 'single_post_authors', 'multi_post_authors', 'author_diversity_ratio', 'top_authors_by_posts', 'top_authors_by_engagement', 'avg_posts_per_author', 'most_active_author', 'most_active_author_posts']
2025-05-29 18:02:43,465 [INFO] app:760: Quality analysis keys: ['avg_title_length', 'avg_content_length', 'avg_title_quality', 'avg_content_quality', 'avg_overall_quality', 'quality_distribution', 'posts_with_questions', 'posts_with_numbers', 'posts_with_links', 'quality_vs_engagement_correlation', 'best_quality_posts']
2025-05-29 18:02:43,465 [INFO] app:810: Search completed: jhol -> 21 mentions
2025-05-29 18:02:43,637 [ERROR] app:1461: Visualization generation failed: 'list' object has no attribute 'get'
2025-05-29 18:02:43,709 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:02:47,754 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:02:47,763 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:02:55,636 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:02:55,793 [DEBUG] app:1598: Monitor refresh completed successfully
2025-05-29 18:02:55,795 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:03:23,700 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:04:29,792 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:04:39,791 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 961277619.00 (avg: 960485648.21, std: 262070.38)
2025-05-29 18:09:40,375 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:09:40,375 [INFO] scraper.reddit_scraper:294: HTTP session initialized successfully
2025-05-29 18:09:40,375 [INFO] scraper.reddit_scraper:277: Reddit scraper initialized with API-only configuration
2025-05-29 18:09:40,375 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 18:09:40,376 [INFO] app:388: Database initialized successfully
2025-05-29 18:09:40,379 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 18:09:40,381 [INFO] app:254: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 18:09:40,595 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 18:09:40,603 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 18:09:40,604 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 18:09:40,604 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 18:09:40,615 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 18:09:40,618 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 18:09:40,646 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 18:09:40,651 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 18:09:40,736 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C67071D700>
2025-05-29 18:09:40,736 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001C66F033E50> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 18:09:40,816 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C67071D610>
2025-05-29 18:09:40,816 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 18:09:40,817 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 18:09:40,817 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 18:09:40,817 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 18:09:40,817 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 18:09:40,895 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 12:39:40 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 18:09:40,896 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 18:09:40,897 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 18:09:40,897 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 18:09:40,898 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 18:09:40,898 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 18:09:40,898 [DEBUG] httpcore.connection:47: close.started
2025-05-29 18:09:40,899 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 18:09:40,919 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C66F1F61E0>
2025-05-29 18:09:40,920 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001C66F033F50> server_hostname='api.gradio.app' timeout=3
2025-05-29 18:09:41,158 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 18:09:41,388 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.47 (threshold: 85)
2025-05-29 18:09:41,467 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C6706BB9E0>
2025-05-29 18:09:41,468 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 18:09:41,468 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 18:09:41,468 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 18:09:41,468 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 18:09:41,469 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 18:09:41,742 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 12:39:41 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 18:09:41,743 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 18:09:41,743 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 18:09:41,743 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 18:09:41,743 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 18:09:41,743 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 18:09:41,744 [DEBUG] httpcore.connection:47: close.started
2025-05-29 18:09:41,744 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 18:09:42,394 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:09:43,166 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C66F0C7440>
2025-05-29 18:09:43,166 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 18:09:43,167 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 18:09:43,167 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 18:09:43,167 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 18:09:43,167 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 18:09:43,168 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:39:43 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 18:09:43,168 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 18:09:43,168 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 18:09:43,168 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 18:09:43,168 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 18:09:43,169 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 18:09:43,169 [DEBUG] httpcore.connection:47: close.started
2025-05-29 18:09:43,169 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 18:09:43,175 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 18:09:45,206 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C6710CF290>
2025-05-29 18:09:45,206 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 18:09:45,206 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 18:09:45,208 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 18:09:45,208 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 18:09:45,208 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 18:09:45,272 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 12:39:44 GMT'), (b'server', b'uvicorn'), (b'content-length', b'65850'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 18:09:45,272 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 18:09:45,273 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 18:09:45,273 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 18:09:45,273 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 18:09:45,273 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 18:09:45,273 [DEBUG] httpcore.connection:47: close.started
2025-05-29 18:09:45,273 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 18:09:52,396 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.45 (threshold: 85)
2025-05-29 18:09:52,879 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:09:52,880 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:09:52,881 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:10:06,683 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:10:06,689 [INFO] scraper.reddit_scraper:403: Starting Reddit API search for: my
2025-05-29 18:10:06,689 [INFO] scraper.reddit_scraper.APIClient:97: [SEARCH] Starting COMPREHENSIVE search for 'my' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 18:10:06,689 [INFO] scraper.reddit_scraper.APIClient:112: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 18:10:06,689 [INFO] scraper.reddit_scraper.APIClient:116:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 18:10:10,341 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 96 posts with sort=relevance, time=week. New unique: 96. Total unique: 96
2025-05-29 18:10:10,342 [INFO] scraper.reddit_scraper.APIClient:116:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 18:10:12,051 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 98 posts with sort=top, time=week. New unique: 82. Total unique: 178
2025-05-29 18:10:12,051 [INFO] scraper.reddit_scraper.APIClient:116:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 18:10:14,550 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=hot, time=week. New unique: 95. Total unique: 273
2025-05-29 18:10:14,550 [INFO] scraper.reddit_scraper.APIClient:116:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 18:10:16,400 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 100. Total unique: 373
2025-05-29 18:10:16,400 [INFO] scraper.reddit_scraper.APIClient:116:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 18:10:19,334 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 96 posts with sort=relevance, time=week. New unique: 0. Total unique: 373
2025-05-29 18:10:19,334 [INFO] scraper.reddit_scraper.APIClient:116:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 18:10:20,942 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 373
2025-05-29 18:10:20,942 [INFO] scraper.reddit_scraper.APIClient:143: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'my':
[STATS] Total unique posts found: 373
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 242
[RESULT] SUCCESS: Found 373 posts that will be processed and saved

2025-05-29 18:10:20,943 [INFO] scraper.reddit_scraper:415: Reddit API returned 373 mentions
2025-05-29 18:10:20,943 [INFO] scraper.reddit_scraper:487: Post-processing 373 mentions...
2025-05-29 18:10:20,943 [INFO] scraper.reddit_scraper:589: Deduplication: 373 -> 373 mentions (removed 0 exact duplicates)
2025-05-29 18:10:20,943 [INFO] scraper.reddit_scraper:491: After deduplication: 373 mentions
2025-05-29 18:10:20,943 [INFO] scraper.reddit_scraper:653: Validation: 373 raw mentions  373 kept (EXTREMELY LENIENT - keeping everything possible)
2025-05-29 18:10:20,944 [INFO] scraper.reddit_scraper:495: After validation: 373 mentions
2025-05-29 18:10:21,106 [INFO] scraper.reddit_scraper:512: After database mapping: 373 mentions (ALL KEPT - no quality filtering)
2025-05-29 18:10:21,106 [INFO] scraper.reddit_scraper:524: Enhanced 373 mentions with sentiment analysis
2025-05-29 18:10:21,106 [INFO] scraper.reddit_scraper:538: Final result: 373 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 18:10:21,108 [INFO] scraper.reddit_scraper:544: 
        Scraping completed for 'my':
        - Duration: 14.42s
        - Pages scraped: 0
        - Mentions found: 373
        - Success rate: 0.00%
        - Errors: 0
        - Retries: 0
        - Rate limit hits: 0
        - Cache hits: 0
        
2025-05-29 18:10:21,109 [INFO] analytics.data_validator:236: Validating dataset of 373 mentions
2025-05-29 18:10:24,519 [INFO] analytics.data_validator:259: Validation complete: 373/373 mentions passed
2025-05-29 18:10:24,520 [INFO] app:661: Data validation: 373/373 mentions passed
2025-05-29 18:10:25,453 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.46 (threshold: 85)
2025-05-29 18:10:25,671 [INFO] app:721: Initial metrics from analyzer: ['overview', 'engagement', 'temporal', 'subreddit_analysis', 'sentiment', 'content_analysis', 'trending', 'author_analysis', 'performance_analysis', 'quality_analysis', 'competition_analysis']
2025-05-29 18:10:25,672 [INFO] app:752: Final metrics structure: ['overview', 'engagement', 'temporal', 'subreddit_analysis', 'sentiment', 'content_analysis', 'trending', 'author_analysis', 'performance_analysis', 'quality_analysis', 'competition_analysis']
2025-05-29 18:10:25,672 [INFO] app:756: Temporal metrics keys: ['daily_timeline', 'hourly_distribution', 'day_of_week_distribution', 'trend', 'peak_day', 'peak_hour']
2025-05-29 18:10:25,672 [INFO] app:758: Author analysis keys: ['total_unique_authors', 'single_post_authors', 'multi_post_authors', 'author_diversity_ratio', 'top_authors_by_posts', 'top_authors_by_engagement', 'avg_posts_per_author', 'most_active_author', 'most_active_author_posts']
2025-05-29 18:10:25,672 [INFO] app:760: Quality analysis keys: ['avg_title_length', 'avg_content_length', 'avg_title_quality', 'avg_content_quality', 'avg_overall_quality', 'quality_distribution', 'posts_with_questions', 'posts_with_numbers', 'posts_with_links', 'quality_vs_engagement_correlation', 'best_quality_posts']
2025-05-29 18:10:25,672 [INFO] app:810: Search completed: my -> 373 mentions
2025-05-29 18:10:25,964 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:10:26,014 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:10:26,176 [DEBUG] app:1598: Monitor refresh completed successfully
2025-05-29 18:10:26,178 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:10:26,284 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:10:26,303 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:10:48,486 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:10:52,973 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:10:53,111 [DEBUG] app:1598: Monitor refresh completed successfully
2025-05-29 18:10:53,113 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:11:54,595 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:13:00,696 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:14:06,828 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:40:21,355 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:40:21,356 [INFO] scraper.reddit_scraper:294: HTTP session initialized successfully
2025-05-29 18:40:21,356 [INFO] scraper.reddit_scraper:277: Reddit scraper initialized with API-only configuration
2025-05-29 18:40:21,356 [WARNING] analytics.advanced_sentiment:29: Transformers not available, falling back to TextBlob only
2025-05-29 18:40:21,357 [INFO] app:388: Database initialized successfully
2025-05-29 18:40:21,360 [INFO] monitoring.system_monitor:179: System monitoring started
2025-05-29 18:40:21,360 [INFO] app:254: Enhanced Reddit Mention Tracker initialized successfully
2025-05-29 18:40:21,529 [DEBUG] matplotlib:342: matplotlib data path: C:\Program Files\Python312\Lib\site-packages\matplotlib\mpl-data
2025-05-29 18:40:21,537 [DEBUG] matplotlib:342: CONFIGDIR=C:\Users\Devansh\.matplotlib
2025-05-29 18:40:21,538 [DEBUG] matplotlib:1557: interactive is False
2025-05-29 18:40:21,538 [DEBUG] matplotlib:1558: platform is win32
2025-05-29 18:40:21,573 [DEBUG] matplotlib:342: CACHEDIR=C:\Users\Devansh\.matplotlib
2025-05-29 18:40:21,579 [DEBUG] matplotlib.font_manager:1635: Using fontManager instance from C:\Users\Devansh\.matplotlib\fontlist-v390.json
2025-05-29 18:40:21,595 [DEBUG] httpcore.connection:47: connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 18:40:21,595 [DEBUG] httpcore.connection:47: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-29 18:40:21,681 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F230524260>
2025-05-29 18:40:21,681 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F22EC97BD0> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-29 18:40:21,757 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F2304FB530>
2025-05-29 18:40:21,758 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 18:40:21,758 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 18:40:21,758 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 18:40:21,759 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 18:40:21,759 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 18:40:21,835 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Thu, 29 May 2025 13:10:21 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'14'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-29 18:40:21,835 [INFO] httpx:1025: HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-29 18:40:21,835 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 18:40:21,835 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 18:40:21,835 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 18:40:21,836 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 18:40:21,836 [DEBUG] httpcore.connection:47: close.started
2025-05-29 18:40:21,836 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 18:40:21,941 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F23045F4D0>
2025-05-29 18:40:21,941 [DEBUG] httpcore.connection:47: start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F22EC967D0> server_hostname='api.gradio.app' timeout=3
2025-05-29 18:40:22,029 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=None socket_options=None
2025-05-29 18:40:22,367 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] system_metrics: disk_usage is high: 93.46 (threshold: 85)
2025-05-29 18:40:22,490 [DEBUG] httpcore.connection:47: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F23056A180>
2025-05-29 18:40:22,490 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 18:40:22,490 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 18:40:22,491 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 18:40:22,491 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 18:40:22,491 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 18:40:22,765 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 29 May 2025 13:10:22 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-29 18:40:22,765 [INFO] httpx:1025: HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-29 18:40:22,765 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 18:40:22,766 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 18:40:22,766 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 18:40:22,766 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 18:40:22,766 [DEBUG] httpcore.connection:47: close.started
2025-05-29 18:40:22,766 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 18:40:23,374 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:40:24,052 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F230B96090>
2025-05-29 18:40:24,052 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'GET']>
2025-05-29 18:40:24,053 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 18:40:24,053 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'GET']>
2025-05-29 18:40:24,053 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 18:40:24,053 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'GET']>
2025-05-29 18:40:24,054 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 13:10:24 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-29 18:40:24,054 [INFO] httpx:1025: HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-29 18:40:24,054 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'GET']>
2025-05-29 18:40:24,054 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 18:40:24,054 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 18:40:24,055 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 18:40:24,055 [DEBUG] httpcore.connection:47: close.started
2025-05-29 18:40:24,055 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 18:40:24,062 [DEBUG] httpcore.connection:47: connect_tcp.started host='localhost' port=7860 local_address=None timeout=3 socket_options=None
2025-05-29 18:40:26,072 [DEBUG] httpcore.connection:47: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F230CDF950>
2025-05-29 18:40:26,072 [DEBUG] httpcore.http11:47: send_request_headers.started request=<Request [b'HEAD']>
2025-05-29 18:40:26,073 [DEBUG] httpcore.http11:47: send_request_headers.complete
2025-05-29 18:40:26,073 [DEBUG] httpcore.http11:47: send_request_body.started request=<Request [b'HEAD']>
2025-05-29 18:40:26,073 [DEBUG] httpcore.http11:47: send_request_body.complete
2025-05-29 18:40:26,073 [DEBUG] httpcore.http11:47: receive_response_headers.started request=<Request [b'HEAD']>
2025-05-29 18:40:26,082 [DEBUG] httpcore.http11:47: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 29 May 2025 13:10:25 GMT'), (b'server', b'uvicorn'), (b'content-length', b'65908'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-29 18:40:26,082 [INFO] httpx:1025: HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-29 18:40:26,082 [DEBUG] httpcore.http11:47: receive_response_body.started request=<Request [b'HEAD']>
2025-05-29 18:40:26,083 [DEBUG] httpcore.http11:47: receive_response_body.complete
2025-05-29 18:40:26,083 [DEBUG] httpcore.http11:47: response_closed.started
2025-05-29 18:40:26,083 [DEBUG] httpcore.http11:47: response_closed.complete
2025-05-29 18:40:26,083 [DEBUG] httpcore.connection:47: close.started
2025-05-29 18:40:26,083 [DEBUG] httpcore.connection:47: close.complete
2025-05-29 18:40:32,056 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:40:32,057 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:40:32,059 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:40:35,767 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:40:35,772 [INFO] app:1622: Search history cleared: 2008 mentions, 10 sessions
2025-05-29 18:40:35,774 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:40:42,185 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:40:42,191 [INFO] scraper.reddit_scraper:403: Starting Reddit API search for: machine learning
2025-05-29 18:40:42,191 [INFO] scraper.reddit_scraper.APIClient:97: [SEARCH] Starting COMPREHENSIVE search for 'machine learning' targeting 1000 mentions (LAST 7 DAYS ONLY)
2025-05-29 18:40:42,191 [INFO] scraper.reddit_scraper.APIClient:112: [EXEC] Executing 6 search configurations to maximize coverage...
2025-05-29 18:40:42,192 [INFO] scraper.reddit_scraper.APIClient:116:    [1/6] Searching with sort=relevance, time=week, limit=200
2025-05-29 18:40:45,406 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 96 posts with sort=relevance, time=week. New unique: 96. Total unique: 96
2025-05-29 18:40:45,406 [INFO] scraper.reddit_scraper.APIClient:116:    [2/6] Searching with sort=top, time=week, limit=200
2025-05-29 18:40:49,696 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 97 posts with sort=top, time=week. New unique: 81. Total unique: 177
2025-05-29 18:40:49,697 [INFO] scraper.reddit_scraper.APIClient:116:    [3/6] Searching with sort=hot, time=week, limit=200
2025-05-29 18:40:52,184 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 97 posts with sort=hot, time=week. New unique: 39. Total unique: 216
2025-05-29 18:40:52,184 [INFO] scraper.reddit_scraper.APIClient:116:    [4/6] Searching with sort=new, time=week, limit=200
2025-05-29 18:40:54,300 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 98. Total unique: 314
2025-05-29 18:40:54,300 [INFO] scraper.reddit_scraper.APIClient:116:    [5/6] Searching with sort=relevance, time=week, limit=100
2025-05-29 18:40:55,833 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 96 posts with sort=relevance, time=week. New unique: 0. Total unique: 314
2025-05-29 18:40:55,833 [INFO] scraper.reddit_scraper.APIClient:116:    [6/6] Searching with sort=new, time=week, limit=100
2025-05-29 18:40:58,343 [INFO] scraper.reddit_scraper.APIClient:130:    [FOUND] Found 100 posts with sort=new, time=week. New unique: 0. Total unique: 314
2025-05-29 18:40:58,344 [INFO] scraper.reddit_scraper.APIClient:143: 
[SUCCESS] COMPREHENSIVE SEARCH COMPLETED FOR 'machine learning':
[STATS] Total unique posts found: 314
[TARGET] Target was: 1000 posts  
[PERIOD] Time period: Last 7 days only
[SUBS] Unique subreddits: 228
[RESULT] SUCCESS: Found 314 posts that will be processed and saved

2025-05-29 18:40:58,344 [INFO] scraper.reddit_scraper:415: Reddit API returned 314 mentions
2025-05-29 18:40:58,345 [INFO] scraper.reddit_scraper:487: Post-processing 314 mentions...
2025-05-29 18:40:58,345 [INFO] scraper.reddit_scraper:589: Deduplication: 314 -> 314 mentions (removed 0 exact duplicates)
2025-05-29 18:40:58,345 [INFO] scraper.reddit_scraper:491: After deduplication: 314 mentions
2025-05-29 18:40:58,346 [INFO] scraper.reddit_scraper:653: Validation: 314 raw mentions  314 kept (EXTREMELY LENIENT - keeping everything possible)
2025-05-29 18:40:58,346 [INFO] scraper.reddit_scraper:495: After validation: 314 mentions
2025-05-29 18:40:58,900 [INFO] scraper.reddit_scraper:512: After database mapping: 314 mentions (ALL KEPT - no quality filtering)
2025-05-29 18:40:58,900 [INFO] scraper.reddit_scraper:524: Enhanced 314 mentions with sentiment analysis
2025-05-29 18:40:58,900 [INFO] scraper.reddit_scraper:538: Final result: 314 mentions (ALL CAPTURED, ZERO FILTERING)
2025-05-29 18:40:58,903 [INFO] scraper.reddit_scraper:544: 
        Scraping completed for 'machine learning':
        - Duration: 16.71s
        - Pages scraped: 0
        - Mentions found: 314
        - Success rate: 0.00%
        - Errors: 0
        - Retries: 0
        - Rate limit hits: 0
        - Cache hits: 0
        
2025-05-29 18:40:58,904 [INFO] analytics.data_validator:236: Validating dataset of 314 mentions
2025-05-29 18:41:06,997 [INFO] analytics.data_validator:259: Validation complete: 314/314 mentions passed
2025-05-29 18:41:06,997 [INFO] app:661: Data validation: 314/314 mentions passed
2025-05-29 18:41:08,432 [INFO] app:721: Initial metrics from analyzer: ['overview', 'engagement', 'temporal', 'subreddit_analysis', 'sentiment', 'content_analysis', 'trending', 'author_analysis', 'performance_analysis', 'quality_analysis', 'competition_analysis']
2025-05-29 18:41:08,432 [INFO] app:752: Final metrics structure: ['overview', 'engagement', 'temporal', 'subreddit_analysis', 'sentiment', 'content_analysis', 'trending', 'author_analysis', 'performance_analysis', 'quality_analysis', 'competition_analysis']
2025-05-29 18:41:08,433 [INFO] app:756: Temporal metrics keys: ['daily_timeline', 'hourly_distribution', 'day_of_week_distribution', 'trend', 'peak_day', 'peak_hour']
2025-05-29 18:41:08,433 [INFO] app:758: Author analysis keys: ['total_unique_authors', 'single_post_authors', 'multi_post_authors', 'author_diversity_ratio', 'top_authors_by_posts', 'top_authors_by_engagement', 'avg_posts_per_author', 'most_active_author', 'most_active_author_posts']
2025-05-29 18:41:08,433 [INFO] app:760: Quality analysis keys: ['avg_title_length', 'avg_content_length', 'avg_title_quality', 'avg_content_quality', 'avg_overall_quality', 'quality_distribution', 'posts_with_questions', 'posts_with_numbers', 'posts_with_links', 'quality_vs_engagement_correlation', 'best_quality_posts']
2025-05-29 18:41:08,433 [INFO] app:810: Search completed: machine learning -> 314 mentions
2025-05-29 18:41:08,626 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:41:23,462 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:41:23,471 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:41:29,547 [INFO] database.cache_manager:19: Redis cache connected successfully
2025-05-29 18:41:32,503 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:41:32,653 [DEBUG] app:1598: Monitor refresh completed successfully
2025-05-29 18:41:32,655 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:42:02,174 [DEBUG] matplotlib.pyplot:497: Loaded backend agg version v2.2.
2025-05-29 18:42:02,313 [DEBUG] app:1598: Monitor refresh completed successfully
2025-05-29 18:42:02,315 [DEBUG] matplotlib.pyplot:497: Loaded backend tkagg version 8.6.
2025-05-29 18:42:23,607 [WARNING] monitoring.system_monitor:527: ALERT [WARNING] anomaly_detection: Anomaly detected in network_bytes_sent: 89954624.00 (avg: 84679179.00, std: 1663351.47)
