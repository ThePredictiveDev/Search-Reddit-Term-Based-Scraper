<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="pytest" errors="14" failures="64" skipped="0" tests="121" time="97.564" timestamp="2025-05-29T02:05:54.030574+05:30" hostname="DESKTOP-P3MSMOS"><testcase classname="tests.test_database" name="test_database_initialization" time="0.023" /><testcase classname="tests.test_database" name="test_create_search_session" time="0.020" /><testcase classname="tests.test_database" name="test_add_mention" time="0.007"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_database.py, line 38&#10;  @pytest.mark.unit&#10;  def test_add_mention(db_manager, sample_mention):&#10;E       fixture 'sample_mention' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_database.py:38&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_database.py, line 38
  @pytest.mark.unit
  def test_add_mention(db_manager, sample_mention):
E       fixture 'sample_mention' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_database.py:38</error></testcase><testcase classname="tests.test_database" name="test_get_mentions_by_session" time="0.013"><failure message="sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: reddit_mentions.post_type&#10;[SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]&#10;[parameters: (1, 'test0', None, 'Test post 0 about OpenAI', 'This is a test post discussing OpenAI technology and its impact.', 'test_user', 'technology', 'https://reddit.com/r/technology/comments/test123', 10, 15, None, '2025-05-28 20:36:08.584201', '2025-05-28 20:36:08.584201', -0.5, 0.8)]&#10;(Background on this error at: https://sqlalche.me/e/20/gkpj)">self = &lt;sqlalchemy.engine.base.Connection object at 0x000002962DE85430&gt;
dialect = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x000002962DE85070&gt;
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x000002962DE85F10&gt;
statement = &lt;sqlalchemy.dialects.sqlite.base.SQLiteCompiler object at 0x000002962DE85F70&gt;
parameters = [(1, 'test0', None, 'Test post 0 about OpenAI', 'This is a test post discussing OpenAI technology and its impact.', 'test_user', ...)]

    def _exec_single_context(
        self,
        dialect: Dialect,
        context: ExecutionContext,
        statement: Union[str, Compiled],
        parameters: Optional[_AnyMultiExecuteParams],
    ) -&gt; CursorResult[Any]:
        """continue the _execute_context() method for a single DBAPI
        cursor.execute() or cursor.executemany() call.
    
        """
        if dialect.bind_typing is BindTyping.SETINPUTSIZES:
            generic_setinputsizes = context._prepare_set_input_sizes()
    
            if generic_setinputsizes:
                try:
                    dialect.do_set_input_sizes(
                        context.cursor, generic_setinputsizes, context
                    )
                except BaseException as e:
                    self._handle_dbapi_exception(
                        e, str(statement), parameters, None, context
                    )
    
        cursor, str_statement, parameters = (
            context.cursor,
            context.statement,
            context.parameters,
        )
    
        effective_parameters: Optional[_AnyExecuteParams]
    
        if not context.executemany:
            effective_parameters = parameters[0]
        else:
            effective_parameters = parameters
    
        if self._has_events or self.engine._has_events:
            for fn in self.dispatch.before_cursor_execute:
                str_statement, effective_parameters = fn(
                    self,
                    cursor,
                    str_statement,
                    effective_parameters,
                    context,
                    context.executemany,
                )
    
        if self._echo:
            self._log_info(str_statement)
    
            stats = context._get_cache_stats()
    
            if not self.engine.hide_parameters:
                self._log_info(
                    "[%s] %r",
                    stats,
                    sql_util._repr_params(
                        effective_parameters,
                        batches=10,
                        ismulti=context.executemany,
                    ),
                )
            else:
                self._log_info(
                    "[%s] [SQL parameters hidden due to hide_parameters=True]",
                    stats,
                )
    
        evt_handled: bool = False
        try:
            if context.execute_style is ExecuteStyle.EXECUTEMANY:
                effective_parameters = cast(
                    "_CoreMultiExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_executemany:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_executemany(
                        cursor,
                        str_statement,
                        effective_parameters,
                        context,
                    )
            elif not effective_parameters and context.no_parameters:
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute_no_params:
                        if fn(cursor, str_statement, context):
                            evt_handled = True
                            break
                if not evt_handled:
                    self.dialect.do_execute_no_params(
                        cursor, str_statement, context
                    )
            else:
                effective_parameters = cast(
                    "_CoreSingleExecuteParams", effective_parameters
                )
                if self.dialect._has_events:
                    for fn in self.dialect.dispatch.do_execute:
                        if fn(
                            cursor,
                            str_statement,
                            effective_parameters,
                            context,
                        ):
                            evt_handled = True
                            break
                if not evt_handled:
&gt;                   self.dialect.do_execute(
                        cursor, str_statement, effective_parameters, context
                    )

..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\engine\base.py:1964: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x000002962DE85070&gt;
cursor = &lt;sqlite3.Cursor object at 0x000002962DEF31C0&gt;
statement = 'INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_com...ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)'
parameters = (1, 'test0', None, 'Test post 0 about OpenAI', 'This is a test post discussing OpenAI technology and its impact.', 'test_user', ...)
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x000002962DE85F10&gt;

    def do_execute(self, cursor, statement, parameters, context=None):
&gt;       cursor.execute(statement, parameters)
E       sqlite3.IntegrityError: NOT NULL constraint failed: reddit_mentions.post_type

..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\engine\default.py:942: IntegrityError

The above exception was the direct cause of the following exception:

db_manager = &lt;database.models.DatabaseManager object at 0x000002962DE84A10&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...logy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 36, 8, 584201), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_get_mentions_by_session(db_manager, sample_mentions_list):
        """Test retrieving mentions by session."""
        session_id = db_manager.create_search_session("test")
    
        # Add multiple mentions
        for mention in sample_mentions_list:
            mention_data = mention.copy()
            mention_data['session_id'] = session_id
&gt;           db_manager.add_mention(mention_data)

tests\test_database.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
database\models.py:97: in add_mention
    db.commit()
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\orm\session.py:2032: in commit
    trans.commit(_to_root=True)
&lt;string&gt;:2: in commit
    ???
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\orm\state_changes.py:139: in _go
    ret_value = fn(self, *arg, **kw)
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\orm\session.py:1313: in commit
    self._prepare_impl()
&lt;string&gt;:2: in _prepare_impl
    ???
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\orm\state_changes.py:139: in _go
    ret_value = fn(self, *arg, **kw)
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\orm\session.py:1288: in _prepare_impl
    self.session.flush()
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\orm\session.py:4353: in flush
    self._flush(objects)
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\orm\session.py:4488: in _flush
    with util.safe_reraise():
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\util\langhelpers.py:146: in __exit__
    raise exc_value.with_traceback(exc_tb)
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\orm\session.py:4449: in _flush
    flush_context.execute()
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\orm\unitofwork.py:466: in execute
    rec.execute(self)
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\orm\unitofwork.py:642: in execute
    util.preloaded.orm_persistence.save_obj(
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\orm\persistence.py:93: in save_obj
    _emit_insert_statements(
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\orm\persistence.py:1233: in _emit_insert_statements
    result = connection.execute(
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\engine\base.py:1416: in execute
    return meth(
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\sql\elements.py:523: in _execute_on_connection
    return connection._execute_clauseelement(
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\engine\base.py:1638: in _execute_clauseelement
    ret = self._execute_context(
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\engine\base.py:1843: in _execute_context
    return self._exec_single_context(
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\engine\base.py:1983: in _exec_single_context
    self._handle_dbapi_exception(
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\engine\base.py:2352: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\engine\base.py:1964: in _exec_single_context
    self.dialect.do_execute(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;sqlalchemy.dialects.sqlite.pysqlite.SQLiteDialect_pysqlite object at 0x000002962DE85070&gt;
cursor = &lt;sqlite3.Cursor object at 0x000002962DEF31C0&gt;
statement = 'INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_com...ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)'
parameters = (1, 'test0', None, 'Test post 0 about OpenAI', 'This is a test post discussing OpenAI technology and its impact.', 'test_user', ...)
context = &lt;sqlalchemy.dialects.sqlite.base.SQLiteExecutionContext object at 0x000002962DE85F10&gt;

    def do_execute(self, cursor, statement, parameters, context=None):
&gt;       cursor.execute(statement, parameters)
E       sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) NOT NULL constraint failed: reddit_mentions.post_type
E       [SQL: INSERT INTO reddit_mentions (session_id, reddit_id, post_type, title, content, author, subreddit, url, score, num_comments, upvote_ratio, created_utc, scraped_at, sentiment_score, relevance_score) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E       [parameters: (1, 'test0', None, 'Test post 0 about OpenAI', 'This is a test post discussing OpenAI technology and its impact.', 'test_user', 'technology', 'https://reddit.com/r/technology/comments/test123', 10, 15, None, '2025-05-28 20:36:08.584201', '2025-05-28 20:36:08.584201', -0.5, 0.8)]
E       (Background on this error at: https://sqlalche.me/e/20/gkpj)

..\..\AppData\Roaming\Python\Python312\site-packages\sqlalchemy\engine\default.py:942: IntegrityError</failure></testcase><testcase classname="tests.test_database" name="test_update_session_status" time="0.014"><failure message="AttributeError: 'DatabaseManager' object has no attribute 'update_session_status'">db_manager = &lt;database.models.DatabaseManager object at 0x000002962DE2B6E0&gt;

    @pytest.mark.unit
    def test_update_session_status(db_manager):
        """Test updating session status."""
        session_id = db_manager.create_search_session("test")
    
        # Update to completed
&gt;       db_manager.update_session_status(session_id, "completed")
E       AttributeError: 'DatabaseManager' object has no attribute 'update_session_status'

tests\test_database.py:81: AttributeError</failure></testcase><testcase classname="tests.test_database" name="test_get_session_statistics" time="0.013"><failure message="TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given">db_manager = &lt;database.models.DatabaseManager object at 0x000002962DF28D70&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...logy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 36, 9, 374201), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_get_session_statistics(db_manager, sample_mentions_list):
        """Test getting session statistics."""
        session_id = db_manager.create_search_session("test")
    
        # Add mentions with different scores
        for mention in sample_mentions_list:
&gt;           db_manager.add_mention(session_id, mention)
E           TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given

tests\test_database.py:95: TypeError</failure></testcase><testcase classname="tests.test_database" name="test_search_mentions" time="0.013"><failure message="TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given">db_manager = &lt;database.models.DatabaseManager object at 0x000002962F0FC4A0&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...logy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 36, 9, 396201), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_search_mentions(db_manager, sample_mentions_list):
        """Test searching mentions with filters."""
        session_id = db_manager.create_search_session("test")
    
        # Add mentions
        for mention in sample_mentions_list:
&gt;           db_manager.add_mention(session_id, mention)
E           TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given

tests\test_database.py:112: TypeError</failure></testcase><testcase classname="tests.test_database" name="test_get_trending_subreddits" time="0.012"><failure message="TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given">db_manager = &lt;database.models.DatabaseManager object at 0x000002962DF2A150&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...logy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 36, 9, 417200), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_get_trending_subreddits(db_manager, sample_mentions_list):
        """Test getting trending subreddits."""
        session_id = db_manager.create_search_session("test")
    
        # Add mentions from different subreddits
        for mention in sample_mentions_list:
&gt;           db_manager.add_mention(session_id, mention)
E           TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given

tests\test_database.py:135: TypeError</failure></testcase><testcase classname="tests.test_database" name="test_get_sentiment_distribution" time="0.012"><failure message="TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given">db_manager = &lt;database.models.DatabaseManager object at 0x000002962F0FE450&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...logy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 36, 9, 439200), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_get_sentiment_distribution(db_manager, sample_mentions_list):
        """Test getting sentiment distribution."""
        session_id = db_manager.create_search_session("test")
    
        # Add mentions with various sentiments
        for mention in sample_mentions_list:
&gt;           db_manager.add_mention(session_id, mention)
E           TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given

tests\test_database.py:154: TypeError</failure></testcase><testcase classname="tests.test_database" name="test_get_time_series_data" time="0.012"><failure message="TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given">db_manager = &lt;database.models.DatabaseManager object at 0x000002962EF8C800&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...logy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 36, 9, 458202), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_get_time_series_data(db_manager, sample_mentions_list):
        """Test getting time series data."""
        session_id = db_manager.create_search_session("test")
    
        # Add mentions with different timestamps
        for mention in sample_mentions_list:
&gt;           db_manager.add_mention(session_id, mention)
E           TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given

tests\test_database.py:175: TypeError</failure></testcase><testcase classname="tests.test_database" name="test_cleanup_old_sessions" time="0.012"><failure message="AttributeError: 'DatabaseManager' object has no attribute 'Session'">db_manager = &lt;database.models.DatabaseManager object at 0x000002962EF8E7E0&gt;

    @pytest.mark.unit
    def test_cleanup_old_sessions(db_manager):
        """Test cleaning up old sessions."""
        # Create old session
        old_session_id = db_manager.create_search_session("old_test")
    
        # Manually set old timestamp
&gt;       with db_manager.Session() as session:
E       AttributeError: 'DatabaseManager' object has no attribute 'Session'

tests\test_database.py:195: AttributeError</failure></testcase><testcase classname="tests.test_database" name="test_database_error_handling" time="0.014"><failure message="TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given">db_manager = &lt;database.models.DatabaseManager object at 0x000002962EF9D8B0&gt;

    @pytest.mark.unit
    def test_database_error_handling(db_manager):
        """Test database error handling."""
        # Test with invalid session ID
        mentions = db_manager.get_mentions_by_session("invalid_session_id")
        assert mentions == []
    
        # Test with invalid mention data
        session_id = db_manager.create_search_session("error_test")
    
        invalid_mention = {
            'reddit_id': None,  # Invalid - should be string
            'title': 'Test',
            'content': 'Test content'
        }
    
        # Should handle gracefully
&gt;       mention_id = db_manager.add_mention(session_id, invalid_mention)
E       TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given

tests\test_database.py:234: TypeError</failure></testcase><testcase classname="tests.test_database" name="test_concurrent_database_access" time="0.062" /><testcase classname="tests.test_database" name="test_database_backup_restore" time="0.012"><failure message="TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given">db_manager = &lt;database.models.DatabaseManager object at 0x000002962EF4C0E0&gt;

    @pytest.mark.unit
    def test_database_backup_restore(db_manager):
        """Test database backup and restore functionality."""
        # Create test data
        session_id = db_manager.create_search_session("backup_test")
    
        test_mention = {
            'reddit_id': 'backup_test_1',
            'title': 'Backup test post',
            'content': 'This is a backup test.',
            'author': 'test_user',
            'subreddit': 'test',
            'score': 42,
            'num_comments': 5,
            'created_utc': datetime.utcnow(),
            'sentiment_score': 0.5,
            'relevance_score': 0.8
        }
    
&gt;       db_manager.add_mention(session_id, test_mention)
E       TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given

tests\test_database.py:290: TypeError</failure></testcase><testcase classname="tests.test_database" name="test_async_database_operations" time="0.131" /><testcase classname="tests.test_database.TestSearchSession" name="test_search_session_creation" time="0.001"><failure message="assert None is not None&#10; +  where None = &lt;database.models.SearchSession object at 0x000002962EF4FE60&gt;.created_at">self = &lt;tests.test_database.TestSearchSession object at 0x000002967689A240&gt;

    @pytest.mark.unit
    def test_search_session_creation(self):
        """Test SearchSession model creation."""
        session = SearchSession(
            search_term="test",
            status="active"
        )
    
        assert session.search_term == "test"
        assert session.status == "active"
&gt;       assert session.created_at is not None
E       assert None is not None
E        +  where None = &lt;database.models.SearchSession object at 0x000002962EF4FE60&gt;.created_at

tests\test_database.py:344: AssertionError</failure></testcase><testcase classname="tests.test_database.TestSearchSession" name="test_search_session_to_dict" time="0.001"><failure message="AttributeError: 'SearchSession' object has no attribute 'to_dict'">self = &lt;tests.test_database.TestSearchSession object at 0x0000029676899EE0&gt;

    @pytest.mark.unit
    def test_search_session_to_dict(self):
        """Test SearchSession to_dict method."""
        session = SearchSession(
            search_term="test",
            status="completed"
        )
    
&gt;       session_dict = session.to_dict()
E       AttributeError: 'SearchSession' object has no attribute 'to_dict'

tests\test_database.py:355: AttributeError</failure></testcase><testcase classname="tests.test_database.TestRedditMention" name="test_reddit_mention_creation" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_database.py, line 367&#10;      @pytest.mark.unit&#10;      def test_reddit_mention_creation(self, sample_mention):&#10;E       fixture 'sample_mention' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_database.py:367&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_database.py, line 367
      @pytest.mark.unit
      def test_reddit_mention_creation(self, sample_mention):
E       fixture 'sample_mention' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_database.py:367</error></testcase><testcase classname="tests.test_database.TestRedditMention" name="test_reddit_mention_to_dict" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_database.py, line 377&#10;      @pytest.mark.unit&#10;      def test_reddit_mention_to_dict(self, sample_mention):&#10;E       fixture 'sample_mention' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_database.py:377&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_database.py, line 377
      @pytest.mark.unit
      def test_reddit_mention_to_dict(self, sample_mention):
E       fixture 'sample_mention' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_database.py:377</error></testcase><testcase classname="tests.test_database.TestRedditMention" name="test_reddit_mention_validation" time="0.001"><failure message="assert None == 0&#10; +  where None = &lt;database.models.RedditMention object at 0x000002962EF9EFF0&gt;.score">self = &lt;tests.test_database.TestRedditMention object at 0x000002967689AB10&gt;

    @pytest.mark.unit
    def test_reddit_mention_validation(self):
        """Test RedditMention validation."""
        # Test with minimal required fields
        minimal_mention = RedditMention(
            reddit_id="test123",
            title="Test Title"
        )
    
        assert minimal_mention.reddit_id == "test123"
        assert minimal_mention.title == "Test Title"
    
        # Optional fields should have defaults
&gt;       assert minimal_mention.score == 0
E       assert None == 0
E        +  where None = &lt;database.models.RedditMention object at 0x000002962EF9EFF0&gt;.score

tests\test_database.py:401: AssertionError</failure></testcase><testcase classname="tests.test_scraper.TestRedditScraper" name="test_scraper_initialization" time="0.040" /><testcase classname="tests.test_scraper.TestRedditScraper" name="test_sanitize_search_term" time="0.036" /><testcase classname="tests.test_scraper.TestRedditScraper" name="test_generate_cache_key" time="0.036" /><testcase classname="tests.test_scraper.TestRedditScraper" name="test_extract_reddit_id" time="0.036" /><testcase classname="tests.test_scraper.TestRedditScraper" name="test_parse_score" time="0.034" /><testcase classname="tests.test_scraper.TestRedditScraper" name="test_parse_comment_count" time="0.034" /><testcase classname="tests.test_scraper.TestRedditScraper" name="test_calculate_relevance_score" time="0.034"><failure message="TypeError: RedditScraper._calculate_relevance_score() missing 1 required positional argument: 'search_term'">self = &lt;tests.test_scraper.TestRedditScraper object at 0x00000296291E7B30&gt;
scraper = &lt;scraper.reddit_scraper.RedditScraper object at 0x000002962EF9F860&gt;

    @pytest.mark.unit
    def test_calculate_relevance_score(self, scraper):
        """Test relevance score calculation."""
        # High relevance post
        high_relevance = {
            'title': 'OpenAI releases new GPT model',
            'content': 'OpenAI has announced a breakthrough in AI technology',
            'score': 1000,
            'num_comments': 500
        }
    
&gt;       score = scraper._calculate_relevance_score(high_relevance, "OpenAI")
E       TypeError: RedditScraper._calculate_relevance_score() missing 1 required positional argument: 'search_term'

tests\test_scraper.py:104: TypeError</failure></testcase><testcase classname="tests.test_scraper.TestRedditScraper" name="test_calculate_basic_sentiment" time="0.190" /><testcase classname="tests.test_scraper.TestRedditScraper" name="test_is_relevant" time="0.040" /><testcase classname="tests.test_scraper.TestRedditScraper" name="test_remove_duplicates" time="0.037" /><testcase classname="tests.test_scraper.TestScrapingWithMocks" name="test_extract_post_data_success" time="0.048" /><testcase classname="tests.test_scraper.TestScrapingWithMocks" name="test_extract_post_data_missing_elements" time="0.038" /><testcase classname="tests.test_scraper.TestScrapingWithMocks" name="test_handle_popups" time="3.060" /><testcase classname="tests.test_scraper.TestScrapingWithMocks" name="test_scrape_mentions_with_cache_hit" time="0.038" /><testcase classname="tests.test_scraper.TestScrapingWithMocks" name="test_scrape_mentions_error_handling" time="0.040" /><testcase classname="tests.test_scraper.TestScrapingWithMocks" name="test_circuit_breaker_functionality" time="0.039" /><testcase classname="tests.test_scraper.TestScrapingPerformance" name="test_rate_limiting" time="2.172" /><testcase classname="tests.test_scraper.TestScrapingPerformance" name="test_concurrent_scraping_limits" time="0.153" /><testcase classname="tests.test_scraper.TestScrapingIntegration" name="test_full_scraping_pipeline_mock" time="60.872"><failure message="scraper.reddit_scraper.ScrapingError: Scraping failed for 'OpenAI': 'RedditScraper' object has no attribute '_post_process_mentions'">self = &lt;scraper.reddit_scraper.RedditScraper object at 0x000002962F8EA7E0&gt;
search_term = 'OpenAI', session_id = 1, max_pages = 1, progress_callback = None
use_advanced_patterns = True, quality_threshold = 0.3

    async def scrape_mentions(
        self,
        search_term: str,
        session_id: int,
        max_pages: int = 5,
        progress_callback: Optional[Callable] = None,
        use_advanced_patterns: bool = True,
        quality_threshold: float = 0.3
    ) -&gt; List[Dict[str, Any]]:
        """
        Enhanced scraping with comprehensive error handling and quality filtering.
        """
        # Input validation and sanitization
        search_term = self._sanitize_search_term(search_term)
        if not search_term:
            raise ValueError("Invalid search term after sanitization")
    
        # Check circuit breaker
        if not self._check_circuit_breaker():
            raise ScrapingError("Circuit breaker is open, scraping temporarily disabled")
    
        # Initialize metrics
        metrics = ScrapingMetrics(start_time=datetime.utcnow())
    
        # Check cache with enhanced key
        cache_key = self._generate_cache_key(search_term, max_pages, quality_threshold)
        if self.cache_manager:
            cached_mentions = self.cache_manager.get_search_results(cache_key)
            if cached_mentions:
                metrics.cache_hits = 1
                self.logger.info(f"Found {len(cached_mentions)} cached mentions for '{search_term}'")
                if self.monitor:
                    self.monitor.emit_cache_hit(search_term, len(cached_mentions))
                return cached_mentions
    
        all_mentions = []
    
        try:
            # Enhanced browser setup with stealth mode
            async with async_playwright() as p:
                browser = await self._create_stealth_browser(p)
    
                try:
                    context = await self._create_browser_context(browser)
                    page = await context.new_page()
    
                    # Set up page monitoring
                    await self._setup_page_monitoring(page)
    
                    # Progressive scraping strategy
                    if use_advanced_patterns:
                        mentions = await self._progressive_scraping(
                            page, search_term, session_id, max_pages,
                            progress_callback, metrics, quality_threshold
                        )
                    else:
                        mentions = await self._basic_scraping(
                            page, search_term, session_id, max_pages,
                            progress_callback, metrics
                        )
    
                    all_mentions.extend(mentions)
    
                finally:
                    await browser.close()
    
            # Post-processing and quality enhancement
            if progress_callback:
                progress_callback("#x1F50D Processing and enhancing mentions...", 0.9)
    
&gt;           enhanced_mentions = await self._post_process_mentions(
                all_mentions, search_term, quality_threshold, metrics
            )
E           AttributeError: 'RedditScraper' object has no attribute '_post_process_mentions'

scraper\reddit_scraper.py:316: AttributeError

The above exception was the direct cause of the following exception:

self = &lt;tests.test_scraper.TestScrapingIntegration object at 0x0000029629208440&gt;
db_manager = &lt;database.models.DatabaseManager object at 0x000002962F8EA9C0&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 36, 15, 917808), 'num_comments': 15, ...}, ...]

    @pytest.mark.asyncio
    async def test_full_scraping_pipeline_mock(self, db_manager, sample_mentions_list):
        """Test full scraping pipeline with mocked browser."""
        scraper = RedditScraper(db_manager)
    
        # Mock the entire browser interaction
        with patch('scraper.reddit_scraper.async_playwright') as mock_playwright:
            # Setup mock browser
            mock_browser = AsyncMock()
            mock_context = AsyncMock()
            mock_page = AsyncMock()
    
            mock_playwright.return_value.__aenter__.return_value.chromium.launch.return_value = mock_browser
            mock_browser.new_context.return_value = mock_context
            mock_context.new_page.return_value = mock_page
    
            # Mock page interactions
            mock_page.goto = AsyncMock()
            mock_page.wait_for_selector = AsyncMock()
            mock_page.query_selector_all.return_value = []  # No posts found
    
            # Run scraping
&gt;           result = await scraper.scrape_mentions("OpenAI", 1, max_pages=1)

tests\test_scraper.py:389: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;scraper.reddit_scraper.RedditScraper object at 0x000002962F8EA7E0&gt;
search_term = 'OpenAI', session_id = 1, max_pages = 1, progress_callback = None
use_advanced_patterns = True, quality_threshold = 0.3

    async def scrape_mentions(
        self,
        search_term: str,
        session_id: int,
        max_pages: int = 5,
        progress_callback: Optional[Callable] = None,
        use_advanced_patterns: bool = True,
        quality_threshold: float = 0.3
    ) -&gt; List[Dict[str, Any]]:
        """
        Enhanced scraping with comprehensive error handling and quality filtering.
        """
        # Input validation and sanitization
        search_term = self._sanitize_search_term(search_term)
        if not search_term:
            raise ValueError("Invalid search term after sanitization")
    
        # Check circuit breaker
        if not self._check_circuit_breaker():
            raise ScrapingError("Circuit breaker is open, scraping temporarily disabled")
    
        # Initialize metrics
        metrics = ScrapingMetrics(start_time=datetime.utcnow())
    
        # Check cache with enhanced key
        cache_key = self._generate_cache_key(search_term, max_pages, quality_threshold)
        if self.cache_manager:
            cached_mentions = self.cache_manager.get_search_results(cache_key)
            if cached_mentions:
                metrics.cache_hits = 1
                self.logger.info(f"Found {len(cached_mentions)} cached mentions for '{search_term}'")
                if self.monitor:
                    self.monitor.emit_cache_hit(search_term, len(cached_mentions))
                return cached_mentions
    
        all_mentions = []
    
        try:
            # Enhanced browser setup with stealth mode
            async with async_playwright() as p:
                browser = await self._create_stealth_browser(p)
    
                try:
                    context = await self._create_browser_context(browser)
                    page = await context.new_page()
    
                    # Set up page monitoring
                    await self._setup_page_monitoring(page)
    
                    # Progressive scraping strategy
                    if use_advanced_patterns:
                        mentions = await self._progressive_scraping(
                            page, search_term, session_id, max_pages,
                            progress_callback, metrics, quality_threshold
                        )
                    else:
                        mentions = await self._basic_scraping(
                            page, search_term, session_id, max_pages,
                            progress_callback, metrics
                        )
    
                    all_mentions.extend(mentions)
    
                finally:
                    await browser.close()
    
            # Post-processing and quality enhancement
            if progress_callback:
                progress_callback("#x1F50D Processing and enhancing mentions...", 0.9)
    
            enhanced_mentions = await self._post_process_mentions(
                all_mentions, search_term, quality_threshold, metrics
            )
    
            # Cache results with enhanced key
            if self.cache_manager and enhanced_mentions:
                self.cache_manager.set_search_results(cache_key, enhanced_mentions)
    
            # Record success
            self._record_success()
            metrics.end_time = datetime.utcnow()
            metrics.mentions_found = len(enhanced_mentions)
    
            # Log comprehensive metrics
            self._log_scraping_metrics(metrics, search_term)
    
            if progress_callback:
                progress_callback(f"✅ Completed! Found {len(enhanced_mentions)} high-quality mentions", 1.0)
    
            return enhanced_mentions
    
        except Exception as e:
            self._record_failure()
            metrics.end_time = datetime.utcnow()
            metrics.errors_encountered += 1
    
            error_msg = f"Scraping failed for '{search_term}': {str(e)}"
            self.logger.error(error_msg)
    
            if self.monitor:
                self.monitor.emit_search_failed(session_id, error_msg)
    
            if progress_callback:
                progress_callback(f"❌ Error: {error_msg}", 1.0)
    
            # Return partial results if any
            if all_mentions:
                self.logger.info(f"Returning {len(all_mentions)} partial results despite error")
                return all_mentions
    
&gt;           raise ScrapingError(error_msg) from e
E           scraper.reddit_scraper.ScrapingError: Scraping failed for 'OpenAI': 'RedditScraper' object has no attribute '_post_process_mentions'

scraper\reddit_scraper.py:356: ScrapingError</failure></testcase><testcase classname="tests.test_scraper.TestScrapingIntegration" name="test_post_processing_pipeline" time="0.039"><failure message="AttributeError: 'RedditScraper' object has no attribute '_post_process_mentions'">self = &lt;tests.test_scraper.TestScrapingIntegration object at 0x0000029629208E00&gt;
db_manager = &lt;database.models.DatabaseManager object at 0x00000296302B8650&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 16, 853117), 'num_comments': 15, ...}, ...]

    @pytest.mark.asyncio
    async def test_post_processing_pipeline(self, db_manager, sample_mentions_list):
        """Test post-processing pipeline."""
        scraper = RedditScraper(db_manager)
    
        # Test the post-processing method directly
&gt;       enhanced_mentions = await scraper._post_process_mentions(
            sample_mentions_list, "OpenAI", 0.3, None
        )
E       AttributeError: 'RedditScraper' object has no attribute '_post_process_mentions'

tests\test_scraper.py:403: AttributeError</failure></testcase><testcase classname="tests.test_scraper.TestScrapingIntegration" name="test_error_recovery" time="0.039"><failure message="ValueError: Invalid search term after sanitization">self = &lt;tests.test_scraper.TestScrapingIntegration object at 0x0000029629208530&gt;
db_manager = &lt;database.models.DatabaseManager object at 0x0000029630255880&gt;

    @pytest.mark.asyncio
    async def test_error_recovery(self, db_manager):
        """Test error recovery mechanisms."""
        scraper = RedditScraper(db_manager)
    
        # Test with invalid search term
&gt;       result = await scraper.scrape_mentions("", 1)

tests\test_scraper.py:422: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;scraper.reddit_scraper.RedditScraper object at 0x00000296302557C0&gt;
search_term = '', session_id = 1, max_pages = 5, progress_callback = None
use_advanced_patterns = True, quality_threshold = 0.3

    async def scrape_mentions(
        self,
        search_term: str,
        session_id: int,
        max_pages: int = 5,
        progress_callback: Optional[Callable] = None,
        use_advanced_patterns: bool = True,
        quality_threshold: float = 0.3
    ) -&gt; List[Dict[str, Any]]:
        """
        Enhanced scraping with comprehensive error handling and quality filtering.
        """
        # Input validation and sanitization
        search_term = self._sanitize_search_term(search_term)
        if not search_term:
&gt;           raise ValueError("Invalid search term after sanitization")
E           ValueError: Invalid search term after sanitization

scraper\reddit_scraper.py:261: ValueError</failure></testcase><testcase classname="tests.test_scraper.TestScrapingConfiguration" name="test_proxy_configuration" time="0.040" /><testcase classname="tests.test_scraper.TestScrapingConfiguration" name="test_user_agent_rotation" time="0.040" /><testcase classname="tests.test_scraper.TestScrapingConfiguration" name="test_search_patterns_configuration" time="0.039" /><testcase classname="tests.test_scraper.TestScrapingConfiguration" name="test_subreddit_categories" time="0.039" /><testcase classname="tests.test_scraper.TestScrapingConfiguration" name="test_build_search_patterns" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py, line 487&#10;      @pytest.mark.unit&#10;      def test_build_search_patterns(self, scraper):&#10;E       fixture 'scraper' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py:487&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py, line 487
      @pytest.mark.unit
      def test_build_search_patterns(self, scraper):
E       fixture 'scraper' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py:487</error></testcase><testcase classname="tests.test_scraper.TestScrapingConfiguration" name="test_filter_by_quality" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py, line 498&#10;      @pytest.mark.unit&#10;      def test_filter_by_quality(self, scraper):&#10;E       fixture 'scraper' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py:498&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py, line 498
      @pytest.mark.unit
      def test_filter_by_quality(self, scraper):
E       fixture 'scraper' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py:498</error></testcase><testcase classname="tests.test_scraper.TestScrapingConfiguration" name="test_deduplicate_mentions" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py, line 513&#10;      @pytest.mark.unit&#10;      def test_deduplicate_mentions(self, scraper):&#10;E       fixture 'scraper' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py:513&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py, line 513
      @pytest.mark.unit
      def test_deduplicate_mentions(self, scraper):
E       fixture 'scraper' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py:513</error></testcase><testcase classname="tests.test_scraper.TestScrapingConfiguration" name="test_validate_mention_data" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py, line 529&#10;      @pytest.mark.unit&#10;      def test_validate_mention_data(self, scraper):&#10;E       fixture 'scraper' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py:529&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py, line 529
      @pytest.mark.unit
      def test_validate_mention_data(self, scraper):
E       fixture 'scraper' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py:529</error></testcase><testcase classname="tests.test_scraper.TestScrapingConfiguration" name="test_error_handling" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py, line 565&#10;      @pytest.mark.unit&#10;      def test_error_handling(self, scraper):&#10;E       fixture 'scraper' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py:565&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py, line 565
      @pytest.mark.unit
      def test_error_handling(self, scraper):
E       fixture 'scraper' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py:565</error></testcase><testcase classname="tests.test_scraper.TestScrapingConfiguration" name="test_rate_limiting_configuration" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py, line 579&#10;      @pytest.mark.unit&#10;      def test_rate_limiting_configuration(self, scraper):&#10;E       fixture 'scraper' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py:579&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py, line 579
      @pytest.mark.unit
      def test_rate_limiting_configuration(self, scraper):
E       fixture 'scraper' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_scraper.py:579</error></testcase><testcase classname="tests.test_analytics.TestMetricsAnalyzer" name="test_analyzer_initialization" time="0.008" /><testcase classname="tests.test_analytics.TestMetricsAnalyzer" name="test_calculate_basic_metrics" time="0.045" /><testcase classname="tests.test_analytics.TestMetricsAnalyzer" name="test_analyze_session_metrics" time="0.032" /><testcase classname="tests.test_analytics.TestMetricsAnalyzer" name="test_calculate_temporal_metrics" time="0.008"><failure message="AttributeError: 'MetricsAnalyzer' object has no attribute 'calculate_temporal_metrics'. Did you mean: '_calculate_temporal_metrics'?">self = &lt;tests.test_analytics.TestMetricsAnalyzer object at 0x000002962922B860&gt;
metrics_analyzer = &lt;analytics.metrics_analyzer.MetricsAnalyzer object at 0x00000296304AD070&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 15, 37, 17, 233116), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_calculate_temporal_metrics(self, metrics_analyzer, sample_mentions_list):
        """Test temporal metrics calculation."""
        # Add timestamps to mentions
        now = datetime.utcnow()
        for i, mention in enumerate(sample_mentions_list):
            mention['created_utc'] = now - timedelta(hours=i)
    
&gt;       metrics = metrics_analyzer.calculate_temporal_metrics(sample_mentions_list)
E       AttributeError: 'MetricsAnalyzer' object has no attribute 'calculate_temporal_metrics'. Did you mean: '_calculate_temporal_metrics'?

tests\test_analytics.py:112: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestMetricsAnalyzer" name="test_calculate_subreddit_metrics" time="0.007"><failure message="AttributeError: 'MetricsAnalyzer' object has no attribute 'calculate_subreddit_metrics'. Did you mean: '_calculate_subreddit_metrics'?">self = &lt;tests.test_analytics.TestMetricsAnalyzer object at 0x000002962922BAD0&gt;
metrics_analyzer = &lt;analytics.metrics_analyzer.MetricsAnalyzer object at 0x00000296304AC2C0&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 17, 255116), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_calculate_subreddit_metrics(self, metrics_analyzer, sample_mentions_list):
        """Test subreddit metrics calculation."""
        # Add different subreddits
        subreddits = ['technology', 'programming', 'artificial', 'MachineLearning']
        for i, mention in enumerate(sample_mentions_list):
            mention['subreddit'] = subreddits[i % len(subreddits)]
    
&gt;       metrics = metrics_analyzer.calculate_subreddit_metrics(sample_mentions_list)
E       AttributeError: 'MetricsAnalyzer' object has no attribute 'calculate_subreddit_metrics'. Did you mean: '_calculate_subreddit_metrics'?

tests\test_analytics.py:130: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestMetricsAnalyzer" name="test_calculate_sentiment_metrics" time="0.008"><failure message="AttributeError: 'MetricsAnalyzer' object has no attribute 'calculate_sentiment_metrics'. Did you mean: '_calculate_sentiment_metrics'?">self = &lt;tests.test_analytics.TestMetricsAnalyzer object at 0x000002962922BD40&gt;
metrics_analyzer = &lt;analytics.metrics_analyzer.MetricsAnalyzer object at 0x00000296304AFC50&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 17, 275117), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_calculate_sentiment_metrics(self, metrics_analyzer, sample_mentions_list):
        """Test sentiment metrics calculation."""
&gt;       metrics = metrics_analyzer.calculate_sentiment_metrics(sample_mentions_list)
E       AttributeError: 'MetricsAnalyzer' object has no attribute 'calculate_sentiment_metrics'. Did you mean: '_calculate_sentiment_metrics'?

tests\test_analytics.py:142: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestMetricsAnalyzer" name="test_calculate_engagement_metrics" time="0.008"><failure message="AttributeError: 'MetricsAnalyzer' object has no attribute 'calculate_engagement_metrics'. Did you mean: '_calculate_engagement_metrics'?">self = &lt;tests.test_analytics.TestMetricsAnalyzer object at 0x000002962922BFB0&gt;
metrics_analyzer = &lt;analytics.metrics_analyzer.MetricsAnalyzer object at 0x00000296304FCB90&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 17, 297116), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_calculate_engagement_metrics(self, metrics_analyzer, sample_mentions_list):
        """Test engagement metrics calculation."""
&gt;       metrics = metrics_analyzer.calculate_engagement_metrics(sample_mentions_list)
E       AttributeError: 'MetricsAnalyzer' object has no attribute 'calculate_engagement_metrics'. Did you mean: '_calculate_engagement_metrics'?

tests\test_analytics.py:159: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestMetricsAnalyzer" name="test_detect_trending_topics" time="0.007"><failure message="AttributeError: 'MetricsAnalyzer' object has no attribute 'detect_trending_topics'">self = &lt;tests.test_analytics.TestMetricsAnalyzer object at 0x0000029629248260&gt;
metrics_analyzer = &lt;analytics.metrics_analyzer.MetricsAnalyzer object at 0x00000296304FE270&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 17, 315116), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_detect_trending_topics(self, metrics_analyzer, sample_mentions_list):
        """Test trending topic detection."""
        # Add keywords to titles
        keywords = ['AI', 'machine learning', 'neural networks', 'deep learning']
        for i, mention in enumerate(sample_mentions_list):
            mention['title'] = f"Post about {keywords[i % len(keywords)]} technology"
    
&gt;       trending = metrics_analyzer.detect_trending_topics(sample_mentions_list)
E       AttributeError: 'MetricsAnalyzer' object has no attribute 'detect_trending_topics'

tests\test_analytics.py:180: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestMetricsAnalyzer" name="test_generate_comprehensive_metrics" time="0.008"><failure message="AttributeError: 'MetricsAnalyzer' object has no attribute 'generate_comprehensive_metrics'">self = &lt;tests.test_analytics.TestMetricsAnalyzer object at 0x00000296292484D0&gt;
metrics_analyzer = &lt;analytics.metrics_analyzer.MetricsAnalyzer object at 0x00000296304ADD90&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 17, 340117), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_generate_comprehensive_metrics(self, metrics_analyzer, sample_mentions_list):
        """Test comprehensive metrics generation."""
&gt;       metrics = metrics_analyzer.generate_comprehensive_metrics(sample_mentions_list)
E       AttributeError: 'MetricsAnalyzer' object has no attribute 'generate_comprehensive_metrics'

tests\test_analytics.py:192: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestMetricsAnalyzer" name="test_empty_mentions_handling" time="0.007"><failure message="AttributeError: 'MetricsAnalyzer' object has no attribute 'generate_comprehensive_metrics'">self = &lt;tests.test_analytics.TestMetricsAnalyzer object at 0x0000029629248740&gt;
metrics_analyzer = &lt;analytics.metrics_analyzer.MetricsAnalyzer object at 0x00000296304AEAB0&gt;

    @pytest.mark.unit
    def test_empty_mentions_handling(self, metrics_analyzer):
        """Test handling of empty mentions list."""
&gt;       metrics = metrics_analyzer.generate_comprehensive_metrics([])
E       AttributeError: 'MetricsAnalyzer' object has no attribute 'generate_comprehensive_metrics'

tests\test_analytics.py:211: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestMetricsAnalyzer" name="test_malformed_data_handling" time="0.007"><failure message="AttributeError: 'MetricsAnalyzer' object has no attribute 'generate_comprehensive_metrics'">self = &lt;tests.test_analytics.TestMetricsAnalyzer object at 0x00000296292489B0&gt;
metrics_analyzer = &lt;analytics.metrics_analyzer.MetricsAnalyzer object at 0x0000029630447650&gt;

    @pytest.mark.unit
    def test_malformed_data_handling(self, metrics_analyzer):
        """Test handling of malformed data."""
        malformed_mentions = [
            {'title': 'Valid post', 'score': 10},  # Missing fields
            {'reddit_id': 'test', 'score': 'invalid'},  # Invalid score type
            {},  # Empty mention
            None  # None mention
        ]
    
        # Should handle gracefully without crashing
&gt;       metrics = metrics_analyzer.generate_comprehensive_metrics(malformed_mentions)
E       AttributeError: 'MetricsAnalyzer' object has no attribute 'generate_comprehensive_metrics'

tests\test_analytics.py:229: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestDataValidator" name="test_validator_initialization" time="0.001" /><testcase classname="tests.test_analytics.TestDataValidator" name="test_validate_mention_valid_data" time="0.248" /><testcase classname="tests.test_analytics.TestDataValidator" name="test_validate_mention_invalid_data" time="0.013" /><testcase classname="tests.test_analytics.TestDataValidator" name="test_validate_mention_spam_detection" time="0.018" /><testcase classname="tests.test_analytics.TestDataValidator" name="test_validate_dataset" time="0.169" /><testcase classname="tests.test_analytics.TestDataValidator" name="test_duplicate_detection" time="0.001" /><testcase classname="tests.test_analytics.TestDataValidator" name="test_language_analysis" time="0.012" /><testcase classname="tests.test_analytics.TestDataValidator" name="test_content_quality_analysis" time="0.001" /><testcase classname="tests.test_analytics.TestDataValidator" name="test_quality_metrics_calculation" time="0.144" /><testcase classname="tests.test_analytics.TestAdvancedSentimentAnalyzer" name="test_analyzer_initialization" time="0.001"><failure message="AssertionError: assert False&#10; +  where False = hasattr(&lt;analytics.advanced_sentiment.AdvancedSentimentAnalyzer object at 0x0000029633AE7FE0&gt;, 'providers')">self = &lt;tests.test_analytics.TestAdvancedSentimentAnalyzer object at 0x0000029629248BF0&gt;
sentiment_analyzer = &lt;analytics.advanced_sentiment.AdvancedSentimentAnalyzer object at 0x0000029633AE7FE0&gt;

    def test_analyzer_initialization(self, sentiment_analyzer):
        """Test sentiment analyzer initialization."""
        assert sentiment_analyzer is not None
&gt;       assert hasattr(sentiment_analyzer, 'providers')
E       AssertionError: assert False
E        +  where False = hasattr(&lt;analytics.advanced_sentiment.AdvancedSentimentAnalyzer object at 0x0000029633AE7FE0&gt;, 'providers')

tests\test_analytics.py:396: AssertionError</failure></testcase><testcase classname="tests.test_analytics.TestAdvancedSentimentAnalyzer" name="test_is_available" time="0.001" /><testcase classname="tests.test_analytics.TestAdvancedSentimentAnalyzer" name="test_analyze_sentiment_positive" time="0.002" /><testcase classname="tests.test_analytics.TestAdvancedSentimentAnalyzer" name="test_analyze_sentiment_negative" time="0.001" /><testcase classname="tests.test_analytics.TestAdvancedSentimentAnalyzer" name="test_analyze_sentiment_empty" time="0.012" /><testcase classname="tests.test_analytics.TestAdvancedSentimentAnalyzer" name="test_analyze_batch" time="0.002" /><testcase classname="tests.test_analytics.TestAdvancedSentimentAnalyzer" name="test_textblob_analysis" time="0.001"><failure message="AttributeError: 'AdvancedSentimentAnalyzer' object has no attribute '_analyze_textblob'">self = &lt;tests.test_analytics.TestAdvancedSentimentAnalyzer object at 0x0000029629249E50&gt;
sentiment_analyzer = &lt;analytics.advanced_sentiment.AdvancedSentimentAnalyzer object at 0x0000029633AF12B0&gt;

    def test_textblob_analysis(self, sentiment_analyzer):
        """Test TextBlob sentiment analysis."""
        text = "This is a great product!"
    
&gt;       result = sentiment_analyzer._analyze_textblob(text)
E       AttributeError: 'AdvancedSentimentAnalyzer' object has no attribute '_analyze_textblob'

tests\test_analytics.py:461: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestAdvancedSentimentAnalyzer" name="test_vader_analysis" time="0.001"><failure message="AttributeError: 'AdvancedSentimentAnalyzer' object has no attribute '_is_vader_available'">self = &lt;tests.test_analytics.TestAdvancedSentimentAnalyzer object at 0x000002962924A030&gt;
sentiment_analyzer = &lt;analytics.advanced_sentiment.AdvancedSentimentAnalyzer object at 0x0000029633AF1EB0&gt;

    def test_vader_analysis(self, sentiment_analyzer):
        """Test VADER sentiment analysis."""
        text = "This is a great product!"
    
&gt;       if sentiment_analyzer._is_vader_available():
E       AttributeError: 'AdvancedSentimentAnalyzer' object has no attribute '_is_vader_available'

tests\test_analytics.py:473: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestAdvancedSentimentAnalyzer" name="test_emotion_detection" time="0.001"><failure message="AttributeError: 'AdvancedSentimentAnalyzer' object has no attribute '_detect_emotions'">self = &lt;tests.test_analytics.TestAdvancedSentimentAnalyzer object at 0x000002962924A210&gt;
sentiment_analyzer = &lt;analytics.advanced_sentiment.AdvancedSentimentAnalyzer object at 0x0000029633AF2CF0&gt;

    def test_emotion_detection(self, sentiment_analyzer):
        """Test emotion detection."""
        emotional_texts = {
            "I'm so happy and excited!": "joy",
            "I'm really angry about this!": "anger",
            "This makes me so sad.": "sadness",
            "I'm terrified and scared.": "fear"
        }
    
        for text, expected_emotion in emotional_texts.items():
&gt;           emotions = sentiment_analyzer._detect_emotions(text)
E           AttributeError: 'AdvancedSentimentAnalyzer' object has no attribute '_detect_emotions'

tests\test_analytics.py:491: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestAnalyticsIntegration" name="test_full_analytics_pipeline" time="0.113"><failure message="AttributeError: 'MetricsAnalyzer' object has no attribute 'generate_comprehensive_metrics'">self = &lt;tests.test_analytics.TestAnalyticsIntegration object at 0x000002962924A480&gt;
metrics_analyzer = &lt;analytics.metrics_analyzer.MetricsAnalyzer object at 0x0000029633AE79B0&gt;
data_validator = &lt;analytics.data_validator.DataValidator object at 0x0000029633AF36E0&gt;
sample_mentions_list = [{'_quality_level': 'good', '_quality_score': 0.718749610501103, '_validation_issues': 2, 'author': 'test_user', ...},...uality_level': 'good', '_quality_score': 0.7687495811866125, '_validation_issues': 2, 'author': 'test_user', ...}, ...]

    def test_full_analytics_pipeline(self, metrics_analyzer, data_validator, sample_mentions_list):
        """Test full analytics pipeline."""
        # Step 1: Validate data
        validated_mentions, quality_metrics = data_validator.validate_dataset(sample_mentions_list)
    
        # Step 2: Generate metrics
&gt;       analytics_metrics = metrics_analyzer.generate_comprehensive_metrics(validated_mentions)
E       AttributeError: 'MetricsAnalyzer' object has no attribute 'generate_comprehensive_metrics'

tests\test_analytics.py:508: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestAnalyticsIntegration" name="test_analytics_with_empty_data" time="0.009"><failure message="AttributeError: 'MetricsAnalyzer' object has no attribute 'generate_comprehensive_metrics'">self = &lt;tests.test_analytics.TestAnalyticsIntegration object at 0x000002962924A6F0&gt;
metrics_analyzer = &lt;analytics.metrics_analyzer.MetricsAnalyzer object at 0x0000029633B26210&gt;
data_validator = &lt;analytics.data_validator.DataValidator object at 0x0000029633B25D60&gt;

    def test_analytics_with_empty_data(self, metrics_analyzer, data_validator):
        """Test analytics pipeline with empty data."""
        empty_mentions = []
    
        # Should handle empty data gracefully
        validated_mentions, quality_metrics = data_validator.validate_dataset(empty_mentions)
&gt;       analytics_metrics = metrics_analyzer.generate_comprehensive_metrics(validated_mentions)
E       AttributeError: 'MetricsAnalyzer' object has no attribute 'generate_comprehensive_metrics'

tests\test_analytics.py:525: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestAnalyticsIntegration" name="test_analytics_performance" time="9.567"><failure message="AttributeError: 'MetricsAnalyzer' object has no attribute 'generate_comprehensive_metrics'">self = &lt;tests.test_analytics.TestAnalyticsIntegration object at 0x000002962924A960&gt;
metrics_analyzer = &lt;analytics.metrics_analyzer.MetricsAnalyzer object at 0x0000029633B249B0&gt;
data_validator = &lt;analytics.data_validator.DataValidator object at 0x0000029633B24DA0&gt;
performance_monitor = &lt;tests.conftest.performance_monitor.&lt;locals&gt;.PerformanceMonitor object at 0x0000029633B24EF0&gt;

    def test_analytics_performance(self, metrics_analyzer, data_validator, performance_monitor):
        """Test analytics performance with large dataset."""
        # Create large dataset
        large_dataset = []
        base_mention = {
            'reddit_id': 'test',
            'title': 'Test post about technology',
            'content': 'This is a test post discussing various technology topics.',
            'author': 'test_user',
            'subreddit': 'technology',
            'score': 42,
            'num_comments': 15,
            'created_utc': datetime.utcnow(),
            'sentiment_score': 0.5,
            'relevance_score': 0.8
        }
    
        for i in range(1000):
            mention = base_mention.copy()
            mention['reddit_id'] = f'test_{i}'
            mention['title'] = f'Test post {i} about technology'
            large_dataset.append(mention)
    
        performance_monitor.start()
    
        # Run analytics pipeline
        validated_mentions, quality_metrics = data_validator.validate_dataset(large_dataset)
&gt;       analytics_metrics = metrics_analyzer.generate_comprehensive_metrics(validated_mentions)
E       AttributeError: 'MetricsAnalyzer' object has no attribute 'generate_comprehensive_metrics'

tests\test_analytics.py:558: AttributeError</failure></testcase><testcase classname="tests.test_analytics.TestAnalyticsEdgeCases" name="test_metrics_with_extreme_values" time="0.015"><failure message="TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given">self = &lt;tests.test_analytics.TestAnalyticsEdgeCases object at 0x000002962924AC00&gt;
db_manager = &lt;database.models.DatabaseManager object at 0x00000296317C3590&gt;

    def test_metrics_with_extreme_values(self, db_manager):
        """Test metrics calculation with extreme values."""
        metrics_analyzer = MetricsAnalyzer(db_manager)
    
        extreme_mentions = [
            {
                'reddit_id': 'extreme1',
                'title': 'Post with extreme score',
                'score': 999999,  # Very high score
                'num_comments': 50000,  # Very high comments
                'sentiment_score': 1.0,  # Maximum sentiment
                'created_utc': datetime.utcnow(),
                'author': 'user1',
                'subreddit': 'test'
            },
            {
                'reddit_id': 'extreme2',
                'title': 'Post with negative score',
                'score': -1000,  # Negative score
                'num_comments': 0,  # No comments
                'sentiment_score': -1.0,  # Minimum sentiment
                'created_utc': datetime.utcnow(),
                'author': 'user2',
                'subreddit': 'test'
            }
        ]
    
        # Store in database
        session_id = db_manager.create_search_session("extreme_values_test")
        for mention in extreme_mentions:
&gt;           db_manager.add_mention(session_id, mention)
E           TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given

tests\test_analytics.py:603: TypeError</failure></testcase><testcase classname="tests.test_analytics.TestAnalyticsEdgeCases" name="test_validation_with_unicode_content" time="0.029" /><testcase classname="tests.test_analytics.TestAnalyticsEdgeCases" name="test_analytics_with_missing_timestamps" time="0.015"><failure message="TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given">self = &lt;tests.test_analytics.TestAnalyticsEdgeCases object at 0x000002962924B020&gt;
db_manager = &lt;database.models.DatabaseManager object at 0x0000029631877F80&gt;

    def test_analytics_with_missing_timestamps(self, db_manager):
        """Test analytics with missing or invalid timestamps."""
        metrics_analyzer = MetricsAnalyzer(db_manager)
    
        mentions_no_timestamps = [
            {
                'reddit_id': 'no_time1',
                'title': 'Post without timestamp',
                'score': 10,
                'author': 'user1',
                'subreddit': 'test'
                # Missing created_utc
            },
            {
                'reddit_id': 'bad_time1',
                'title': 'Post with bad timestamp',
                'score': 15,
                'created_utc': datetime.utcnow(),  # Valid timestamp
                'author': 'user2',
                'subreddit': 'test'
            }
        ]
    
        # Store in database (database will handle missing timestamps)
        session_id = db_manager.create_search_session("missing_timestamps_test")
        for mention in mentions_no_timestamps:
            # Add default timestamp if missing
            if 'created_utc' not in mention:
                mention['created_utc'] = datetime.utcnow()
&gt;           db_manager.add_mention(session_id, mention)
E           TypeError: DatabaseManager.add_mention() takes 2 positional arguments but 3 were given

tests\test_analytics.py:660: TypeError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_visualizer_initialization" time="0.001"><failure message="AssertionError: assert False&#10; +  where False = hasattr(&lt;ui.visualization.MetricsVisualizer object at 0x00000296318788C0&gt;, 'reddit_colors')">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DB961E0&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x00000296318788C0&gt;

    @pytest.mark.unit
    def test_visualizer_initialization(self, visualizer):
        """Test visualizer initialization."""
        assert visualizer is not None
        assert hasattr(visualizer, 'color_scheme')
&gt;       assert hasattr(visualizer, 'reddit_colors')
E       AssertionError: assert False
E        +  where False = hasattr(&lt;ui.visualization.MetricsVisualizer object at 0x00000296318788C0&gt;, 'reddit_colors')

tests\test_ui.py:24: AssertionError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_create_mentions_timeline" time="0.001"><failure message="AttributeError: 'MetricsVisualizer' object has no attribute 'create_mentions_timeline'">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DD2D580&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x000002963187A630&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 15, 37, 27, 992645), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_create_mentions_timeline(self, visualizer, sample_mentions_list):
        """Test mentions timeline creation."""
        # Add timestamps to mentions
        now = datetime.utcnow()
        for i, mention in enumerate(sample_mentions_list):
            mention['created_utc'] = now - timedelta(hours=i)
    
&gt;       fig = visualizer.create_mentions_timeline(sample_mentions_list)
E       AttributeError: 'MetricsVisualizer' object has no attribute 'create_mentions_timeline'

tests\test_ui.py:34: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_create_sentiment_distribution" time="0.001"><failure message="AttributeError: 'MetricsVisualizer' object has no attribute 'create_sentiment_distribution'">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DD2D760&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x000002963187BAA0&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...logy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 28, 13645), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_create_sentiment_distribution(self, visualizer, sample_mentions_list):
        """Test sentiment distribution chart."""
&gt;       fig = visualizer.create_sentiment_distribution(sample_mentions_list)
E       AttributeError: 'MetricsVisualizer' object has no attribute 'create_sentiment_distribution'

tests\test_ui.py:44: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_create_subreddit_breakdown" time="0.001"><failure message="AttributeError: 'MetricsVisualizer' object has no attribute 'create_subreddit_breakdown'. Did you mean: 'create_subreddit_analysis'?">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DD2D940&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x00000296320CF230&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...logy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 28, 33645), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_create_subreddit_breakdown(self, visualizer, sample_mentions_list):
        """Test subreddit breakdown chart."""
        # Add subreddits to mentions
        subreddits = ['technology', 'programming', 'artificial', 'MachineLearning']
        for i, mention in enumerate(sample_mentions_list):
            mention['subreddit'] = subreddits[i % len(subreddits)]
    
&gt;       fig = visualizer.create_subreddit_breakdown(sample_mentions_list)
E       AttributeError: 'MetricsVisualizer' object has no attribute 'create_subreddit_breakdown'. Did you mean: 'create_subreddit_analysis'?

tests\test_ui.py:58: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_create_engagement_metrics" time="0.001"><failure message="AttributeError: 'MetricsVisualizer' object has no attribute 'create_engagement_metrics'. Did you mean: 'create_engagement_analysis'?">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DD2DB20&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x00000296320CD400&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...logy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 28, 51646), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_create_engagement_metrics(self, visualizer, sample_mentions_list):
        """Test engagement metrics visualization."""
&gt;       fig = visualizer.create_engagement_metrics(sample_mentions_list)
E       AttributeError: 'MetricsVisualizer' object has no attribute 'create_engagement_metrics'. Did you mean: 'create_engagement_analysis'?

tests\test_ui.py:67: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_create_trending_topics" time="0.001"><failure message="AttributeError: 'MetricsVisualizer' object has no attribute 'create_trending_topics'">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DD2DD00&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x00000296320C6690&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...logy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 28, 69645), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_create_trending_topics(self, visualizer, sample_mentions_list):
        """Test trending topics visualization."""
        # Add keywords to titles
        keywords = ['AI', 'machine learning', 'neural networks', 'deep learning']
        for i, mention in enumerate(sample_mentions_list):
            mention['title'] = f"Post about {keywords[i % len(keywords)]} technology"
    
&gt;       fig = visualizer.create_trending_topics(sample_mentions_list)
E       AttributeError: 'MetricsVisualizer' object has no attribute 'create_trending_topics'

tests\test_ui.py:81: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_create_quality_metrics" time="0.001"><failure message="AttributeError: 'MetricsVisualizer' object has no attribute 'create_quality_metrics'">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DD2DEE0&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x00000296320C6ED0&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...logy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 28, 89645), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_create_quality_metrics(self, visualizer, sample_mentions_list):
        """Test quality metrics visualization."""
        # Add quality scores
        for mention in sample_mentions_list:
            mention['relevance_score'] = 0.8
            mention['quality_score'] = 0.7
    
&gt;       fig = visualizer.create_quality_metrics(sample_mentions_list)
E       AttributeError: 'MetricsVisualizer' object has no attribute 'create_quality_metrics'

tests\test_ui.py:95: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_create_comprehensive_dashboard" time="0.001"><failure message="AttributeError: 'MetricsVisualizer' object has no attribute 'create_comprehensive_dashboard'">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DD2E0C0&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x00000296320C4EF0&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 15, 37, 28, 111495), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_create_comprehensive_dashboard(self, visualizer, sample_mentions_list):
        """Test comprehensive dashboard creation."""
        # Add required fields
        now = datetime.utcnow()
        subreddits = ['technology', 'programming', 'artificial']
    
        for i, mention in enumerate(sample_mentions_list):
            mention['created_utc'] = now - timedelta(hours=i)
            mention['subreddit'] = subreddits[i % len(subreddits)]
            mention['relevance_score'] = 0.8
            mention['quality_score'] = 0.7
    
&gt;       dashboard = visualizer.create_comprehensive_dashboard(sample_mentions_list)
E       AttributeError: 'MetricsVisualizer' object has no attribute 'create_comprehensive_dashboard'

tests\test_ui.py:114: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_empty_data_handling" time="0.001"><failure message="AttributeError: 'MetricsVisualizer' object has no attribute 'create_mentions_timeline'">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DD2E2A0&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x00000296320C5730&gt;

    @pytest.mark.unit
    def test_empty_data_handling(self, visualizer):
        """Test handling of empty data."""
        empty_mentions = []
    
        # Should handle empty data gracefully
&gt;       fig = visualizer.create_mentions_timeline(empty_mentions)
E       AttributeError: 'MetricsVisualizer' object has no attribute 'create_mentions_timeline'

tests\test_ui.py:132: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_malformed_data_handling" time="0.001"><failure message="AttributeError: 'MetricsVisualizer' object has no attribute 'create_sentiment_distribution'">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DD2E480&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x000002963021B200&gt;

    @pytest.mark.unit
    def test_malformed_data_handling(self, visualizer):
        """Test handling of malformed data."""
        malformed_mentions = [
            {'title': 'Valid post'},  # Missing fields
            {'score': 'invalid'},  # Invalid data types
            {},  # Empty mention
            None  # None mention
        ]
    
        # Should handle malformed data gracefully
&gt;       fig = visualizer.create_sentiment_distribution(malformed_mentions)
E       AttributeError: 'MetricsVisualizer' object has no attribute 'create_sentiment_distribution'

tests\test_ui.py:153: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_color_scheme_consistency" time="0.001"><failure message="AttributeError: 'MetricsVisualizer' object has no attribute 'create_comprehensive_dashboard'">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DD2E660&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x000002962F563200&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 28, 381493), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_color_scheme_consistency(self, visualizer, sample_mentions_list):
        """Test color scheme consistency across charts."""
&gt;       dashboard = visualizer.create_comprehensive_dashboard(sample_mentions_list)
E       AttributeError: 'MetricsVisualizer' object has no attribute 'create_comprehensive_dashboard'

tests\test_ui.py:159: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_chart_interactivity" time="0.001"><failure message="AttributeError: 'MetricsVisualizer' object has no attribute 'create_mentions_timeline'">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DD2E840&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x00000296320B3DA0&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 28, 401492), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_chart_interactivity(self, visualizer, sample_mentions_list):
        """Test chart interactivity features."""
&gt;       fig = visualizer.create_mentions_timeline(sample_mentions_list)
E       AttributeError: 'MetricsVisualizer' object has no attribute 'create_mentions_timeline'

tests\test_ui.py:170: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestMetricsVisualizer" name="test_responsive_design" time="0.001"><failure message="AttributeError: 'MetricsVisualizer' object has no attribute 'create_subreddit_breakdown'. Did you mean: 'create_subreddit_analysis'?">self = &lt;tests.test_ui.TestMetricsVisualizer object at 0x000002962DD2EA20&gt;
visualizer = &lt;ui.visualization.MetricsVisualizer object at 0x000002963198EF30&gt;
sample_mentions_list = [{'author': 'test_user', 'content': 'This is a test post discussing OpenAI technology and its impact.', 'created_utc':...ogy and its impact.', 'created_utc': datetime.datetime(2025, 5, 28, 20, 37, 28, 422493), 'num_comments': 15, ...}, ...]

    @pytest.mark.unit
    def test_responsive_design(self, visualizer, sample_mentions_list):
        """Test responsive design features."""
&gt;       fig = visualizer.create_subreddit_breakdown(sample_mentions_list)
E       AttributeError: 'MetricsVisualizer' object has no attribute 'create_subreddit_breakdown'. Did you mean: 'create_subreddit_analysis'?

tests\test_ui.py:179: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestRealtimeMonitor" name="test_monitor_initialization" time="0.001"><failure message="AssertionError: assert False&#10; +  where False = hasattr(&lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029631881730&gt;, 'system_monitor')">self = &lt;tests.test_ui.TestRealtimeMonitor object at 0x000002962DD2EC60&gt;
realtime_monitor = &lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029631881730&gt;

    @pytest.mark.unit
    def test_monitor_initialization(self, realtime_monitor):
        """Test monitor initialization."""
        assert realtime_monitor is not None
&gt;       assert hasattr(realtime_monitor, 'system_monitor')
E       AssertionError: assert False
E        +  where False = hasattr(&lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029631881730&gt;, 'system_monitor')

tests\test_ui.py:198: AssertionError</failure></testcase><testcase classname="tests.test_ui.TestRealtimeMonitor" name="test_get_system_status" time="0.001"><failure message="AttributeError: 'RealTimeMonitor' object has no attribute 'get_system_status'. Did you mean: 'emit_system_status'?">self = &lt;tests.test_ui.TestRealtimeMonitor object at 0x000002962DD2EE40&gt;
realtime_monitor = &lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029633B17050&gt;

    @pytest.mark.unit
    def test_get_system_status(self, realtime_monitor):
        """Test system status retrieval."""
&gt;       status = realtime_monitor.get_system_status()
E       AttributeError: 'RealTimeMonitor' object has no attribute 'get_system_status'. Did you mean: 'emit_system_status'?

tests\test_ui.py:204: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestRealtimeMonitor" name="test_get_scraping_metrics" time="0.001"><failure message="AttributeError: 'RealTimeMonitor' object has no attribute 'get_scraping_metrics'">self = &lt;tests.test_ui.TestRealtimeMonitor object at 0x000002962DD2F020&gt;
realtime_monitor = &lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029633B18050&gt;

    def test_get_scraping_metrics(self, realtime_monitor):
        """Test scraping metrics retrieval."""
&gt;       metrics = realtime_monitor.get_scraping_metrics()
E       AttributeError: 'RealTimeMonitor' object has no attribute 'get_scraping_metrics'

tests\test_ui.py:219: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestRealtimeMonitor" name="test_get_database_metrics" time="0.001"><failure message="AttributeError: 'RealTimeMonitor' object has no attribute 'get_database_metrics'">self = &lt;tests.test_ui.TestRealtimeMonitor object at 0x000002962DD2F200&gt;
realtime_monitor = &lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029633B1AA80&gt;

    def test_get_database_metrics(self, realtime_monitor):
        """Test database metrics retrieval."""
&gt;       metrics = realtime_monitor.get_database_metrics()
E       AttributeError: 'RealTimeMonitor' object has no attribute 'get_database_metrics'

tests\test_ui.py:230: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestRealtimeMonitor" name="test_create_system_dashboard" time="0.001"><failure message="AttributeError: 'RealTimeMonitor' object has no attribute 'create_system_dashboard'">self = &lt;tests.test_ui.TestRealtimeMonitor object at 0x000002962DD2F3E0&gt;
realtime_monitor = &lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029633B0E000&gt;

    def test_create_system_dashboard(self, realtime_monitor):
        """Test system dashboard creation."""
&gt;       dashboard = realtime_monitor.create_system_dashboard()
E       AttributeError: 'RealTimeMonitor' object has no attribute 'create_system_dashboard'

tests\test_ui.py:240: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestRealtimeMonitor" name="test_create_performance_dashboard" time="0.001"><failure message="AttributeError: 'RealTimeMonitor' object has no attribute 'create_performance_dashboard'">self = &lt;tests.test_ui.TestRealtimeMonitor object at 0x000002962DD2F170&gt;
realtime_monitor = &lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029633B079E0&gt;

    def test_create_performance_dashboard(self, realtime_monitor):
        """Test performance dashboard creation."""
&gt;       dashboard = realtime_monitor.create_performance_dashboard()
E       AttributeError: 'RealTimeMonitor' object has no attribute 'create_performance_dashboard'

tests\test_ui.py:253: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestRealtimeMonitor" name="test_start_monitoring" time="0.002"><failure message="AttributeError: &lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029633AF43E0&gt; does not have the attribute '_monitoring_loop'">self = &lt;tests.test_ui.TestRealtimeMonitor object at 0x000002962DD2EB70&gt;
realtime_monitor = &lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029633AF43E0&gt;

    @pytest.mark.asyncio
    async def test_start_monitoring(self, realtime_monitor):
        """Test monitoring start functionality."""
        # Mock the monitoring loop to avoid infinite loop
&gt;       with patch.object(realtime_monitor, '_monitoring_loop') as mock_loop:

tests\test_ui.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\Python312\Lib\unittest\mock.py:1455: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;unittest.mock._patch object at 0x0000029633AF4F80&gt;

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
&gt;           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: &lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029633AF43E0&gt; does not have the attribute '_monitoring_loop'

C:\Program Files\Python312\Lib\unittest\mock.py:1428: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestRealtimeMonitor" name="test_stop_monitoring" time="0.001"><failure message="AttributeError: 'RealTimeMonitor' object has no attribute 'stop_monitoring'. Did you mean: 'is_monitoring'?">self = &lt;tests.test_ui.TestRealtimeMonitor object at 0x000002962DD2E570&gt;
realtime_monitor = &lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029633AFC980&gt;

    def test_stop_monitoring(self, realtime_monitor):
        """Test monitoring stop functionality."""
        # Start monitoring first
        realtime_monitor.is_monitoring = True
    
&gt;       realtime_monitor.stop_monitoring()
E       AttributeError: 'RealTimeMonitor' object has no attribute 'stop_monitoring'. Did you mean: 'is_monitoring'?

tests\test_ui.py:280: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestRealtimeMonitor" name="test_alert_system" time="0.001"><failure message="AttributeError: 'RealTimeMonitor' object has no attribute '_check_alerts'">self = &lt;tests.test_ui.TestRealtimeMonitor object at 0x000002962DD2DE50&gt;
realtime_monitor = &lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029633B0D100&gt;

    def test_alert_system(self, realtime_monitor):
        """Test alert system functionality."""
        # Test high CPU alert
        high_cpu_status = {
            'cpu_usage': 95.0,
            'memory_usage': 50.0,
            'disk_usage': 30.0,
            'timestamp': datetime.utcnow()
        }
    
&gt;       alerts = realtime_monitor._check_alerts(high_cpu_status)
E       AttributeError: 'RealTimeMonitor' object has no attribute '_check_alerts'

tests\test_ui.py:294: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestRealtimeMonitor" name="test_metrics_history" time="0.001"><failure message="AttributeError: 'RealTimeMonitor' object has no attribute '_add_to_history'">self = &lt;tests.test_ui.TestRealtimeMonitor object at 0x000002962DD2D790&gt;
realtime_monitor = &lt;ui.realtime_monitor.RealTimeMonitor object at 0x0000029633B1AA80&gt;

    def test_metrics_history(self, realtime_monitor):
        """Test metrics history tracking."""
        # Add some metrics to history
        for i in range(5):
            status = {
                'cpu_usage': 50.0 + i,
                'memory_usage': 60.0 + i,
                'timestamp': datetime.utcnow() - timedelta(minutes=i)
            }
&gt;           realtime_monitor._add_to_history(status)
E           AttributeError: 'RealTimeMonitor' object has no attribute '_add_to_history'

tests\test_ui.py:310: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestGradioInterface" name="test_interface_creation" time="0.187"><failure message="AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'create_gradio_interface'">self = &lt;tests.test_ui.TestGradioInterface object at 0x000002962DD2F5C0&gt;
mock_app_components = {'db_manager': &lt;Mock id='2844099347952'&gt;, 'metrics_analyzer': &lt;Mock id='2844099348480'&gt;, 'realtime_monitor': &lt;Mock id='2844099352368'&gt;, 'scraper': &lt;Mock id='2844099351840'&gt;, ...}

    def test_interface_creation(self, mock_app_components):
        """Test Gradio interface creation."""
&gt;       with patch('app.create_gradio_interface') as mock_create:

tests\test_ui.py:338: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\Python312\Lib\unittest\mock.py:1455: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;unittest.mock._patch object at 0x0000029631880050&gt;

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
&gt;           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'create_gradio_interface'

C:\Program Files\Python312\Lib\unittest\mock.py:1428: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestGradioInterface" name="test_search_functionality" time="0.001"><failure message="AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'search_mentions'">self = &lt;tests.test_ui.TestGradioInterface object at 0x000002962DD2F7A0&gt;
mock_app_components = {'db_manager': &lt;Mock id='2844074451328'&gt;, 'metrics_analyzer': &lt;Mock id='2844074451424'&gt;, 'realtime_monitor': &lt;Mock id='2844074451520'&gt;, 'scraper': &lt;Mock id='2844074451232'&gt;, ...}

    def test_search_functionality(self, mock_app_components):
        """Test search functionality in UI."""
        # Mock scraper response
        mock_mentions = [
            {'reddit_id': 'test1', 'title': 'Test post 1', 'score': 10},
            {'reddit_id': 'test2', 'title': 'Test post 2', 'score': 20}
        ]
    
        mock_app_components['scraper'].scrape_mentions = AsyncMock(return_value=mock_mentions)
        mock_app_components['metrics_analyzer'].generate_comprehensive_metrics.return_value = {
            'basic': {'total_mentions': 2}
        }
    
        # Test search function
&gt;       with patch('app.search_mentions') as mock_search:

tests\test_ui.py:360: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\Python312\Lib\unittest\mock.py:1455: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;unittest.mock._patch object at 0x00000296300C3050&gt;

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
&gt;           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'search_mentions'

C:\Program Files\Python312\Lib\unittest\mock.py:1428: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestGradioInterface" name="test_export_functionality" time="0.001"><failure message="AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'export_data'">self = &lt;tests.test_ui.TestGradioInterface object at 0x000002962DD2F980&gt;
mock_app_components = {'db_manager': &lt;Mock id='2844074325392'&gt;, 'metrics_analyzer': &lt;Mock id='2844074457808'&gt;, 'realtime_monitor': &lt;Mock id='2844074458096'&gt;, 'scraper': &lt;Mock id='2844074314592'&gt;, ...}

    def test_export_functionality(self, mock_app_components):
        """Test data export functionality."""
        mock_mentions = [
            {'reddit_id': 'test1', 'title': 'Test post 1', 'score': 10},
            {'reddit_id': 'test2', 'title': 'Test post 2', 'score': 20}
        ]
    
&gt;       with patch('app.export_data') as mock_export:

tests\test_ui.py:375: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\Python312\Lib\unittest\mock.py:1455: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;unittest.mock._patch object at 0x00000296300A3890&gt;

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
&gt;           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'export_data'

C:\Program Files\Python312\Lib\unittest\mock.py:1428: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestGradioInterface" name="test_real_time_updates" time="0.001"><failure message="AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'update_realtime_metrics'">self = &lt;tests.test_ui.TestGradioInterface object at 0x000002962DD2FB60&gt;
mock_app_components = {'db_manager': &lt;Mock id='2844124228608'&gt;, 'metrics_analyzer': &lt;Mock id='2844124218768'&gt;, 'realtime_monitor': &lt;Mock id='2844124217712'&gt;, 'scraper': &lt;Mock id='2844124217568'&gt;, ...}

    def test_real_time_updates(self, mock_app_components):
        """Test real-time updates functionality."""
        mock_status = {
            'cpu_usage': 45.0,
            'memory_usage': 60.0,
            'active_sessions': 3
        }
    
        mock_app_components['realtime_monitor'].get_system_status.return_value = mock_status
    
&gt;       with patch('app.update_realtime_metrics') as mock_update:

tests\test_ui.py:393: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\Python312\Lib\unittest\mock.py:1455: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;unittest.mock._patch object at 0x00000296330387D0&gt;

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
&gt;           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'update_realtime_metrics'

C:\Program Files\Python312\Lib\unittest\mock.py:1428: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestGradioInterface" name="test_error_handling_in_ui" time="0.001"><failure message="AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'search_mentions'">self = &lt;tests.test_ui.TestGradioInterface object at 0x000002962DD2FD40&gt;
mock_app_components = {'db_manager': &lt;Mock id='2844124221504'&gt;, 'metrics_analyzer': &lt;Mock id='2844124221552'&gt;, 'realtime_monitor': &lt;Mock id='2844124221456'&gt;, 'scraper': &lt;Mock id='2844124230624'&gt;, ...}

    def test_error_handling_in_ui(self, mock_app_components):
        """Test error handling in UI components."""
        # Mock scraper to raise an error
        mock_app_components['scraper'].scrape_mentions = AsyncMock(
            side_effect=Exception("Scraping error")
        )
    
&gt;       with patch('app.search_mentions') as mock_search:

tests\test_ui.py:407: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\Python312\Lib\unittest\mock.py:1455: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;unittest.mock._patch object at 0x000002963303A150&gt;

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
&gt;           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'search_mentions'

C:\Program Files\Python312\Lib\unittest\mock.py:1428: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestGradioInterface" name="test_input_validation" time="0.001"><failure message="AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'validate_search_input'">self = &lt;tests.test_ui.TestGradioInterface object at 0x000002962DD2FF20&gt;
mock_app_components = {'db_manager': &lt;Mock id='2844124229232'&gt;, 'metrics_analyzer': &lt;Mock id='2844124229472'&gt;, 'realtime_monitor': &lt;Mock id='2844124229280'&gt;, 'scraper': &lt;Mock id='2844124229616'&gt;, ...}

    def test_input_validation(self, mock_app_components):
        """Test input validation in UI."""
&gt;       with patch('app.validate_search_input') as mock_validate:

tests\test_ui.py:418: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\Python312\Lib\unittest\mock.py:1455: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;unittest.mock._patch object at 0x000002963303B6B0&gt;

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
&gt;           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'validate_search_input'

C:\Program Files\Python312\Lib\unittest\mock.py:1428: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestGradioInterface" name="test_progress_tracking" time="0.001"><failure message="AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'track_progress'">self = &lt;tests.test_ui.TestGradioInterface object at 0x000002962DD50140&gt;
mock_app_components = {'db_manager': &lt;Mock id='2844124054112'&gt;, 'metrics_analyzer': &lt;Mock id='2844124054208'&gt;, 'realtime_monitor': &lt;Mock id='2844124054064'&gt;, 'scraper': &lt;Mock id='2844124053824'&gt;, ...}

    def test_progress_tracking(self, mock_app_components):
        """Test progress tracking in UI."""
&gt;       with patch('app.track_progress') as mock_progress:

tests\test_ui.py:432: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\Program Files\Python312\Lib\unittest\mock.py:1455: in __enter__
    original, local = self.get_original()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = &lt;unittest.mock._patch object at 0x0000029633010110&gt;

    def get_original(self):
        target = self.getter()
        name = self.attribute
    
        original = DEFAULT
        local = False
    
        try:
            original = target.__dict__[name]
        except (AttributeError, KeyError):
            original = getattr(target, name, DEFAULT)
        else:
            local = True
    
        if name in _builtins and isinstance(target, ModuleType):
            self.create = True
    
        if not self.create and original is DEFAULT:
&gt;           raise AttributeError(
                "%s does not have the attribute %r" % (target, name)
            )
E           AttributeError: &lt;module 'app' from 'C:\\Users\\Devansh\\Downloads\\intern_application_round1\\app.py'&gt; does not have the attribute 'track_progress'

C:\Program Files\Python312\Lib\unittest\mock.py:1428: AttributeError</failure></testcase><testcase classname="tests.test_ui.TestUIIntegration" name="test_full_ui_workflow" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py, line 450&#10;      def test_full_ui_workflow(self, mock_app_components, sample_mentions_list):&#10;E       fixture 'mock_app_components' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py:450&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py, line 450
      def test_full_ui_workflow(self, mock_app_components, sample_mentions_list):
E       fixture 'mock_app_components' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py:450</error></testcase><testcase classname="tests.test_ui.TestUIIntegration" name="test_ui_performance_with_large_dataset" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py, line 481&#10;      def test_ui_performance_with_large_dataset(self, mock_app_components, performance_monitor):&#10;E       fixture 'mock_app_components' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py:481&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py, line 481
      def test_ui_performance_with_large_dataset(self, mock_app_components, performance_monitor):
E       fixture 'mock_app_components' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py:481</error></testcase><testcase classname="tests.test_ui.TestUIIntegration" name="test_concurrent_ui_operations" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py, line 517&#10;      def test_concurrent_ui_operations(self, mock_app_components):&#10;E       fixture 'mock_app_components' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py:517&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py, line 517
      def test_concurrent_ui_operations(self, mock_app_components):
E       fixture 'mock_app_components' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py:517</error></testcase><testcase classname="tests.test_ui.TestUIIntegration" name="test_ui_state_management" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py, line 546&#10;      def test_ui_state_management(self, mock_app_components):&#10;E       fixture 'mock_app_components' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py:546&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py, line 546
      def test_ui_state_management(self, mock_app_components):
E       fixture 'mock_app_components' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py:546</error></testcase><testcase classname="tests.test_ui.TestUIIntegration" name="test_ui_accessibility" time="0.000"><error message="failed on setup with &quot;file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py, line 563&#10;      def test_ui_accessibility(self, mock_app_components):&#10;E       fixture 'mock_app_components' not found&#10;&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id&#10;&gt;       use 'pytest --fixtures [testpath]' for help on them.&#10;&#10;C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py:563&quot;">file C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py, line 563
      def test_ui_accessibility(self, mock_app_components):
E       fixture 'mock_app_components' not found
&gt;       available fixtures: anyio_backend, anyio_backend_name, anyio_backend_options, async_test_client, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, cov, data_validator, db_manager, doctest_namespace, event_loop, extra, extras, include_metadata_in_junit_xml, metadata, metrics_analyzer, mock_browser_context, mock_playwright_page, mock_redis, mock_requests, mock_textblob, monkeypatch, no_cover, performance_monitor, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_mention_data, sample_mentions_list, temp_database, test_config, test_data_generator, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, visualizer, worker_id
&gt;       use 'pytest --fixtures [testpath]' for help on them.

C:\Users\Devansh\Downloads\intern_application_round1\tests\test_ui.py:563</error></testcase></testsuite></testsuites>